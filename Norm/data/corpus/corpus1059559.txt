Question (ID-1059559): Python strings split with multiple separators Weird - I think what I want to do is a fairly common task but I've found no reference on the web. I have text, with punctuation, and I want an array of the words. i.e - "Hey, you - what are you doing here!?" should be ['hey', 'you', 'what', 'are', 'you', 'doing', 'here']. But python's split() only works with one argument... so I have all words with punctuation after I split with white space. Any ideas? 

 Thanks 
 Answers (Total-12): #0 A case where regular expressions are justified: 

 import re
DATA = "Hey, you - what are you doing here!?"
print re.findall(r'\w+', DATA)
# Prints ['Hey', 'you', 'what', 'are', 'you', 'doing', 'here']
 
 #1 re.split() 

 
 re.split(pattern, string[, maxsplit=0]) 
 
 Split string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. (Incompatibility note: in the original Python 1.5 release, maxsplit was ignored. This has been fixed in later releases.) 
 

 &gt;&gt;&gt; re.split('\W+', 'Words, words, words.')
['Words', 'words', 'words', '']
&gt;&gt;&gt; re.split('(\W+)', 'Words, words, words.')
['Words', ', ', 'words', ', ', 'words', '.', '']
&gt;&gt;&gt; re.split('\W+', 'Words, words, words.', 1)
['Words', 'words, words.']
 
 #2 another way, without regex 

 import string
punc = string.punctuation
thestring = "Hey, you - what are you doing here!?"
s=list(thestring)
''.join([ o for o in s if not o in string.punctuation ]).split()
 
 #3 Kinda late answer :), but I had a similar dilemma and didn't want to use 're' module. 

 def my_split(s, seps):
 res = [s]
 for sep in seps:
  s, res = res, []
  for seq in s:
   res += seq.split(sep)
 return res

print my_split('1111 2222 3333;4444,5555;6666', [' ', ';', ','])
['1111', '', '2222', '3333', '4444', '5555', '6666']
 
 #4 try this: 

 import re

phrase = "Hey, you - what are you doing here!?"
matches = re.findall('\w+', phrase)
print matches
 

 this will print ['Hey', 'you', 'what', 'are', 'you', 'doing', 'here'] 
 #5 Another way to achieve this is to use the Natural Language Tool Kit ( nltk ). 

 import nltk
data= "Hey, you - what are you doing here!?"
word_tokens = nltk.tokenize.regexp_tokenize(data, r'\w+')
print word_tokens
 

 This prints: ['Hey', 'you', 'what', 'are', 'you', 'doing', 'here'] 

 The biggest drawback of this method is that you need to install the nltk package . 

 The benefits are that you can do a lot of fun stuff with the rest of the nltk package once you get your tokens. 
 #6 join = lambda x: sum(x,[]) # a.k.a. flatten1([[1],[2,3],[4]]) -&gt; [1,2,3,4]
 

 Three-liner: 

 fragments = [text]
for token in tokens:
 fragments = join(f.split(token) for f in fragments)
 

 

 Explanation 

 This is what in Haskell is known as the List monad. The idea behind the monad is that once "in the monad" you "stay in the monad" until something takes you out. For example in Haskell, say you map the python range(n) -&gt; [1,2,...,n] function over a List. If the result is a List, it will be append to the List in-place, so you'd get something like map(range, [3,4,1]) -&gt; [0,1,2,0,1,2,3,0] . This is known as map-append (or mappend, or maybe something like that). The idea here is that you've got this operation you're applying (splitting on a token), and whenever you do that, you join the result into the list. 

 You can abstract this into a function and have tokens=string.punctuation by default. 

 Advantages of this approach: 

 
 This approach (unlike naive regex-based approaches) can work with arbitrary-length tokens (which regex can also do with more advanced syntax). 
 You are not restricted to mere tokens; you could have arbitrary logic in place of each token, for example one of the "tokens" could be a function which splits according to how nested parentheses are. 
 
 #7 Another quick way to do this without a regexp is to replace the characters first, as below: 

 &gt;&gt;&gt; 'a;bcd,ef g'.replace(';',' ').replace(',',' ').split()
['a', 'bcd', 'ef', 'g']
 
 #8 You want Python's RegEx module's findall() method: 

 http://www.regular-expressions.info/python.html 

 Example 
 #9 got same problem as @ooboo and find this topic
@ghostdog74 inspired me, maybe someone finds my solution usefull 

 str1='adj:sg:nom:m1.m2.m3:pos'
splitat=':.'
''.join([ s if s not in splitat else ' ' for s in str1]).split()
 

 input something in space place and split using same character if you dont want to split at spaces. 
 #10 Here is my go at a split with multiple deliminaters: 

 def msplit( str, delims ):
 w = ''
 for z in str:
 if z not in delims:
  w += z
 else:
  if len(w) &gt; 0 :
   yield w
  w = ''
 if len(w) &gt; 0 :
 yield w
 
 #11 Use list comprehensions for this stuff...it seems easier 

 data= "Hey, you - what are you doing here!?"
tokens = [c for c in data if c not in (',', ' ', '-', '!', '?')]
 

 I find this easier to comprehend (read..maintain) than using regexp, simply because I am not that good at regexp...which is the case with most of us :) . Also if you know what set of separators you might be using, you can keep them in a set. With a very huge set, this might be slower...but the 're' module is slow as well.