Question (ID-110259): Python memory profiler I want to know the memory usage of my Python application and specifically want to know what code blocks/portions or objects are consuming most memory.
Google search shows a commercial one is Python Memory Validator . 

 And open source ones are PySizer and Heapy . 

 I haven't tried anyone, so I wanted to know which one is the best considering: 

 
 Gives most details. 
 I have to do least or no changes to my code. 
 
 Answers (Total-3): #0 Heapy is quite simple to use. At some point in your code, you have to write the following: 

 from guppy import hpy
h = hpy()
print h.heap()
 

 This gives you some output like this: 

 Partition of a set of 132527 objects. Total size = 8301532 bytes.
Index Count %  Size % Cumulative % Kind (class / dict of class)
0 35144 27 2140412 26 2140412 26 str
1 38397 29 1309020 16 3449432 42 tuple
2 530 0 739856 9 4189288 50 dict (no owner)
 

 You can also find out from where objects are referenced and get statistics about that, but somehow the docs on that are a bit sparse. 

 There is a graphical browser as well, written in Tk. 
 #1 I recommend Dowser . It is very easy to setup, and you need zero changes to your code. You can view counts of objects of each type through time, view list of live objects, view references to live objects, all from the simple web interface. 

 # memdebug.py

import cherrypy
import dowser

def start(port):
 cherrypy.tree.mount(dowser.Root())
 cherrypy.config.update({
  'environment': 'embedded',
  'server.socket_port': port
 })
 cherrypy.server.quickstart()
 cherrypy.engine.start(blocking=False)
 

 You import memdebug, and call memdebug.start. That's all. 

 I haven't tried PySizer or Heapy. I would appreciate others' reviews. 
 #2 Consider the objgraph library (see http://www.lshift.net/blog/2008/11/14/tracing-python-memory-leaks for an example use case).