[[{"text": ["(3) is not necessarily a bad idea -- Python makes it easy to process \"CSV\" file (and despite the C standing for Comma, tab as a separator is just as easy to handle) and of course gets just about as much bandwidth in I/O ops as any other language.", "As for other recommendations, numpy, besides fast computation (which you may not need as per your statements) provides very handy, flexible multi-dimensional arrays, which may be quite handy for your tasks; and the standard library module multiprocessing lets you exploit multiple cores for any task that's easy to parallelize (important since just about every machine these days has multi-cores;-)."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "numpy", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "multiprocessing", "childNum": 0, "tag": "code", "childList": []}]}], [{"text": ["Ok, so just to be different, why not R?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 6, "lis": [{"text": "You seem to know R so you may get to working code quickly", "tag": "none", "senID": 1}, {"text": "30 mb per file is not large on standard workstation with a few gb of ram", "tag": "none", "senID": 2}, {"text": "the ", "tag": "none", "senID": 3}, {"text": "the bottleneck here is i/o from the disk and that is the same for every language", "tag": "none", "senID": 4}, {"text": "R has ", "tag": "none", "senID": 5}, {"text": "Should you want to employ the 'embarrassingly parallel' structure of the problem, R has several packages that are well-suited to data-parallel problems: E.g. ", "tag": "none", "senID": 6}]}], [{"text": ["Have a look at Disco.", "It is a lightweight distributed MapReduce engine, written in about 2000 lines of Erlang, but specifically designed for Python development.", "It supports not only working on your data, but also storing an replication reliably.", "They've just released version 0.3, which includes an indexing and database layer."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Disco", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://discoproject.org/"}]}], [{"text": ["With terabytes, you will want to parallelize your reads over many disks anyway; so might as well go straight into Hadoop."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Use Pig or Hive to query the data; both have extensive support for user-defined transformations, so you should be able to implement what you need to do using custom code."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["I've had good luck using R with Hadoop on Amazon's Elastic Map Reduce.", "With EMR you pay only for the computer time you use and AMZN takes care of spinning up and spinning down the instances.", "Exactly how to structure the job in EMR really depends on how your analysis workflow is structured.", "For example, are all the records needed for one job contained completely inside of each csv or do you needs bits from each csv to complete an analysis?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Here's some resources you might find useful:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"tag": "ul", "num": 3, "lis": [{"text": "Pete Skomoroch talking about ", "tag": "none", "senID": 2}, {"text": "My blog post about ", "tag": "none", "senID": 3}, {"text": "The ", "tag": "none", "senID": 4}]}, {"text": ["The problem I mentioned in my blog post is more one of being CPU bound, not IO bound.", "Your issues are more IO, but the tips on loading libraries and cachefiles might be useful. "], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["While it's tempting to try to shove this in/out of a relational database, I recommend carefully considering if you really need all the overhead of an RDB.", "If you don't, then you may create a bottleneck and development challenge with no real reward."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}], [{"text": ["In case you have a cluster of machines you can parallelize your application using Hadoop Mapreduce.", "Although Hadoop is written in Java it can run Python too.", "You can checkout the following link for pointers in parallelizing your code - PythonWordCount"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "PythonWordCount", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://wiki.apache.org/hadoop/PythonWordCount"}]}], [{"text": ["Yes.", "You are right!", "I/O would cost most of your processing time.", "I don't suggest you to use distributed systems, like hadoop, for this task. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Your task could be done in a modest workstation.", "I am not an Python expert, I think it has support for asynchronous programming.", "In F#/.Net, the platform has well support for that.", "I was once doing an image processing job, loading 20K images on disk and transform them into feature vectors only costs several minutes in parallel. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["all in all, load and process your data in parallel and save the result in memory (if small), in database (if big). "], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["When you say \"accumulating\" then solution (2) looks most suitable to problem.", "After initial load up to database you only update database with new files (daily, weekly?", "depends how often you need this)."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "", "childNum": 0, "tag": "br", "pos": 0, "childList": []}]}, {"text": ["In cases (1) and (3) you need to process files each time (what was stated earlier as most time/resources-consuming), unless you find a way to stored results and update them with new files."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["You could use R to process files from csv to, for example, SQLite database."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}]]