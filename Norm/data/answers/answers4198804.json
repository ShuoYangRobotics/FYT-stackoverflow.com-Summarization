[[{"text": ["First, the easy cases:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["ASCII"], "childNum": 0, "tag": "h2", "senID": 1, "childList": []}, {"text": ["If your data contains no bytes above 0x7F, then it's ASCII.", "(Or a 7-bit ISO646 encoding, but those are very obsolete."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["UTF-8"], "childNum": 0, "tag": "h2", "senID": 3, "childList": []}, {"text": ["If your data validates as UTF-8, then you can safely assume it is UTF-8.", "Due to UTF-8's strict validation rules, false positives are extremely rare."], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "is", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["ISO-8859-1 vs. windows-1252"], "childNum": 0, "tag": "h2", "senID": 5, "childList": []}, {"text": ["The only difference between these two encodings is that ISO-8859-1 has the C1 control characters where windows-1252 has the printable characters \u20ac\u201a\u0192\u201e\u2026\u2020\u2021\u02c6\u2030\u0160\u2039\u0152\u017d\u2018\u2019\u201c\u201d\u2022\u2013\u2014\u02dc\u2122\u0161\u203a\u0153\u017e\u0178.", "I've seen plenty of files that use curly quotes or dashes, but none that use C1 control characters.", "So don't even bother with them, or ISO-8859-1, just detect windows-1252 instead."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["That now leaves you with only one question."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["This is a lot trickier."], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"text": ["Undefined characters"], "childNum": 0, "tag": "h2", "senID": 9, "childList": []}, {"text": ["The bytes 0x81, 0x8D, 0x8F, 0x90, 0x9D are not used in windows-1252.", "If they occur, then assume the data is MacRoman."], "childNum": 0, "tag": "p", "senID": 10, "childList": []}, {"text": ["Identical characters"], "childNum": 0, "tag": "h2", "senID": 11, "childList": []}, {"text": ["The bytes 0xA2 (\u00a2), 0xA3 (\u00a3), 0xA9 (\u00a9), 0xB1 (\u00b1), 0xB5 (\u00b5) happen to be the same in both encodings.", "If these are the only non-ASCII bytes, then it doesn't matter whether you choose MacRoman or cp1252."], "childNum": 0, "tag": "p", "senID": 12, "childList": []}, {"text": ["Statistical approach"], "childNum": 0, "tag": "h2", "senID": 13, "childList": []}, {"text": ["Count character (NOT byte!", ") frequencies in the data you know to be UTF-8.", "Determine the most frequent characters.", "Then use this data to determine whether the cp1252 or MacRoman characters are more common."], "childNum": 0, "tag": "p", "senID": 14, "childList": []}, {"text": ["For example, in a search I just performed on 100 random English Wikipedia articles, the most common non-ASCII characters are \u00b7\u2022\u2013\u00e9\u00b0\u00ae\u2019\u00e8\u00f6\u2014.", "Based on this fact,"], "childNum": 1, "tag": "p", "senID": 15, "childList": [{"text": "\u00b7\u2022\u2013\u00e9\u00b0\u00ae\u2019\u00e8\u00f6\u2014", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"tag": "ul", "num": 2, "lis": [{"text": "The bytes 0x92, 0x95, 0x96, 0x97, 0xAE, 0xB0, 0xB7, 0xE8, 0xE9, or 0xF6 suggest windows-1252.", "tag": "none", "senID": 16}, {"text": "The bytes 0x8E, 0x8F, 0x9A, 0xA1, 0xA5, 0xA8, 0xD0, 0xD1, 0xD5, or 0xE1 suggest MacRoman.", "tag": "none", "senID": 17}]}, {"text": ["Count up the cp1252-suggesting bytes and the MacRoman-suggesting bytes, and go with whichever is greatest."], "childNum": 0, "tag": "p", "senID": 18, "childList": []}], [{"text": ["My attempt at such a heuristic (assuming that you've ruled out ASCII and UTF-8):"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 3, "lis": [{"text": "If 0x7f to 0x9f don't appear at all, it's probably ISO-8859-1, because those are very rarely used control codes", "tag": "none", "senID": 1}, {"text": "If 0x91 through 0x94 appear at lot, it's probably Windows-1252, because those are the \"smart quotes\", by far the most likely characters in that range to be used in English text. To be more certain you could look for pairs.", "tag": "none", "senID": 2}, {"text": "Otherwise, it's MacRoman, especially if you see a lot of 0xd2 through 0xd5 (that'S where the typographic quotes are in MacRoman).", "tag": "none", "senID": 3}]}, {"text": ["Side note:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["Do not do this!"], "childNum": 1, "tag": "p", "senID": 5, "childList": [{"text": "Do not do this!!", "childNum": 0, "tag": "strong", "pos": -1, "childList": []}]}, {"text": ["The Java compiler expects file names to match class names, so renaming the files will render the source code uncompilable.", "The correct thing would be to guess the encoding, then use the native2ascii tool to convert all non-ASCII characters to unicode escape seqeuences."], "childNum": 3, "tag": "p", "senID": 6, "childList": [{"text": "native2ascii", "tag": "a", "pos": 1, "childList": [{"text": "native2ascii", "tag": "code"}], "childNum": 1, "href": "http://download.oracle.com/javase/1.4.2/docs/tooldocs/windows/native2ascii.html"}, {"text": "native2ascii", "childNum": 0, "tag": "code", "childList": []}, {"href": "http://en.wikibooks.org/wiki/Java_Programming/Syntax/Unicode_Escape_Sequences", "text": "unicode escape seqeuences", "childNum": 0, "tag": "a", "childList": []}]}], [{"text": ["Mozilla nsUniversalDetector (Perl bindings: Encode::Detect/Encode::Detect::Detector) is millionfold proven."], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "Mozilla nsUniversalDetector", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.mozilla.org/projects/intl/UniversalCharsetDetection.html"}, {"href": "http://p3rl.org/Encode%3a%3aDetect", "text": "Encode::Detect", "childNum": 0, "tag": "a", "childList": []}, {"href": "http://p3rl.org/Encode%3a%3aDetect%3a%3aDetector", "text": "Encode::Detect::Detector", "childNum": 0, "tag": "a", "childList": []}]}], [{"text": ["As you have discovered, there is no perfect way to solve this problem, because without the implicit knowledge about which encoding a file uses, all 8-bit encodings are exactly the same: A collection of bytes.", "All bytes are valid for all 8-bit encodings."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["The best you can hope for, is some sort of algorithm that analyzes the bytes, and based on probabilities of a certain byte being used in a certain language with a certain encoding will guess at what encoding the files uses.", "But that has to know which language the file uses, and becomes completely useless when you have files with mixed encodings."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["On the upside, if you know that the text in a file is written in English, then the you're unlikely to notice any difference whichever encoding you decide to use for that file, as the differences between all the mentioned encodings are all localized in the parts of the encodings that specify characters not normally used in the English language.", "You might have some troubles where the text uses special formatting, or special versions of punctuation (CP1252 has several versions of the quote characters for instance), but for the gist of the text there will probably be no problems."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["\"Perl, C, Java, or Python, and in that order\": interesting attitude :-)"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["\"we stand a good change of knowing if something is probably UTF-8\": Actually the chance that a file containing meaningful text encoded in some other charset that uses high-bit-set bytes will decode successfully as UTF-8 is vanishingly small."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["UTF-8 strategies (in least preferred language):"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n # 100% Unicode-standard-compliant UTF-8\ndef utf8_strict(text):\n    try:\n        text.decode('utf8')\n        return True\n    except UnicodeDecodeError:\n        return False\n\n# looking for almost all UTF-8 with some junk\ndef utf8_replace(text):\n    utext = text.decode('utf8', 'replace')\n    dodgy_count = utext.count(u'\\uFFFD') \n    return dodgy_count, utext\n    # further action depends on how large dodgy_count / float(len(utext)) is\n\n# checking for UTF-8 structure but non-compliant\n# e.g. encoded surrogates, not minimal length, more than 4 bytes:\n# Can be done with a regex, if you need it\n</code>\n</pre>\n", "senID": 3}, {"text": ["Once you've decided that it's neither ASCII nor UTF-8:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["The Mozilla-origin charset detectors that I'm aware of don't support MacRoman and in any case don't do a good job on 8-bit charsets especially with English because AFAICT they depend on checking whether the decoding makes sense in the given language, ignoring the punctuation characters, and based on a wide selection of documents in that language."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["As others have remarked, you really only have the high-bit-set punctuation characters available to distinguish between cp1252 and macroman.", "I'd suggest training a Mozilla-type model on your own documents, not Shakespeare or Hansard or the KJV Bible, and taking all 256 bytes into account.", "I presume that your files have no markup (HTML, XML, etc) in them -- that would distort the probabilities something shocking."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["You've mentioned files that are mostly UTF-8 but fail to decode.", "You should also be very suspicious of:"], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["(1) files that are allegedly encoded in ISO-8859-1 but contain \"control characters\" in the range 0x80 to 0x9F inclusive ... this is so prevalent that the draft HTML5 standard says to decode ALL HTML streams declared as ISO-8859-1 using cp1252."], "childNum": 1, "tag": "p", "senID": 8, "childList": [{"text": "ALL", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["(2) files that decode OK as UTF-8 but the resultant Unicode contains \"control characters\" in the range U+0080 to U+009F inclusive ... this can result from transcoding cp1252 / cp850 (seen it happen!", ") / etc files from \"ISO-8859-1\" to UTF-8."], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"text": ["Background: I have a wet-Sunday-afternoon project to create a Python-based charset detector that's file-oriented (instead of web-oriented) and works well with 8-bit character sets including legacy ** n ones like cp850 and cp437.", "It's nowhere near prime time yet.", "I'm interested in training files; are your ISO-8859-1 / cp1252 / MacRoman files as equally \"unencumbered\" as you expect anyone's code solution to be?"], "childNum": 1, "tag": "p", "senID": 10, "childList": [{"text": "legacy ** n", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["If you can detect every encoding EXCEPT for macroman, than it would be logical to assume that the ones that can't be deciphered are in macroman.", "In other words, just make a list of files that couldn't be processed and handle those as if they were macroman."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Another way to sort these files would be to make a server based program that allows users to decide which encoding isn't garbled.", "Of course, it would be within the company, but with 100 employees doing a few each day, you'll have thousands of files done in no time."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Finally, wouldn't it be better to just convert all existing files to a single format, and require that new files be in that format."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["You may consider this C source code for MacRoman: http://alienryderflex.com/utf-8/"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://alienryderflex.com/utf-8/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://alienryderflex.com/utf-8/"}]}, {"text": ["For general guessing, would the method used in CSS 2.1 work?", "This is the method used by BBEdit and it seems to be correct 99% of the time -- at least as implemented by BBEdit."], "childNum": 3, "tag": "p", "senID": 1, "childList": [{"text": "the method", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.w3.org/TR/CSS2/syndata.html#escaping"}, {"text": "CSS 2.1", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://www.w3.org/TR/CSS2/"}, {"text": "BBEdit", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://www.barebones.com/products/bbedit/"}]}, {"text": ["Another potential source is the text encoding auto detector used in Jedit.", "It also seems pretty accurate, and is written in open source Java. "], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "text encoding auto detector", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.jedit.org/api/org/gjt/sp/jedit/io/EncodingDetector.html"}]}], [{"text": ["\"The problem usually derives from programmers who think they can reliably process a \u201ctext\u201d file without specifying the encoding.", "\"\nThere are also protocol designers who define a series of octets to be an \"ASCII\" string without even thinking what values greater than 127 means."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["Check out the chardet library for Python."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "chardet", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://chardet.feedparser.org/"}]}]]