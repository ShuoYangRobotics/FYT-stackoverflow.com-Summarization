[[{"tag": "ul", "num": 4, "lis": [{"text": ["Mechanize"], "childNum": 0, "tag": "a", "senID": 0, "childList": []}, {"text": ["Twill"], "childNum": 0, "tag": "a", "senID": 1, "childList": []}, {"text": ["BeautifulSoup"], "childNum": 0, "tag": "a", "senID": 2, "childList": []}, {"text": ["Scrapy"], "childNum": 0, "tag": "a", "senID": 3, "childList": []}]}], [{"text": ["Use Scrapy."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Scrapy", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://scrapy.org/"}]}, {"text": ["It is a twisted-based web crawler framework.", "Still under heavy development but it works already.", "Has many goodies:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"tag": "ul", "num": 7, "lis": [{"text": "Built-in support for parsing HTML, XML, CSV, and Javascript", "tag": "none", "senID": 2}, {"text": "A media pipeline for scraping items with images (or any other media) and download the image files as well", "tag": "none", "senID": 3}, {"text": "Support for extending Scrapy by plugging your own functionality using middlewares, extensions, and pipelines", "tag": "none", "senID": 4}, {"text": "Wide range of built-in middlewares and extensions for handling of compression, cache, cookies, authentication, user-agent spoofing, robots.txt handling, statistics, crawl depth restriction, etc", "tag": "none", "senID": 5}, {"text": "Interactive scraping shell console, very useful for developing and debugging", "tag": "none", "senID": 6}, {"text": "Web management console for monitoring and controlling your bot", "tag": "none", "senID": 7}, {"text": "Telnet console for low-level access to the Scrapy process", "tag": "none", "senID": 8}]}, {"text": ["Example code to extract information about all torrent files added today in the mininova torrent site, by using a XPath selector on the HTML returned:"], "childNum": 1, "tag": "p", "senID": 9, "childList": [{"text": "mininova", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.mininova.org/"}]}, {"code": "<pre>\n<code>\n class Torrent(ScrapedItem):\n    pass\n\nclass MininovaSpider(CrawlSpider):\n    domain_name = 'mininova.org'\n    start_urls = ['http://www.mininova.org/today']\n    rules = [Rule(RegexLinkExtractor(allow=['/tor/\\d+']), 'parse_torrent')]\n\n    def parse_torrent(self, response):\n        x = HtmlXPathSelector(response)\n        torrent = Torrent()\n\n        torrent.url = response.url\n        torrent.name = x.x(\"//h1/text()\").extract()\n        torrent.description = x.x(\"//div[@id='description']\").extract()\n        torrent.size = x.x(\"//div[@id='info-left']/p[2]/text()[2]\").extract()\n        return [torrent]\n</code>\n</pre>\n", "senID": 10}], [{"text": ["Check the HarvestMan, a multi-threaded web-crawler written in Python, also give a look to the spider.py module."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "HarvestMan", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://bulba.sdsu.edu/docwiki/HarvestMan"}, {"href": "http://pypi.python.org/pypi/spider.py/0.5", "text": "spider.py", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["And here you can find code samples to build a simple web-crawler."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "here", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.example-code.com/python/pythonspider.asp"}]}], [{"text": ["I hacked the above script to include a login page as I needed it to access a drupal site.", "Not pretty but may help someone out there."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n #!/usr/bin/python\n\nimport httplib2\nimport urllib\nimport urllib2\nfrom cookielib import CookieJar\nimport sys\nimport re\nfrom HTMLParser import HTMLParser\n\nclass miniHTMLParser( HTMLParser ):\n\n  viewedQueue = []\n  instQueue = []\n  headers = {}\n  opener = \"\"\n\n  def get_next_link( self ):\n    if self.instQueue == []:\n      return ''\n    else:\n      return self.instQueue.pop(0)\n\n\n  def gethtmlfile( self, site, page ):\n    try:\n        url = 'http://'+site+''+page\n        response = self.opener.open(url)\n        return response.read()\n    except Exception, err:\n        print \" Error retrieving: \"+page\n        sys.stderr.write('ERROR: %s\\n' % str(err))\n    return \"\" \n\n    return resppage\n\n  def loginSite( self, site_url ):\n    try:\n    cj = CookieJar()\n    self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n\n    url = 'http://'+site_url \n        params = {'name': 'customer_admin', 'pass': 'customer_admin123', 'opt': 'Log in', 'form_build_id': 'form-3560fb42948a06b01d063de48aa216ab', 'form_id':'user_login_block'}\n    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'\n    self.headers = { 'User-Agent' : user_agent }\n\n    data = urllib.urlencode(params)\n    response = self.opener.open(url, data)\n    print \"Logged in\"\n    return response.read() \n\n    except Exception, err:\n    print \" Error logging in\"\n    sys.stderr.write('ERROR: %s\\n' % str(err))\n\n    return 1\n\n  def handle_starttag( self, tag, attrs ):\n    if tag == 'a':\n      newstr = str(attrs[0][1])\n      print newstr\n      if re.search('http', newstr) == None:\n        if re.search('mailto', newstr) == None:\n          if re.search('#', newstr) == None:\n            if (newstr in self.viewedQueue) == False:\n              print \"  adding\", newstr\n              self.instQueue.append( newstr )\n              self.viewedQueue.append( newstr )\n          else:\n            print \"  ignoring\", newstr\n        else:\n          print \"  ignoring\", newstr\n      else:\n        print \"  ignoring\", newstr\n\n\ndef main():\n\n  if len(sys.argv)!=3:\n    print \"usage is ./minispider.py site link\"\n    sys.exit(2)\n\n  mySpider = miniHTMLParser()\n\n  site = sys.argv[1]\n  link = sys.argv[2]\n\n  url_login_link = site+\"/node?destination=node\"\n  print \"\\nLogging in\", url_login_link\n  x = mySpider.loginSite( url_login_link )\n\n  while link != '':\n\n    print \"\\nChecking link \", link\n\n    # Get the file from the site and link\n    retfile = mySpider.gethtmlfile( site, link )\n\n    # Feed the file into the HTML parser\n    mySpider.feed(retfile)\n\n    # Search the retfile here\n\n    # Get the next link in level traversal order\n    link = mySpider.get_next_link()\n\n  mySpider.close()\n\n  print \"\\ndone\\n\"\n\nif __name__ == \"__main__\":\n  main()\n</code>\n</pre>\n", "senID": 1}], [{"text": ["I've used Ruya and found it pretty good."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Ruya", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://ruya.sourceforge.net/"}]}], [{"text": ["Another simple spider \nUses BeautifulSoup and urllib2.", "Nothing too sophisticated, just reads all a href's builds a list and goes though it."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "simple spider", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.grenadepod.com/2009/12/13/python-web-crawler/"}]}], [{"text": ["pyspider.py"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "pyspider.py", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://bauerdata.bauerhost.dk/python-program-eksempler/pyspider"}]}]]