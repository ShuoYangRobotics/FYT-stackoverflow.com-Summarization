<h3>Question (ID-845058):</h3><h2>How to get line count cheaply in Python?</h2><p>I need to get a line count of a large file (hundreds of thousands of lines) in python. What is the most efficient way both memory- and time-wise?</p>

<p>At the moment I do:</p>

<pre><code>def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    return i + 1
</code></pre>

<p>is it possible to do any better?</p>
<br /><h3>Answers (Total-15):</h3><b>#0</b><br /><p>I believe that a memory mapped file will be the fastest solution. I tried four functions: the function posted by the OP (<code>opcount</code>); a simple iteration over the lines in the file (<code>simplecount</code>); readline with a memory-mapped filed (mmap) (<code>mapcount</code>); and the buffer read solution offered by Mykola Kharechko (<code>bufcount</code>).</p>

<p>I ran each function five times, and calculated the average run-time for a 1.2 million-line text file.</p>

<p>Windows XP, Python 2.5, 2GB RAM, 2 GHz AMD processor</p>

<p>Here are my results:</p>

<pre><code>mapcount : 0.465599966049
simplecount : 0.756399965286
bufcount : 0.546800041199
opcount : 0.718600034714
</code></pre>

<p><strong>Edit</strong>: numbers for Python 2.6:</p>

<pre><code>mapcount : 0.471799945831
simplecount : 0.634400033951
bufcount : 0.468800067902
opcount : 0.602999973297
</code></pre>

<p>So the buffer read strategy seems to be the fastest for Windows/Python 2.6</p>

<p>Here is the code:</p>

<pre><code>from __future__ import with_statement
import time
import mmap
import random
from collections import defaultdict

def mapcount(filename):
    f = open(filename, "r+")
    buf = mmap.mmap(f.fileno(), 0)
    lines = 0
    readline = buf.readline
    while readline():
        lines += 1
    return lines

def simplecount(filename):
    lines = 0
    for line in open(filename):
        lines += 1
    return lines

def bufcount(filename):
    f = open(filename)                  
    lines = 0
    buf_size = 1024 * 1024
    read_f = f.read # loop optimization

    buf = read_f(buf_size)
    while buf:
        lines += buf.count('\n')
        buf = read_f(buf_size)

    return lines

def opcount(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    return i + 1


counts = defaultdict(list)

for i in range(5):
    for func in [mapcount, simplecount, bufcount, opcount]:
        start_time = time.time()
        assert func("big_file.txt") == 1209138
        counts[func].append(time.time() - start_time)

for key, vals in counts.items():
    print key.__name__, ":", sum(vals) / float(len(vals))
</code></pre>
<br /><b>#1</b><br /><p>You can't get any better than that.</p>

<p>After all, any solution will have to read the entire file, figure out how many <code>\n</code> you have, and return that result.</p>

<p>Do you have a better way of doing that without reading the entire file? Not sure... The best solution will always be I/O-bound, best you can do is make sure you don't use unnecessary memory, but it looks like you have that covered.</p>
<br /><b>#2</b><br /><p>You could execute a subprocess and run <code>wc -l filename</code></p>

<pre><code>import subprocess

def file_len(fname):
    p = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE, 
                                              stderr=subprocess.PIPE)
    result, err = p.communicate()
    if p.returncode != 0:
        raise IOError(err)
    return int(result.strip().split()[0])
</code></pre>
<br /><b>#3</b><br /><p>One line, probably pretty fast:</p>

<pre><code>num_lines = sum(1 for line in open('myfile.txt'))
</code></pre>
<br /><b>#4</b><br /><pre><code>def file_len(full_path):
  """ Count number of lines in a file."""
  f = open(full_path)
  nr_of_lines = sum(1 for line in f)
  f.close()
  return nr_of_lines
</code></pre>
<br /><b>#5</b><br /><p>As for me this variant will be the fastest:</p>

<pre><code>#!/usr/bin/env python

def main():
    f = open('filename')                  
    lines = 0
    buf_size = 1024 * 1024
    read_f = f.read # loop optimization

    buf = read_f(buf_size)
    while buf:
        lines += buf.count('\n')
        buf = read_f(buf_size)

    print lines

if __name__ == '__main__':
    main()
</code></pre>

<p>reasons: buffering faster than reading line by line and <code>string.count</code> is also very fast</p>
<br /><b>#6</b><br /><p>What about this</p>

<pre><code>def file_len(fname):
  counts = itertools.count()
  with open(fname) as f: 
    for _ in f: counts.next()
  return counts.next()
</code></pre>
<br /><b>#7</b><br /><p>Just to complete the above methods I tried a variant with the fileinput module:</p>

<pre><code>import fileinput as fi   
def filecount(fname):
        for line in fi.input(fname):
            pass
        return fi.lineno()
</code></pre>

<p>And passed a 60mil lines file to all the above stated methods:</p>

<pre><code>mapcount : 6.1331050396
simplecount : 4.588793993
opcount : 4.42918205261
filecount : 43.2780818939
bufcount : 0.170812129974
</code></pre>

<p>It's a little surprise to me that fileinput is that bad and scales far worse than all the other methods...</p>
<br /><b>#8</b><br /><p>Why not read the first 100 and the last 100 lines and estimate the average line length, then divide the total file size through that numbers? If you don't need a exact value this could work.</p>
<br /><b>#9</b><br /><p>what about this?</p>

<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.stdin=open('fname','r')
&gt;&gt;&gt; data=sys.stdin.readlines()
&gt;&gt;&gt; print "counted",len(data),"lines"
</code></pre>
<br /><b>#10</b><br /><p>Why wouldn't the following work?</p>

<pre><code>import sys

# input comes from STDIN
file = sys.stdin
data = file.readlines()

# get total number of lines in file
lines = len(data)

print lines
</code></pre>

<p>In this case, the len function uses the input lines as a means of determining the length.</p>
<br /><b>#11</b><br /><p><code>count = max(enumerate(open(filename)))[0]</code></p>
<br /><b>#12</b><br /><p>How about this?</p>

<pre><code>import fileinput
import sys

counter=0
for line in fileinput.input([sys.argv[1]]):
    counter+=1

fileinput.close()
print counter
</code></pre>
<br /><b>#13</b><br /><p>Here is a python function to use mpi (pypar) to distribute the line counting across machines/cores.  My test improves counting a 20mil line file from 26 seconds to 7 seconds using an 8 core windows 64 server.</p>

<pre><code>import pypar, os, logging, sys

def getFileLineCount( file1 ):
    m1 = open(file1, "r")

    #work out file size
    #divide up to farm out line counting

    fSize = os.stat(file1).st_size 
    chunk = (fSize / pypar.size()) + 1

    lines = 0

    #first get max width of each field and the names
    seekStart = chunk * (pypar.rank())
    seekEnd = chunk * (pypar.rank()+1)
    if seekEnd &gt; fSize:
        seekEnd = fSize

    #find where to start
    if pypar.rank() &gt; 0:
        m1.seek( seekStart )
        #read next line
        l1 = m1.readline()
        seekStart = m1.tell()

    #tell previous rank my seek start to make their seek end
    if pypar.rank() &gt; 0:
        pypar.send( seekStart, pypar.rank()-1 )
    if pypar.rank() &lt; pypar.size()-1:
        seekEnd = pypar.receive( pypar.rank()+1 )

    m1.seek( seekStart )    
    l1 = m1.readline()
    while len(l1) &gt; 0:
        lines += 1
        l1 = m1.readline()
        if m1.tell() &gt; seekEnd or len(l1) == 0:
            break

    if pypar.rank() == 0:
        for p in range(1,pypar.size()):
            lines += pypar.receive( p )

        for p in range(1,pypar.size()):
            pypar.send( lines, p )
    else:
        pypar.send( lines,0 )
        lines = pypar.receive(0)

    m1.close()
    return lines
</code></pre>
<br /><b>#14</b><br /><p>the result of opening a file is an iterator, which can be converted to a sequence, which has a length:</p>

<pre><code>with open(filename) as f:
   return len(list(f))
</code></pre>

<p>this is more concise than your explicit loop, and avoids the <code>enumerate</code>.</p>
<br />