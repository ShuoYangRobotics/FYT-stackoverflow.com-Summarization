<h3>Question (ID-419235):</h3><h2>Anyone know of a good Python based web crawler that I could use?</h2><p>I'm half-tempted to write my own, but I don't really have enough time right now.  I've seen the Wikipedia list of <a href="http://en.wikipedia.org/wiki/Web_crawler#Open-source_crawlers" rel="nofollow">open source crawlers</a> but I'd prefer something written in Python.  I realize that I could probably just use one of the tools on the Wikipedia page and wrap it in Python.  I might end up doing that - if anyone has any advice about any of those tools, I'm open to hearing about them.  I've used Heritrix via its web interface and I found it to be quite cumbersome.  I definitely won't be using a browser API for my upcoming project.</p>

<p>Thanks in advance.  Also, this is my first SO question!</p>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><ul>
<li><a href="http://wwwsearch.sourceforge.net/mechanize/" rel="nofollow">Mechanize</a> is my favorite; great high-level browsing capabilities (super-simple form filling and submission).</li>
<li><a href="http://twill.idyll.org/" rel="nofollow">Twill</a> is a simple scripting language built on top of Mechanize</li>
<li><a href="http://www.crummy.com/software/BeautifulSoup/" rel="nofollow">BeautifulSoup</a> + <a href="http://docs.python.org/library/urllib2.html" rel="nofollow">urllib2</a> also works quite nicely.</li>
<li><a href="http://scrapy.org/" rel="nofollow">Scrapy</a> looks like an extremely promising project; it's new.</li>
</ul>
<br /><b>#1</b><br /><p>Use <a href="http://scrapy.org/" rel="nofollow">Scrapy</a>.</p>

<p>It is a twisted-based web crawler framework. Still under heavy development but it works already. Has many goodies:</p>

<ul>
<li>Built-in support for parsing HTML, XML, CSV, and Javascript</li>
<li>A media pipeline for scraping items with images (or any other media) and download the image files as well</li>
<li>Support for extending Scrapy by plugging your own functionality using middlewares, extensions, and pipelines</li>
<li>Wide range of built-in middlewares and extensions for handling of compression, cache, cookies, authentication, user-agent spoofing, robots.txt handling, statistics, crawl depth restriction, etc</li>
<li>Interactive scraping shell console, very useful for developing and debugging</li>
<li>Web management console for monitoring and controlling your bot</li>
<li>Telnet console for low-level access to the Scrapy process</li>
</ul>

<p>Example code to extract information about all torrent files added today in the <a href="http://www.mininova.org/" rel="nofollow">mininova</a> torrent site, by using a XPath selector on the HTML returned:</p>

<pre><code>class Torrent(ScrapedItem):
    pass

class MininovaSpider(CrawlSpider):
    domain_name = 'mininova.org'
    start_urls = ['http://www.mininova.org/today']
    rules = [Rule(RegexLinkExtractor(allow=['/tor/\d+']), 'parse_torrent')]

    def parse_torrent(self, response):
        x = HtmlXPathSelector(response)
        torrent = Torrent()

        torrent.url = response.url
        torrent.name = x.x("//h1/text()").extract()
        torrent.description = x.x("//div[@id='description']").extract()
        torrent.size = x.x("//div[@id='info-left']/p[2]/text()[2]").extract()
        return [torrent]
</code></pre>
<br /><b>#2</b><br /><p>Check the <a href="http://bulba.sdsu.edu/docwiki/HarvestMan" rel="nofollow">HarvestMan</a>, a multi-threaded web-crawler written in Python, also give a look to the <a href="http://pypi.python.org/pypi/spider.py/0.5" rel="nofollow">spider.py</a> module.</p>

<p>And <a href="http://www.example-code.com/python/pythonspider.asp" rel="nofollow">here</a> you can find code samples to build a simple web-crawler.</p>
<br /><b>#3</b><br /><p>I hacked the above script to include a login page as I needed it to access a drupal site. Not pretty but may help someone out there.</p>

<pre><code>#!/usr/bin/python

import httplib2
import urllib
import urllib2
from cookielib import CookieJar
import sys
import re
from HTMLParser import HTMLParser

class miniHTMLParser( HTMLParser ):

  viewedQueue = []
  instQueue = []
  headers = {}
  opener = ""

  def get_next_link( self ):
    if self.instQueue == []:
      return ''
    else:
      return self.instQueue.pop(0)


  def gethtmlfile( self, site, page ):
    try:
        url = 'http://'+site+''+page
        response = self.opener.open(url)
        return response.read()
    except Exception, err:
        print " Error retrieving: "+page
        sys.stderr.write('ERROR: %s\n' % str(err))
    return "" 

    return resppage

  def loginSite( self, site_url ):
    try:
    cj = CookieJar()
    self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))

    url = 'http://'+site_url 
        params = {'name': 'customer_admin', 'pass': 'customer_admin123', 'opt': 'Log in', 'form_build_id': 'form-3560fb42948a06b01d063de48aa216ab', 'form_id':'user_login_block'}
    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
    self.headers = { 'User-Agent' : user_agent }

    data = urllib.urlencode(params)
    response = self.opener.open(url, data)
    print "Logged in"
    return response.read() 

    except Exception, err:
    print " Error logging in"
    sys.stderr.write('ERROR: %s\n' % str(err))

    return 1

  def handle_starttag( self, tag, attrs ):
    if tag == 'a':
      newstr = str(attrs[0][1])
      print newstr
      if re.search('http', newstr) == None:
        if re.search('mailto', newstr) == None:
          if re.search('#', newstr) == None:
            if (newstr in self.viewedQueue) == False:
              print "  adding", newstr
              self.instQueue.append( newstr )
              self.viewedQueue.append( newstr )
          else:
            print "  ignoring", newstr
        else:
          print "  ignoring", newstr
      else:
        print "  ignoring", newstr


def main():

  if len(sys.argv)!=3:
    print "usage is ./minispider.py site link"
    sys.exit(2)

  mySpider = miniHTMLParser()

  site = sys.argv[1]
  link = sys.argv[2]

  url_login_link = site+"/node?destination=node"
  print "\nLogging in", url_login_link
  x = mySpider.loginSite( url_login_link )

  while link != '':

    print "\nChecking link ", link

    # Get the file from the site and link
    retfile = mySpider.gethtmlfile( site, link )

    # Feed the file into the HTML parser
    mySpider.feed(retfile)

    # Search the retfile here

    # Get the next link in level traversal order
    link = mySpider.get_next_link()

  mySpider.close()

  print "\ndone\n"

if __name__ == "__main__":
  main()
</code></pre>
<br /><b>#4</b><br /><p>I've used <a href="http://ruya.sourceforge.net/" rel="nofollow">Ruya</a> and found it pretty good.</p>
<br /><b>#5</b><br /><p>Another <a href="http://www.grenadepod.com/2009/12/13/python-web-crawler/" rel="nofollow">simple spider</a> 
Uses BeautifulSoup and urllib2. Nothing too sophisticated, just reads all a href's builds a list and goes though it.</p>
<br /><b>#6</b><br /><p><a href="http://bauerdata.bauerhost.dk/python-program-eksempler/pyspider" rel="nofollow">pyspider.py</a></p>
<br />