<h3>Question (ID-264154):</h3><h2>Google AppEngine: How to fetch more than 1000?</h2><p>First: I'm a beginner in python.</p>

<p>How can I fetch more than 1000 record from data store and put all in one single list to pass to django?</p>
<br /><h3>Answers (Total-15):</h3><b>#0</b><br /><p>Starting with Version 1.3.6 (released Aug-17-2010) you <strong>CAN</strong>  </p>

<p><a href="http://code.google.com/p/googleappengine/wiki/SdkReleaseNotes#Version_1.3.6_-_August_17,_2010" rel="nofollow">From the changelog:</a></p>

<blockquote>
  <p>Results of datastore count() queries <strong>and offsets for all datastore queries are no longer capped at 1000</strong>.</p>
</blockquote>
<br /><b>#1</b><br /><p>Every time this comes up as a limitation, I always wonder "<em>why</em> do you need more than 1,000 results?"  Did you know that Google themselves doesn't serve up more than 1,000 results?  Try this search:  <a href="http://www.google.ca/search?hl=en&amp;client=firefox-a&amp;rls=org.mozilla:en-US:official&amp;hs=qhu&amp;q=1000+results&amp;start=1000&amp;sa=N" rel="nofollow">http://www.google.ca/search?hl=en&amp;client=firefox-a&amp;rls=org.mozilla:en-US:official&amp;hs=qhu&amp;q=1000+results&amp;start=1000&amp;sa=N</a>  I didn't know that until recently, because I'd never taken the time to click into the 100th page of search results on a query.</p>

<p>If you're actually returning more than 1,000 results back to the user, then I think there's a bigger problem at hand than the fact that the data store won't let you do it.</p>

<p>One possible (legitimate) reason to need that many results is if you were doing a large operation on the data and presenting a summary (for example, what is the average of all this data).  The solution to this problem (which is talked about in the Google I/O talk) is to calculate the summary data on-the-fly, as it comes in, and save it.</p>
<br /><b>#2</b><br /><p>App Engine gives you a nice way of "paging" through the results by 1000 by ordering on Keys and using the last key as the next offset. They even provide some sample code here:</p>

<p><a href="http://code.google.com/appengine/docs/python/datastore/queriesandindexes.html#Queries_on_Keys" rel="nofollow">http://code.google.com/appengine/docs/python/datastore/queriesandindexes.html#Queries_on_Keys</a></p>

<p>Although their example spreads the queries out over many requests, you can change the page size from 20 to 1000 and query in a loop, combining the querysets. Additionally you might use itertools to link the queries without evaluating them before they're needed.</p>

<p>For example, to count how many rows beyond 1000:</p>

<pre><code>class MyModel(db.Expando):
    @classmethod
    def count_all(cls):
        """
        Count *all* of the rows (without maxing out at 1000)
        """
        count = 0
        query = cls.all().order('__key__')

        while count % 1000 == 0:
            current_count = query.count()
            if current_count == 0:
                break

            count += current_count

            if current_count == 1000:
                last_key = query.fetch(1, 999)[0].key()
                query = query.filter('__key__ &gt; ', last_key)

        return count
</code></pre>
<br /><b>#3</b><br /><p><strong>You can't.</strong></p>

<p>Part of the FAQ states that there is no way you can access beyond row 1000 of a query, increasing the "OFFSET" will just result in a shorter result set, </p>

<p>ie: OFFSET 999 --> 1 result comes back. </p>

<p>From Wikipedia: </p>

<blockquote>
  <p>App Engine limits the maximum rows
  returned from an entity get to 1000
  rows per Datastore call. Most web
  database applications use paging and
  caching, and hence do not require this
  much data at once, so this is a
  non-issue in most scenarios.[citation
  needed] If an application needs more
  than 1,000 records per operation, it
  can use its own client-side software
  or an Ajax page to perform an
  operation on an unlimited number of
  rows.</p>
</blockquote>

<p>From <a href="http://code.google.com/appengine/docs/whatisgoogleappengine.html" rel="nofollow">http://code.google.com/appengine/docs/whatisgoogleappengine.html</a></p>

<blockquote>
  <p>Another example of a service limit is
  the number of results returned by a
  query. A query can return at most
  1,000 results. Queries that would
  return more results only return the
  maximum. In this case, a request that
  performs such a query isn't likely to
  return a request before the timeout,
  but the limit is in place to conserve
  resources on the datastore.</p>
</blockquote>

<p>From <a href="http://code.google.com/appengine/docs/datastore/gqlreference.html" rel="nofollow">http://code.google.com/appengine/docs/datastore/gqlreference.html</a></p>

<blockquote>
  <p>Note: A LIMIT clause has a maximum of
  1000. If a limit larger than the maximum is specified, the maximum is
  used. This same maximum applies to the
  fetch() method of the GqlQuery class.</p>
  
  <p>Note: Like the offset parameter for
  the fetch() method, an OFFSET in a GQL
  query string does not reduce the
  number of entities fetched from the
  datastore. It only affects which
  results are returned by the fetch()
  method. A query with an offset has
  performance characteristics that
  correspond linearly with the offset
  size.</p>
</blockquote>

<p>From <a href="http://code.google.com/appengine/docs/datastore/queryclass.html" rel="nofollow">http://code.google.com/appengine/docs/datastore/queryclass.html</a></p>

<blockquote>
  <p>The limit and offset arguments control
  how many results are fetched from the
  datastore, and how many are returned
  by the fetch() method:</p>
  
  <ul>
  <li><p>The datastore fetches offset + limit results to the application. The first offset results are <strong>not</strong> skipped by the datastore itself.</p></li>
  <li><p>The fetch() method skips the first offset results, then returns the rest (limit results).</p></li>
  <li><p>The query has performance characteristics that correspond
  linearly with the offset amount plus the limit.</p></li>
  </ul>
</blockquote>

<h2>What this means is </h2>

<p>If you have a singular query, there is no way to request anything outside the range 0-1000. </p>

<p>Increasing offset will just raise the 0, so</p>

<pre><code>LIMIT 1000  OFFSET 0
</code></pre>

<p>Will return 1000 rows, </p>

<p>and </p>

<pre><code>LIMIT 1000 OFFSET 1000
</code></pre>

<p>Will return <strong>0 rows</strong>, thus, making it impossible to, with a single query syntax, fetch 2000 results either manually or using the API. </p>

<h2>The only plausible exception</h2>

<p>Is to create a numeric index on the table, ie: </p>

<pre><code> SELECT * FROM Foo  WHERE ID &gt; 0 AND ID &lt; 1000 

 SELECT * FROM Foo WHERE ID &gt;= 1000 AND ID &lt; 2000
</code></pre>

<p>If your data or query can't have this 'ID' hardcoded identifier, then you are <strong>out of luck</strong></p>
<br /><b>#4</b><br /><p>Just for the record - fetch limit of 1000 entries is now gone:</p>

<p><a href="http://googleappengine.blogspot.com/2010/02/app-engine-sdk-131-including-major.html" rel="nofollow">http://googleappengine.blogspot.com/2010/02/app-engine-sdk-131-including-major.html</a></p>

<p>Quotation:</p>

<blockquote>
  <p>No more 1000 result limit - That's
  right: with addition of Cursors and
  the culmination of many smaller
  Datastore stability and performance
  improvements over the last few months,
  we're now confident enough to remove
  the maximum result limit altogether.
  Whether you're doing a fetch,
  iterating, or using a Cursor, there's
  no limits on the number of results.</p>
</blockquote>
<br /><b>#5</b><br /><p>The 1000 record limit is a hard limit in Google AppEngine.</p>

<p>This presentation <a href="http://sites.google.com/site/io/building-scalable-web-applications-with-google-app-engine" rel="nofollow">http://sites.google.com/site/io/building-scalable-web-applications-with-google-app-engine</a> explains how to efficiently page through data using AppEngine.</p>

<p>(Basically by using a numeric id as key and specifying a WHERE clause on the id.)</p>
<br /><b>#6</b><br /><p>This 1K limit issue is resolved.</p>

<p>query = MyModel.all()
for doc in query:
    print doc.title</p>

<p>By treating the Query object as an iterable: The iterator retrieves results from the datastore in small batches, allowing for the app to stop iterating on results to avoid fetching more than is needed. Iteration stops when all of the results that match the query have been retrieved. As with fetch(), the iterator interface does not cache results, so creating a new iterator from the Query object will re-execute the query.</p>

<p>The max batch size is 1K. And you still have the auto Datastore quotas as well.</p>

<p>But with the plan 1.3.1 SDK, they've introduced cursors that can be serialized and saved so that a future invocation can begin the query where it last left off at.</p>
<br /><b>#7</b><br /><pre><code>class Count(object):
def getCount(self,cls):
	class Count(object):
def getCount(self,cls):
	"""
	Count *all* of the rows (without maxing out at 1000)
	"""
	count = 0
	query = cls.all().order('__key__')


	while 1:
		current_count = query.count()
		count += current_count
		if current_count == 0:
			break

		last_key = query.fetch(1, current_count-1)[0].key()
		query = query.filter('__key__ &gt; ', last_key)

	return count
</code></pre>
<br /><b>#8</b><br /><p>we are using something in our <code>ModelBase</code> class that is:</p>

<pre><code>@classmethod
def get_all(cls):
  q = cls.all()
  holder = q.fetch(1000)
  result = holder
  while len(holder) == 1000:
    holder = q.with_cursor(q.cursor()).fetch(1000)
    result += holder
  return result
</code></pre>

<p>This gets around the 1000 query limit on every model without having to think about it. I suppose a keys version would be just as easy to implement.</p>
<br /><b>#9</b><br /><p>JJG: your solution above is awesome, except that it causes an infinite loop if you have 0 records.  (I found this out while testing some of my reports locally).</p>

<p>I modified the start of the while loop to look like this:</p>

<pre><code>while count % 1000 == 0:
    current_count = query.count()
    if current_count == 0:
        break
</code></pre>
<br /><b>#10</b><br /><p>To add the contents of the two queries together:</p>

<pre><code>list1 = first query
list2 = second query
list1 += list2
</code></pre>

<p>List 1 now contains all 2000 results.</p>
<br /><b>#11</b><br /><p>The proposed solution only works if entries are sorted by key... If you are sorting by another column first, you still have to use a limit(offset, count) clause, then the 1000 entries limitation still apply. It is the same if you use two requests : one for retrieving indexes (with conditions and sort) and another using where index in () with a subset of indexes from the first result, as the first request cannot return more than 1000 keys ? (The Google <em>Queries on Keys</em> section does not state clearly if we have to sort by <em>key</em> to remove the 1000 results limitation)</p>
<br /><b>#12</b><br /><p>I'm building an app that needs to report on events occurring. An event has a type and I also need to report by event type.</p>

<p>The 1000 limit is throwing a wrench into how I would normally do it. I don't need to retrieve all of the entities and present them to the user, but I do need to show the total count for a specific date range. Any suggestions?</p>

<p>Thanks!</p>
<br /><b>#13</b><br /><p>This is close to the solution provided by Gabriel, but doesn't fetch the results it just counts them:</p>

<pre><code>count = 0
q = YourEntityClass.all().filter('myval = ', 2)
countBatch = q.count()
while countBatch &gt; 0:
    count += countBatch
    countBatch = q.with_cursor(q.cursor()).count()

logging.info('Count=%d' % count)
</code></pre>

<p>Works perfectly for my queries, and fast too (1.1 seconds to count 67,000 entities)</p>

<p>Note that the query must not be an inequality filter or a set or the cursor will not work and you'll get this exception:</p>

<blockquote>
  <p>AssertionError: No cursor available for a MultiQuery (queries using "IN" or "!=" operators)</p>
</blockquote>
<br /><b>#14</b><br /><pre><code>entities = []
for entity in Entity.all():
    entities.append(entity)
</code></pre>

<p>Simple as that. Note that there is an RPC made for every entity which is much slower than fetching in chunks. So if you're concerned about performance, do the following:</p>

<p>If you have less than 1M items:</p>

<pre><code>entities = Entity.all().fetch(999999)
</code></pre>

<p>Otherwise, use a cursor and make sure to order your entities to avoid dupes.</p>

<p>It should also be noted that:</p>

<pre><code>Entity.all().fetch(Entity.all().count())
</code></pre>

<p>returns 1000 max and should not be used.</p>
<br />