<h3>Question ( ID-6964392 ) : </h3><h2>Speed comparison with Project Euler : C vs Python vs Erlang vs Haskell </h2><p>I have taken Problem # 12 from Project Euler as a programming exercise and to compare my ( surely <span style="background-color:yellow;">not</span> optimal ) implementations in C , Python , Erlang and Haskell. In order to get some higher execution times , I search for the first <span style="background-color:yellow;">triangle</span> <span style="background-color:yellow;">number</span> with more than 1000 divisors instead of 500 as stated in the original problem . </p>
<p>The result is the following : </p>
<p>C : </p>
<pre><code>lorenzo@enzo:~/erlang$ gcc -lm -o euler12.bin euler12.c
lorenzo@enzo:~/erlang$ time ./euler12.bin
842161320

real    0m11.074s
user    0m11.070s
sys 0m0.000s
</code></pre>
<p>python : </p>
<pre><code>lorenzo@enzo:~/erlang$ time ./euler12.py 
842161320

real    1m16.632s
user    1m16.370s
sys 0m0.250s
</code></pre>
<p>python with pypy : </p>
<pre><code>lorenzo@enzo:~/Downloads/pypy-c-jit-43780-b590cf6de419-linux64/bin$ time ./pypy /home/lorenzo/erlang/euler12.py 
842161320

real    0m13.082s
user    0m13.050s
sys 0m0.020s
</code></pre>
<p>erlang : </p>
<pre><code>lorenzo@enzo:~/erlang$ erlc euler12.erl 
lorenzo@enzo:~/erlang$ time erl -s euler12 solve
Erlang R13B03 (erts-5.7.4) [source] [64-bit] [smp:4:4] [rq:4] [async-threads:0] [hipe] [kernel-poll:false]

Eshell V5.7.4  (abort with ^G)
1> 842161320

real    0m48.259s
user    0m48.070s
sys 0m0.020s
</code></pre>
<p>haskell : </p>
<pre><code>lorenzo@enzo:~/erlang$ ghc euler12.hs -o euler12.hsx
[1 of 1] Compiling Main             ( euler12.hs, euler12.o )
Linking euler12.hsx ...
lorenzo@enzo:~/erlang$ time ./euler12.hsx 
842161320

real    2m37.326s
user    2m37.240s
sys 0m0.080s
</code></pre>
<p>Summary : </p>
<ul>C : 100 % python : 692 % ( 118 % with pypy ) erlang : 436 % ( 135 % thanks to RichardC ) haskell : 1421 % </ul>
<p>I suppose that C has a big advantage as it uses long for the calculations and <span style="background-color:yellow;">not</span> arbitrary length <span style="background-color:yellow;">integers</span> as the other three. Also it does <span style="background-color:yellow;">n't</span> <span style="background-color:yellow;">need</span> to load a runtime first ( Do the others ? ) . </p>
<p>Question 1 : Do Erlang , Python and Haskell loose speed due to using arbitrary length <span style="background-color:yellow;">integers</span> or do <span style="background-color:yellow;">n't</span> they as long as the values are less than MAXINT ? </p>
<p>Question 2 : Why is Haskell so slow ? Is there a compiler flag that turns off the brakes or is it my implementation ? ( The latter is quite probable as Haskell is a book with seven seals to me. ) </p>
<p>Question 3 : Can you offer me some hints how to optimize these implementations without changing the way I determine the factors ? Optimization in any way : <span style="background-color:yellow;">nicer</span> , faster , more " <span style="background-color:yellow;">native</span> " to the language . </p>
<p>EDIT : </p>
<p>Question 4 : Do my functional implementations permit LCO ( last call optimization , a.k.a tail recursion elimination ) and hence avoid adding unnecessary frames onto the call stack ? </p>
<p>I really tried to implement the same algorithm as similar as possible in the four languages , although I have to admit that my Haskell and Erlang knowledge is very limited . </p>
<hr />
<p>Source codes used : </p>
<pre><code>#include <stdio.h>
#include <math.h>

int factorCount (long n)
{
    double square = sqrt (n);
    int isquare = (int) square;
    int count = isquare == square ? -1 : 0;
    long candidate;
    for (candidate = 1; candidate <= isquare; candidate ++)
        if (0 == n % candidate) count += 2;
    return count;
}

int main ()
{
    long triangle = 1;
    int index = 1;
    while (factorCount (triangle) < 1001)
    {
        index ++;
        triangle += index;
    }
    printf ("%ld\n", triangle);
}
</code></pre>
<hr />
<pre><code>#! /usr/bin/env python3.2

import math

def factorCount (n):
    square = math.sqrt (n)
    isquare = int (square)
    count = -1 if isquare == square else 0
    for candidate in range (1, isquare + 1):
        if not n % candidate: count += 2
    return count

triangle = 1
index = 1
while factorCount (triangle) < 1001:
    index += 1
    triangle += index

print (triangle)
</code></pre>
<hr />
<pre><code>-module (euler12).
-compile (export_all).

factorCount (Number) -> factorCount (Number, math:sqrt (Number), 1, 0).

factorCount (_, Sqrt, Candidate, Count) when Candidate > Sqrt -> Count;

factorCount (_, Sqrt, Candidate, Count) when Candidate == Sqrt -> Count + 1;

factorCount (Number, Sqrt, Candidate, Count) ->
    case Number rem Candidate of
        0 -> factorCount (Number, Sqrt, Candidate + 1, Count + 2);
        _ -> factorCount (Number, Sqrt, Candidate + 1, Count)
    end.

nextTriangle (Index, Triangle) ->
    Count = factorCount (Triangle),
    if
        Count > 1000 -> Triangle;
        true -> nextTriangle (Index + 1, Triangle + Index + 1)  
    end.

solve () ->
    io:format ("~p~n", [nextTriangle (1, 1) ] ),
    halt (0).
</code></pre>
<hr />
<pre><code>factorCount number = factorCount' number isquare 1 0 - (fromEnum $ square == fromIntegral isquare)
    where square = sqrt $ fromIntegral number
          isquare = floor square

factorCount' number sqrt candidate count
    | fromIntegral candidate > sqrt = count
    | number `mod` candidate == 0 = factorCount' number sqrt (candidate + 1) (count + 2)
    | otherwise = factorCount' number sqrt (candidate + 1) count

nextTriangle index triangle
    | factorCount triangle > 1000 = triangle
    | otherwise = nextTriangle (index + 1) (triangle + index + 1)

main = print $ nextTriangle 1 1
</code></pre>
<br /><h3>Answers ( Total-9 ) : </h3><b># <span style="background-color:yellow;">0</span> </b><br /><p>Using GHC 7.0.3 , gcc 4.4.6 , Linux 2.6.29 on an x86_64 Core2 Duo ( 2.5GHz ) machine , compiling using ghc -O2 -fllvm -fforce-recomp for Haskell and gcc -O3 -lm for C . </p>
<ul>Your C routine runs in 8.4 seconds ( faster than your run probably because of -O3 ) The Haskell solution runs in 36 seconds ( due to the -O2 flag ) Your <span style="background-color:yellow;">factorCount</span> ' code is <span style="background-color:yellow;">n't</span> explicitly typed and defaulting to Integer ( thanks to Daniel for correcting my misdiagnosis here ! ) . Giving an explicit type signature ( which is standard practice anyway ) using Int and the time changes to 11.1 seconds in <span style="background-color:yellow;">factorCount</span> ' you have <span style="background-color:yellow;">needlessly</span> called fromIntegral . A fix results in <span style="background-color:yellow;">no</span> change though ( the compiler is smart , lucky for you ) . You used mod where rem is faster and sufficient. This changes the time to 8.5 seconds . <span style="background-color:yellow;">factorCount</span> ' is constantly applying two extra arguments that <span style="background-color:yellow;">never</span> change ( candidate , sqrt ) . A worker/wrapper transformation gives us : $ time ./so 842161320 real <span style="background-color:yellow;">0m7.954s</span> user <span style="background-color:yellow;">0m7.944s</span> <span style="background-color:yellow;">sys</span> <span style="background-color:yellow;">0m0.004s</span> </ul>
<p>That 's right , 7.95 seconds . Consistently half a second faster than the C solution . Without the -fllvm flag I 'm still getting 8.182 seconds , so the <span style="background-color:yellow;">NCG</span> backend is doing well in this case too . </p>
<p>Conclusion : Haskell is awesome . </p>
<p>Resulting Code </p>
<pre class="lang-hs prettyprint-override"><code>factorCount number = factorCount' number isquare 1 0 - (fromEnum $ square == fromIntegral isquare)
    where square = sqrt $ fromIntegral number
          isquare = floor square

factorCount' :: Int -> Int -> Int -> Int -> Int
factorCount' number sqrt candidate0 count0 = go candidate0 count0
  where
  go candidate count
    | candidate > sqrt = count
    | number `rem` candidate == 0 = go (candidate + 1) (count + 2)
    | otherwise = go (candidate + 1) count

nextTriangle index triangle
    | factorCount triangle > 1000 = triangle
    | otherwise = nextTriangle (index + 1) (triangle + index + 1)

main = print $ nextTriangle 1 1
</code></pre>
<p>EDIT : So <span style="background-color:yellow;">now</span> that we 've explored that , lets address the questions </p>
<blockquote>Question 1 : Do erlang , python and haskell loose speed due to using arbitrary length <span style="background-color:yellow;">integers</span> or do <span style="background-color:yellow;">n't</span> they as long as the values are less than MAXINT ? </blockquote>
<p>In Haskell , using Integer is slower than Int but how much slower depends on the computations performed. Luckily ( for 64 bit machines ) Int is sufficient. For portability sake you should probably rewrite my code to use Int64 or Word64 ( C is <span style="background-color:yellow;">n't</span> the only language with a long ) . </p>
<blockquote>Question 2 : Why is haskell so slow ? Is there a compiler flag that turns off the brakes or is it my implementation ? ( The latter is quite probable as haskell is a book with seven seals to me. ) Question 3 : Can you offer me some hints how to optimize these implementations without changing the way I determine the factors ? Optimization in any way : <span style="background-color:yellow;">nicer</span> , faster , more " <span style="background-color:yellow;">native</span> " to the language . </blockquote>
<p>That was what I answered above. The answer was <span style="background-color:yellow;">0</span> ) Use optimization via -O2 1 ) Use fast ( <span style="background-color:yellow;">notably</span> : unbox-able ) types when possible 2 ) rem <span style="background-color:yellow;">not</span> mod ( a frequently forgotten optimization ) and 3 ) worker/wrapper transformation ( perhaps the most common optimization ) . </p>
<blockquote>Question 4 : Do my functional implementations permit LCO and hence avoid adding unnecessary frames onto the call stack ? </blockquote>
<p>Yes , that was <span style="background-color:yellow;">n't</span> the issue. Good work and glad you considered this . </p>
<br /><b># 1 </b><br /><p>There are some problems with the Erlang implementation. As baseline for the following , my measured execution time for your unmodified Erlang program was 47.6 seconds , compared to 12.7 seconds for the C code . </p>
<p>The first thing you should do if you want to run computationally <span style="background-color:yellow;">intensive</span> Erlang code is to use <span style="background-color:yellow;">native</span> code. Compiling with " erlc + <span style="background-color:yellow;">native</span> <span style="background-color:yellow;">euler12</span> " got the time down to 41.3 seconds. This is however a much lower speedup ( just 15 % ) than expected from <span style="background-color:yellow;">native</span> compilation on this kind of code , and the problem is your use of " -compile ( <span style="background-color:yellow;">export_all</span> ) " . This is useful for experimentation , but the fact that all functions are potentially reachable from the outside causes the <span style="background-color:yellow;">native</span> compiler to be very conservative. ( The <span style="background-color:yellow;">normal</span> BEAM emulator is <span style="background-color:yellow;">not</span> that much affected. ) Replacing this declaration with " -export ( [ solve/0 ] ) . " gives a much better speedup : 31.5 seconds ( almost 35 % from the baseline ) . </p>
<p>But the code itself has a problem : for each iteration in the <span style="background-color:yellow;">factorCount</span> loop , you perform this test : </p>
<pre><code>factorCount (_, Sqrt, Candidate, Count) when Candidate == Sqrt -> Count + 1;
</code></pre>
<p>The C code does <span style="background-color:yellow;">n't</span> do this. In general , it can be tricky to make a fair comparison between different implementations of the same code , and in particular if the algorithm is <span style="background-color:yellow;">numerical</span> , because you <span style="background-color:yellow;">need</span> to be sure that they are actually doing the same thing. A slight rounding error in one implementation due to some typecast somewhere may cause it to do many more iterations than the other even though both eventually reach the same result . </p>
<p>To eliminate this possible error source ( and get rid of the extra test in each iteration ) , I rewrote the <span style="background-color:yellow;">factorCount</span> function as follows , closely modelled on the C code : </p>
<pre><code>factorCount (N) ->
    Sqrt = math:sqrt (N),
    ISqrt = trunc(Sqrt),
    if ISqrt == Sqrt -> factorCount (N, ISqrt, 1, -1);
       true          -> factorCount (N, ISqrt, 1, 0)
    end.

factorCount (_N, ISqrt, Candidate, Count) when Candidate > ISqrt -> Count;
factorCount ( N, ISqrt, Candidate, Count) ->
    case N rem Candidate of
        0 -> factorCount (N, ISqrt, Candidate + 1, Count + 2);
        _ -> factorCount (N, ISqrt, Candidate + 1, Count)
    end.
</code></pre>
<p>This rewrite , <span style="background-color:yellow;">no</span> " <span style="background-color:yellow;">export_all</span> " , and <span style="background-color:yellow;">native</span> compilation , gave me the following run time : </p>
<pre><code>$ erlc +native euler12.erl
$ time erl -noshell -s euler12 solve
842161320

real    0m19.468s
user    0m19.450s
sys 0m0.010s
</code></pre>
<p>which is <span style="background-color:yellow;">not</span> too bad compared to the C code : </p>
<pre><code>$ time ./a.out 
842161320

real    0m12.755s
user    0m12.730s
sys 0m0.020s
</code></pre>
<p>considering that Erlang is <span style="background-color:yellow;">not</span> at all geared towards writing <span style="background-color:yellow;">numerical</span> code , being only 50 % slower than C on a program like this is pretty good . </p>
<p>Finally , regarding your questions : </p>
<p>Question 1 : Do erlang , python and haskell loose speed due to using arbitrary length <span style="background-color:yellow;">integers</span> or do <span style="background-color:yellow;">n't</span> they as long as the values are less than MAXINT ? </p>
<p>Yes , somewhat. In Erlang , there is <span style="background-color:yellow;">no</span> way of saying " use 32/64-bit arithmetic with wrap-around " , so unless the compiler can prove some bounds on your <span style="background-color:yellow;">integers</span> ( and it usually ca <span style="background-color:yellow;">n't</span> ) , it must check all computations to see if they can fit in a single tagged word or if it has to turn them <span style="background-color:yellow;">into</span> heap-allocated bignums. Even if <span style="background-color:yellow;">no</span> bignums are ever used in practice at runtime , these checks will have to be performed. On the other hand , that means you know that the algorithm will <span style="background-color:yellow;">never</span> fail because of an unexpected <span style="background-color:yellow;">integer</span> wraparound if you suddenly give it larger inputs than before . </p>
<p>Question 4 : Do my functional implementations permit LCO and hence avoid adding unnecessary frames onto the call stack ? </p>
<p>Yes , your Erlang code is correct with respect to last call optimization . </p>
<br /><b># 2 </b><br /><p>In regards to Python optimization , in addition to using PyPy ( for pretty impressive speed-ups with zero change to your code ) , you could use PyPy 's translation toolchain to compile an RPython-compliant version , or <span style="background-color:yellow;">Cython</span> to build an <span style="background-color:yellow;">extension</span> module , both of which are faster than the C version in my testing , with the <span style="background-color:yellow;">Cython</span> module <span style="background-color:yellow;">nearly</span> twice as fast . For reference I include C and PyPy benchmark results as well : </p>
<p>C ( compiled with gcc -O3 -lm ) </p>
<pre><code>% time ./euler12-c 
842161320
./euler12-c  11.95s user 0.00s system 99% cpu 11.959 total
</code></pre>
<p>PyPy 1.5 </p>
<pre><code>% time pypy euler12.py
842161320
pypy euler12.py  16.44s user 0.01s system 99% cpu 16.449 total
</code></pre>
<p>RPython ( using latest PyPy revision , c2f583445aee ) </p>
<pre><code>% time ./euler12-rpython-c
842161320
./euler12-rpy-c  10.54s user 0.00s system 99% cpu 10.540 total
</code></pre>
<p><span style="background-color:yellow;">Cython</span> <span style="background-color:yellow;">0.15</span> </p>
<pre><code>% time python euler12-cython.py
842161320
python euler12-cython.py  6.27s user 0.00s system 99% cpu 6.274 total
</code></pre>
<p>The RPython version has a couple of key changes. To translate <span style="background-color:yellow;">into</span> a standalone program you <span style="background-color:yellow;">need</span> to define your <span style="background-color:yellow;">target</span> , which in this case is the <span style="background-color:yellow;">main</span> function. It 's expected to accept <span style="background-color:yellow;">sys.argv</span> as it 's only argument , and is required to return an <span style="background-color:yellow;">int.</span> You can translate it by using translate.py , % translate.py <span style="background-color:yellow;">euler12-rpython.py</span> which translates to C and compiles it for you . </p>
<pre><code># euler12-rpython.py

import math, sys

def factorCount(n):
    square = math.sqrt(n)
    isquare = int(square)
    count = -1 if isquare == square else 0
    for candidate in xrange(1, isquare + 1):
        if not n % candidate: count += 2
    return count

def main(argv):
    triangle = 1
    index = 1
    while factorCount(triangle) < 1001:
        index += 1
        triangle += index
    print triangle
    return 0

if __name__ == '__main__':
    main(sys.argv)

def target(*args):
    return main, None
</code></pre>
<p>The <span style="background-color:yellow;">Cython</span> version was rewritten as an <span style="background-color:yellow;">extension</span> module <span style="background-color:yellow;">_euler12.pyx</span> , which I import and call from a <span style="background-color:yellow;">normal</span> python file. The <span style="background-color:yellow;">_euler12.pyx</span> is essentially the same as your version , with some additional static type declarations. The setup.py has the <span style="background-color:yellow;">normal</span> boilerplate to build the <span style="background-color:yellow;">extension</span> , using python setup.py build_ext --inplace . </p>
<pre><code># _euler12.pyx
from libc.math cimport sqrt

cdef int factorCount(int n):
    cdef int candidate, isquare, count
    cdef double square
    square = sqrt(n)
    isquare = int(square)
    count = -1 if isquare == square else 0
    for candidate in range(1, isquare + 1):
        if not n % candidate: count += 2
    return count

cpdef main():
    cdef int triangle = 1, index = 1
    while factorCount(triangle) < 1001:
        index += 1
        triangle += index
    print triangle

# euler12-cython.py
import _euler12
_euler12.main()

# setup.py
from distutils.core import setup
from distutils.extension import Extension
from Cython.Distutils import build_ext

ext_modules = [Extension("_euler12", ["_euler12.pyx"])]

setup(
  name = 'Euler12-Cython',
  cmdclass = {'build_ext': build_ext},
  ext_modules = ext_modules
)
</code></pre>
<p>I honestly have very little experience with either RPython or <span style="background-color:yellow;">Cython</span> , and was pleasantly surprised at the results. If you are using CPython , writing your CPU-intensive bits of code in a <span style="background-color:yellow;">Cython</span> <span style="background-color:yellow;">extension</span> module seems like a really easy way to optimize your program . </p>
<br /><b># 3 </b><br /><p>Take a look at this blog . Over the past year or so he 's done a few of the Project Euler problems in Haskell and Python , and he 's generally found Haskell to be much faster. I think that between those languages it has more to do with your fluency and coding style . </p>
<p>When it comes to Python speed , you 're using the wrong implementation ! Try PyPy , and for things like this you 'll find it to be much , much faster . </p>
<br /><b># 4 </b><br /><blockquote>Question 3 : Can you offer me some hints how to optimize these implementations without changing the way I determine the factors ? Optimization in any way : <span style="background-color:yellow;">nicer</span> , faster , more " <span style="background-color:yellow;">native</span> " to the language . </blockquote>
<p>The C implementation is suboptimal ( as hinted at by Thomas M. DuBuisson ) , the version uses 64-bit <span style="background-color:yellow;">integers</span> ( i.e. long datatype ) . I 'll investigate the assembly listing later , but with an educated guess , there are some memory accesses going on in the compiled code , which make using 64-bit <span style="background-color:yellow;">integers</span> significantly slower. It 's that or generated code ( be it the fact that you can fit less 64-bit <span style="background-color:yellow;">ints</span> in a SSE register or round a double to a 64-bit <span style="background-color:yellow;">integer</span> is slower ) . </p>
<p>Here is the modified code ( simply replace long with <span style="background-color:yellow;">int</span> and I explicitly inlined <span style="background-color:yellow;">factorCount</span> , although I do <span style="background-color:yellow;">not</span> think that this is <span style="background-color:yellow;">necessary</span> with gcc -O3 ) : </p>
<pre><code>#include <stdio.h>
#include <math.h>

static inline int factorCount(int n)
{
    double square = sqrt (n);
    int isquare = (int)square;
    int count = isquare == square ? -1 : 0;
    int candidate;
    for (candidate = 1; candidate <= isquare; candidate ++)
        if (0 == n % candidate) count += 2;
    return count;
}

int main ()
{
    int triangle = 1;
    int index = 1;
    while (factorCount (triangle) < 1001)
    {
        index++;
        triangle += index;
    }
    printf ("%d\n", triangle);
}
</code></pre>
<p>Running + timing it gives : </p>
<pre><code>$ gcc -O3 -lm -o euler12 euler12.c; time ./euler12
842161320
./euler12  2.95s user 0.00s system 99% cpu 2.956 total
</code></pre>
<p>For reference , the haskell implementation by Thomas in the earlier answer gives : </p>
<pre><code>$ ghc -O2 -fllvm -fforce-recomp euler12.hs; time ./euler12                                                                                      [9:40]
[1 of 1] Compiling Main             ( euler12.hs, euler12.o )
Linking euler12 ...
842161320
./euler12  9.43s user 0.13s system 99% cpu 9.602 total
</code></pre>
<p>Conclusion : Taking <span style="background-color:yellow;">nothing</span> away from ghc , its a great compiler , but gcc <span style="background-color:yellow;">normally</span> generates faster code . </p>
<br /><b># 5 </b><br /><blockquote>Question 1 : Do erlang , python and haskell loose speed due to using arbitrary length <span style="background-color:yellow;">integers</span> or do <span style="background-color:yellow;">n't</span> they as long as the values are less than MAXINT ? </blockquote>
<p>This is unlikely. I can <span style="background-color:yellow;">not</span> say much about Erlang and Haskell ( well , maybe a bit about Haskell below ) but I can point a lot of other bottlenecks in Python. Every time the program tries to execute an operation with some values in Python , it should verify whether the values are from the proper type , and it costs a bit of time. Your <span style="background-color:yellow;">factorCount</span> function just allocates a list with range ( 1 , isquare + 1 ) various times , and runtime , malloc -styled memory allocation is way slower than iterating on a range with a counter as you do in C. <span style="background-color:yellow;">Notably</span> , the <span style="background-color:yellow;">factorCount</span> ( ) is called multiple times and so allocates a lot of lists. Also , let us <span style="background-color:yellow;">not</span> forget that Python is <span style="background-color:yellow;">interpreted</span> and the CPython <span style="background-color:yellow;">interpreter</span> has <span style="background-color:yellow;">no</span> great focus on being optimized . </p>
<p>EDIT : oh , well , I <span style="background-color:yellow;">note</span> that you are using Python 3 so range ( ) does <span style="background-color:yellow;">not</span> return a list , but a generator. In this case , my point about allocating lists is half-wrong : the function just allocates range objects , which are inefficient <span style="background-color:yellow;">nonetheless</span> but <span style="background-color:yellow;">not</span> as inefficient as allocating a list with a lot of items . </p>
<blockquote>Question 2 : Why is haskell so slow ? Is there a compiler flag that turns off the brakes or is it my implementation ? ( The latter is quite probable as haskell is a book with seven seals to me. ) </blockquote>
<p>Are you using Hugs ? Hugs is a considerably slow <span style="background-color:yellow;">interpreter.</span> If you are using it , maybe you can get a better time with GHC - but I am only cogitating hypotesis , the kind of stuff a good Haskell compiler does under the hood is pretty fascinating and way beyond my comprehension : ) </p>
<blockquote>Question 3 : Can you offer me some hints how to optimize these implementations without changing the way I determine the factors ? Optimization in any way : <span style="background-color:yellow;">nicer</span> , faster , more " <span style="background-color:yellow;">native</span> " to the language . </blockquote>
<p>I 'd say you are playing an unfunny game. The best part of knowing various languages is to use them the most different way possible : ) But I digress , I just do <span style="background-color:yellow;">not</span> have any recommendation for this point. Sorry , I hope someone can help you in this case : ) </p>
<blockquote>Question 4 : Do my functional implementations permit LCO and hence avoid adding unnecessary frames onto the call stack ? </blockquote>
<p>As far as I remember , you just <span style="background-color:yellow;">need</span> to make sure that your recursive call is the last command before returning a value. In other words , a function like the one below could use such optimization : </p>
<pre><code>def factorial(n, acc=1):
    if n > 1:
        acc = acc * n
        n = n - 1
        return factorial(n, acc)
    else:
        return acc
</code></pre>
<p>However , you would <span style="background-color:yellow;">not</span> have such optimization if your function were such as the one below , because there is an operation ( multiplication ) after the recursive call : </p>
<pre><code>def factorial2(n):
    if n > 1:
        f = factorial2(n-1)
        return f*n
    else:
        return 1
</code></pre>
<p>I separated the operations in some local variables for make it clear which operations are executed. However , the most usual is to see these functions as below , but they are equivalent for the point I am making : </p>
<pre><code>def factorial(n, acc=1):
    if n > 1:
        return factorial(n-1, acc*n)
    else:
        return acc

def factorial2(n):
    if n > 1:
        return n*factorial(n-1)
    else:
        return 1
</code></pre>
<p><span style="background-color:yellow;">Note</span> that it is up to the compiler/interpreter to decide if it will make tail recursion. For example , the Python <span style="background-color:yellow;">interpreter</span> does <span style="background-color:yellow;">not</span> do it if I remember well ( I used Python in my example only because of its fluent syntax ) . Anyway , if you find strange stuff such as <span style="background-color:yellow;">factorial</span> functions with two parameters ( and one of the parameters has <span style="background-color:yellow;">names</span> such as acc , accumulator etc. ) <span style="background-color:yellow;">now</span> you know why people do it : ) </p>
<br /><b># 6 </b><br /><p>Looking at your Erlang implementation. The timing has included the start up of the entire virtual machine , running your program and halting the virtual machine. Am pretty sure that setting up and halting the erlang vm takes some time . </p>
<p>If the timing was done within the erlang virtual machine itself , results would be different as in that case we would have the actual time for only the program in question. Otherwise , i believe that the total time taken by the process of starting and loading of the Erlang Vm plus that of halting it ( as you put it in your program ) are all included in the total time which the method you are using to time the program is outputting. Consider using the erlang timing itself which we use when we want to time our programs within the virtual machine itself timer : tc/1 or timer : tc/2 or timer : tc/3 . In this way , the results from erlang will exclude the time taken to start and stop/kill/halt the virtual machine. That is my reasoning there , think about it , and then try your bench mark again . </p>
<p>I actually suggest that we try to time the program ( for languages that have a runtime ) , within the runtime of those languages in order to get a precise value. C for example has <span style="background-color:yellow;">no</span> overhead of starting and shutting down a runtime <span style="background-color:yellow;">system</span> as does Erlang , Python and Haskell ( 98 % sure of this - i stand correction ) . So ( based on this reasoning ) i conclude by saying that this benchmark wasnot precise /fair enough for languages running on top of a runtime <span style="background-color:yellow;">system.</span> Lets do it again with these changes . </p>
<p>EDIT : besides even if all the languages had runtime <span style="background-color:yellow;">systems</span> , the overhead of starting each and halting it would differ. so i suggest we time from within the runtime <span style="background-color:yellow;">systems</span> ( for the languages for which this applies ) . The Erlang VM is known to have considerable overhead at start up ! </p>
<br /><b># 7 </b><br /><p>C is fast , but parallelized Haskell can be faster. See Haskell in 5 Steps . </p>
<br /><b># 8 </b><br /><p>There is a webpage associated solely to showing the speed differences in languages. Based on your data versus the shootout page , I would say that your algorithm is <span style="background-color:yellow;">not</span> as good as some of the others used in the shootout page . </p>
<br />