<h3>Question ( ID-1136106 ) : </h3><h2>Efficent way to insert thousands of records into a table ( SQLite , Python , Django ) </h2><p>I have to insert 8000 + records into a SQLite database using Django 's ORM. This operation needs to be run as a cronjob about once per minute. At the moment I 'm using a for loop to iterate through all the items and then insert them one by one . Example : </p>
<pre><code>for item in items:
    entry = Entry(a1=item.a1, a2=item.a2)
    entry.save()
</code></pre>
<p>What is an efficent way of doing this ? Edit : A little comparison between the two insertion methods . </p>
<p>Without commit_manually decorator ( 11245 records ) : </p>
<pre><code>nox@noxdevel marinetraffic]$ time python manage.py insrec             

real    1m50.288s
user    0m6.710s
sys     0m23.445s
</code></pre>
<p>Using commit_manually decorator ( 11245 records ) : </p>
<pre><code>[nox@noxdevel marinetraffic]$ time python manage.py insrec                

real    0m18.464s
user    0m5.433s
sys     0m10.163s
</code></pre>
<p>Note : The test script also does some other operations besides inserting into the database ( downloads a ZIP file , extracts an XML file from the ZIP archive , parses the XML file ) so the time needed for execution does not necessarily represent the time needed to insert the records . </p>
<br /><h3>Answers ( Total-7 ) : </h3><b># 0 </b><br /><p>You want to check out " <span style="background-color:yellow;">django.db.transaction.commit_manually</span> " . </p>
<p>http : //docs.djangoproject.com/en/dev/topics/db/transactions/ # <span style="background-color:yellow;">django-db-transaction-commit-manually</span> </p>
<p>So it would be something like : </p>
<pre><code>from django.db import transaction

@transaction.commit_manually
def viewfunc(request):
    ...
    for item in items:
        entry = Entry(a1=item.a1, a2=item.a2)
        entry.save()
    transaction.commit()
</code></pre>
<p>Which will only commit once , instead at each save ( ) . </p>
<p>In <span style="background-color:yellow;">django</span> 1.3 context managers were introduced . So now you can use transaction.commit_on_success ( ) in a similar way : </p>
<pre><code>from django.db import transaction

def viewfunc(request):
    ...
    with transaction.commit_on_success():
        for item in items:
            entry = Entry(a1=item.a1, a2=item.a2)
            entry.save()
</code></pre>
<br /><b># 1 </b><br /><p>Have a look at this . It 's meant for use out-of-the-box with MySQL only , but there are pointers on what to do for other databases . </p>
<br /><b># 2 </b><br /><p>You might be better off bulk-loading the items - prepare a file and use a bulk load tool. This will be vastly more efficient than 8000 individual inserts . </p>
<br /><b># 3 </b><br /><p>You should check out DSE . I wrote DSE to solve these kinds of problems ( massive insert or updates ) . Using the <span style="background-color:yellow;">django</span> orm is a dead-end , you got to do it in plain SQL and DSE takes care of much of that for you . </p>
<p>Thomas </p>
<br /><b># 4 </b><br /><p>I recommend using plain SQL ( not ORM ) you can insert multiple rows with a single insert : </p>
<pre><code>insert into A select from B;
</code></pre>
<p>The select from B portion of your sql could be as complicated as you want it to get as long as the results match the columns in table A and there are no constraint conflicts . </p>
<br /><b># 5 </b><br /><p>Bulk creation will be available in Django 1.4 : </p>
<p>https : //docs.djangoproject.com/en/dev/ref/models/querysets/ # bulk-create </p>
<br /><b># 6 </b><br /><p>I 've ran into the same problem and I ca n't figure out a way to do it without so many inserts . I agree that using transactions is probably the right way to solve it , but here is my hack : </p>
<pre><code> def viewfunc(request):
     ...
     to_save = [];
     for item in items:
         entry = Entry(a1=item.a1, a2=item.a2)
         to_save.append(entry);
     map(lambda x: x.save(), to_save);
</code></pre>
<br />