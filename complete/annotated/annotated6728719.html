<h3>Question ( ID-6728719 ) : </h3><h2>How do I make this <span style="background-color:yellow;">list</span> function faster ? </h2><pre><code>def removeDuplicatesFromList(seq): 
    # Not order preserving 
    keys = {}
    for e in seq:
        keys[e] = 1
    return keys.keys()

def countWordDistances(li):
    '''
    If li = ['that','sank','into','the','ocean']    
    This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }
    However, if there is a duplicate term, take the average of their positions
    '''
    wordmap = {}
    unique_words = removeDuplicatesFromList(li)
    for w in unique_words:
        distances = [i+1 for i,x in enumerate(li) if x == w]
        wordmap[w] = float(sum(distances)) / float(len(distances)) #take average
    return wordmap
</code></pre>
<p>How do I make this function faster ? </p>
<br /><h3>Answers ( Total-8 ) : </h3><b># 0 </b><br /><pre><code>import collections
def countWordDistances(li):
    wordmap = collections.defaultdict(list)
    for i, w in enumerate(li, 1):
        wordmap[w].append(i)
    for k, v in wordmap.iteritems():
        wordmap[k] = sum(v)/float(len(v))

    return wordmap
</code></pre>
<p>This makes only one pass through the <span style="background-color:yellow;">list</span> , and keeps operations to a minimum. I timed this on a word <span style="background-color:yellow;">list</span> with 1.1M entries , 29k unique words , and <span style="background-color:yellow;">it</span> was almost twice as fast as Patrick 's answer. On a <span style="background-color:yellow;">list</span> of 10k words , 2k unique , <span style="background-color:yellow;">it</span> was more than 300x faster than the OP 's code . </p>
<p>To make Python code go faster , there are two rules to keep <span style="background-color:yellow;">in</span> mind : use the best algorithm , and avoid Python . </p>
<p>On the algorithm front , <span style="background-color:yellow;">iterating</span> the <span style="background-color:yellow;">list</span> once <span style="background-color:yellow;">instead</span> of N + 1 times ( N = number of unique words ) <span style="background-color:yellow;">is</span> the main thing that will speed this up . </p>
<p>On the " avoid Python " front , I mean : you want your code to be executing <span style="background-color:yellow;">in</span> C as much as possible. So using defaultdict <span style="background-color:yellow;">is</span> better than a dict where you explicitly check <span style="background-color:yellow;">if</span> the key <span style="background-color:yellow;">is</span> present. defaultdict does that check for you , but does <span style="background-color:yellow;">it</span> <span style="background-color:yellow;">in</span> C , <span style="background-color:yellow;">in</span> the Python <span style="background-color:yellow;">implementation.</span> enumerate <span style="background-color:yellow;">is</span> better than for <span style="background-color:yellow;">i</span> <span style="background-color:yellow;">in</span> range ( <span style="background-color:yellow;">len</span> ( <span style="background-color:yellow;">li</span> ) ) , again because <span style="background-color:yellow;">it</span> 's fewer Python steps. And enumerate ( <span style="background-color:yellow;">li</span> , 1 ) makes the counting start at 1 <span style="background-color:yellow;">instead</span> of having to have a Python + 1 somewhere <span style="background-color:yellow;">in</span> the <span style="background-color:yellow;">loop</span> . </p>
<p>Edited : Third rule : use PyPy. My code goes twice as fast on PyPy as on 2.7 . </p>
<br /><b># 1 </b><br /><p>Based off @ Ned Batchelder 's solution , but without creating dummy <span style="background-color:yellow;">lists</span> : </p>
<pre><code>import collections
def countWordDistances(li):
    wordmap = collections.defaultdict(lambda:[0.0, 0.0])
    for i, w in enumerate(li, 1):
        wordmap[w][0] += i
        wordmap[w][1] += 1.0
    for k, (t, n) in wordmap.iteritems():
        wordmap[k] = t / n
    return wordmap
</code></pre>
<br /><b># 2 </b><br /><p>I 'm not sure <span style="background-color:yellow;">if</span> this will be faster than using a set , but <span style="background-color:yellow;">it</span> requires only one pass through the <span style="background-color:yellow;">list</span> : </p>
<pre><code>def countWordDistances(li):
    wordmap = {}
    for i in range(len(li)):
        if li[i] in wordmap:
            avg, num = wordmap[li[i]]
            new_avg = avg*(num/(num+1.0)) + (1.0/(num+1.0))*i
            wordmap[li[i]] = new_avg, num+1
        else:
            wordmap[li[i]] = (i, 1)

    return wordmap
</code></pre>
<p>This returns a modified <span style="background-color:yellow;">version</span> of wordmap , with the <span style="background-color:yellow;">values</span> associated with each key being a tuple of the average position and the number of occurences. You could obviously easily transform this to the form of the original output , but this would take some time . </p>
<p>The code basically keeps a running average while <span style="background-color:yellow;">iterating</span> through the <span style="background-color:yellow;">list</span> , recalculating each time by taking a weighted average . </p>
<br /><b># 3 </b><br /><p>Use a set : </p>
<pre><code>def countWordDistances(li):
    '''
    If li = ['that','sank','into','the','ocean']    
    This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }
    However, if there is a duplicate term, take the average of their positions
    '''
    wordmap = {}
    unique_words = set(li)
    for w in unique_words:
        distances = [i+1 for i,x in enumerate(li) if x == w]
        wordmap[w] = float(sum(distances)) / float(len(distances)) #take average
    return wordmap
</code></pre>
<br /><b># 4 </b><br /><p>The first thing that springs to mind <span style="background-color:yellow;">is</span> to use a set to remove duplicate words : </p>
<pre><code>unique_words = set(li)
</code></pre>
<p>In general , though , <span style="background-color:yellow;">if</span> you 're worried about speed you need to profile the function to see where the bottleneck <span style="background-color:yellow;">is</span> , and then try to reduce that bottleneck . </p>
<br /><b># 5 </b><br /><p>Use a frozenset <span style="background-color:yellow;">instead</span> of a dict , since you 're not doing anything with the <span style="background-color:yellow;">values</span> : </p>
<pre><code>def removeDuplicatesFromList(seq):
    return frozenset(seq)
</code></pre>
<br /><b># 6 </b><br /><p>Using <span style="background-color:yellow;">list</span> comprehension : </p>
<pre><code>def countWordDistances(l):
    unique_words = set(l)
    idx = [[i for i,x in enumerate(l) if x==item]
            for item in unique_words]
    return {item:1.*sum(idx[i])/len(idx[i]) + 1.
            for i,item in enumerate(unique_words)}

li = ['that','sank','into','the','ocean']
countWordDistances(li)
# {'into': 3.0, 'ocean': 5.0, 'sank': 2.0, 'that': 1.0, 'the': 4.0}

li2 = ['that','sank','into','the','ocean', 'that']
countWordDistances(li2)
# {'into': 3.0, 'ocean': 5.0, 'sank': 2.0, 'that': 3.5, 'the': 4.0}
</code></pre>
<br /><b># 7 </b><br /><p>Oneliner - </p>
<pre><code>from __future__ import division   # no need for this if using py3k

def countWordDistances(li):
    '''
    If li = ['that','sank','into','the','ocean']    
    This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }
    However, if there is a duplicate term, take the average of their positions
    '''
    return {w:sum(dist)/len(dist) for w,dist in zip(set(li), ([i+1 for i,x in enumerate(li) if x==w] for w in set(li))) }
</code></pre>
<p>What I am making <span style="background-color:yellow;">in</span> the <span style="background-color:yellow;">last</span> <span style="background-color:yellow;">line</span> <span style="background-color:yellow;">is</span> a dictionary comprehension , similar to a <span style="background-color:yellow;">list</span> comprehension . </p>
<br />