<h3>Question ( ID-6525314 ) : </h3><h2>Optimized searching in Python against a list </h2><p>Problem : </p>
<p>Given a list of n <span style="background-color:yellow;">objects</span> ( n 's Order of magnitude is 10 ^ 5 ) , search for a given item very fast with a minimum of spacetime tradeoff. Current , unoptimized &amp ; prototype-y solution takes too long and consumes too much RAM ( the optimization is not premature , that is ) . </p>
<p>There is not a primary <span style="background-color:yellow;">key</span> to sort against in the <span style="background-color:yellow;">object</span> , but it can be sorted to a certain degree , such as the following example , where the first column is sorted . </p>
<pre><code>o1 => f, g, h
o2 => f, g, i
o3 => f, j, k
o4 => k, j, m
</code></pre>
<p>To date , the solution has been nested filters : </p>
<pre><code>filter(test1, filter(test2, filter(test3, the_list)))
</code></pre>
<p>But that has been slow , since it involves n * ( n - 1 ) * ( n - 2 ) operations , which approximates to O ( n ^ 3 ) speed , and at least n * 2 extra lists of references . </p>
<p>As a note , it would be vastly preferably to have an in-place search . </p>
<p>I have n't found a standard library for handling this. What is the typical solution to this problem ? </p>
<br /><h3>Answers ( Total-6 ) : </h3><b># 0 </b><br /><pre><code>filter(test1, filter(test2, filter(test3, the_list)))
</code></pre>
<p>Firstly , this is O ( n ) <span style="background-color:yellow;">time</span> , not O ( n ^ 3 ) <span style="background-color:yellow;">time.</span> The <span style="background-color:yellow;">time</span> adds not multiply. The only this could be worse then that is if test3/test2/test1 are doing something odd , in which we should look at those . </p>
<p>If we suggest that each test ? function takes 10 ms , then we have 10 * 3 * 10 ^ 5 ms = 50 minutes. If it was n ^ 3 , then we 'd have ( 10 * 10 ^ 5 ) ^ 3 = 31 million years. I 'm pretty sure you are only one linear <span style="background-color:yellow;">time</span> , you just have a ton of data . </p>
<p>Replace filter with <span style="background-color:yellow;">itertools.ifilter</span> , it 'll avoid generating the list. Instead , python will pull one item out of the list at a <span style="background-color:yellow;">time</span> , pass it through the three tests and give it to you if and only if it passes. It 'll avoid the memory requirement and probably be faster as well . </p>
<p>You are n't going to be able to improve on O ( n ) <span style="background-color:yellow;">time</span> unless you use some indexing techniques. However , the applicability of indexing techniques depends on what you are doing inside the test1/test2/test3 functions. If you want help on that , show an example for those functions . </p>
<p>As other have noted , database were designed to solve these problems. You can make this faster only be reimplementing badly what databases already do for you . </p>
<br /><b># 1 </b><br /><p>If the <span style="background-color:yellow;">objects</span> are unique within the set , and hashable , then you can create a set from them , and search the set . Creation should be O ( N ) ( N calls to a hashing function ) , and search is O ( 1 ) . Here 's an example with a list of <span style="background-color:yellow;">objects</span> holding a <span style="background-color:yellow;">string</span> each : </p>
<pre><code>class Dummy(object) :
   def __init__(self, val) :
      self.val=val

myList = [Dummy(str(val)) for val in xrange(100000)]
mySet = set(myList)
</code></pre>
<p>Now check if one of the elements is in the set : </p>
<pre><code>print myList[3] in mySet
>>> True
</code></pre>
<p>If you want quick look-up via a set of unique attributes of the element , you can use a dict and a tuple of these attributes as <span style="background-color:yellow;">key</span> , or just a single attribute as <span style="background-color:yellow;">key</span> if you have that : </p>
<pre><code>class Dummy(object) :
   def __init__(self, val, someUniqueKey) :
      self.val=val
      self.key=someUniqueKey

myList = ... # create the list of elements with unique keys
myDict = dict(zip([elem.key for elem in myList],myList)
</code></pre>
<p>Then access via the <span style="background-color:yellow;">key.</span> The problem here is that you have to make sure that the elements have a unique <span style="background-color:yellow;">key</span> that you can use to search ( i.e. it needs to have some meaning ) . </p>
<br /><b># 2 </b><br /><p>Concatenate the attribute values for each <span style="background-color:yellow;">object</span> to make unique <span style="background-color:yellow;">keys.</span> You may have to pad the attributes out to the same length to guarantee uniqueness. Construct a hash table to return the <span style="background-color:yellow;">object</span> that matches a <span style="background-color:yellow;">key</span> . </p>
<br /><b># 3 </b><br /><p>10 ^ 5 is not really that big a number of <span style="background-color:yellow;">objects</span> , even in-memory. <span style="background-color:yellow;">littletable</span> is a little module I wrote as an experiment for simulating queries , pivots , etc. using just Python dicts. One nice thing about <span style="background-color:yellow;">littletable</span> queries is that the result of any query or join is itself a new <span style="background-color:yellow;">littletable</span> <span style="background-color:yellow;">Table.</span> Indexes are <span style="background-color:yellow;">kept</span> as dicts of <span style="background-color:yellow;">keys-&gt</span> ; table <span style="background-color:yellow;">objects</span> , and index <span style="background-color:yellow;">keys</span> can be defined to be unique or not . </p>
<p>I created a table of 140K <span style="background-color:yellow;">objects</span> with 3 single letter <span style="background-color:yellow;">keys</span> , and then queried for a specific <span style="background-color:yellow;">key.</span> The <span style="background-color:yellow;">time</span> to build the table itself was the longest , the indexing and querying pretty fast . </p>
<pre><code>from itertools import product
from littletable import Table,DataObject

objects = Table()
alphas = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
alphas += alphas.lower()
import time

print "building table", time.time()
objects.insert_many(
    DataObject(k1=k1, k2=k2, k3=k3, created=time.time())
        for k1,k2,k3 in product(alphas.upper(),alphas,alphas)
    )
print "table complete", time.time()
print len(objects)

print "indexing table", time.time()
for k in "k1 k2 k3".split():
    objects.create_index(k)
print "index complete", time.time()

print "get specific row", time.time()
matches = objects.query(k1="X", k2="k", k3="W")
for o in matches:
    print o
print time.time()
</code></pre>
<p>Prints : </p>
<pre><code>building table 1309377011.63
table complete 1309377012.52
140608
indexing table 1309377012.52
index complete 1309377012.98
get specific row 1309377012.98
{'k3': 'W', 'k2': 'k', 'k1': 'X', 'created': 1309377011.9960001}
{'k3': 'W', 'k2': 'k', 'k1': 'X', 'created': 1309377012.4260001}
1309377013.0
</code></pre>
<br /><b># 4 </b><br /><p>It seems to me one typical solution would be to use a database query. Either SQL ( raw or with some <span style="background-color:yellow;">kind</span> of ORM ) , or some <span style="background-color:yellow;">kind</span> of <span style="background-color:yellow;">object</span> database , maybe MongoDB ? </p>
<br /><b># 5 </b><br /><p>If your data is in a CSV file , you could try sql2csv : https : //sourceforge.net/projects/sql2csv/ . </p>
<p>EDIT : Pardon my early-onset senility , I meant this project : https : //github.com/ccoffey/sql4csv/wiki/Examples . </p>
<br />