<h3>Question ( ID-563384 ) : </h3><h2>What 's the fastest way to test the validity of a large number of well-formed URLs </h2><p>My project requires me to validate a large number of web URLs. These URLs have been captured by a very unreliable process which I do not control. All of the URLs have already been regexp validated and are known to be well-formed. I also know that they all have valid TLDs </p>
<p>I want to be able to filter these URLs quickly in order to determine which of these are incorrect. At this point I do not care what content is on the pages - I 'd just like to know as quickly as possible which of the pages are inaccessible ( e.g. produce a 404 error ) . </p>
<p>Given that there are a lot of these I do not want to download the entire page , just the HTTP header and then take a good guess from the content of the header whether the page is likely to exist . </p>
<p>Can it be done ? </p>
<br /><h3>Answers ( Total-8 ) : </h3><b># 0 </b><br /><p>I 'm assuming you want to do it in Python based on your tags. In that case , I 'd use <span style="background-color:yellow;">httplib.</span> Optionally , somehow group the URLs by <span style="background-color:yellow;">host</span> so you can make multiple <span style="background-color:yellow;">requests</span> in one <span style="background-color:yellow;">connection</span> for those URLs that have the same <span style="background-color:yellow;">host.</span> Use the HEAD <span style="background-color:yellow;">request</span> . </p>
<pre><code>conn = httplib.HTTPConnection("example.com")
conn.request("HEAD", "/index.html")
resp = conn.getresponse()
print resp.status
</code></pre>
<br /><b># 1 </b><br /><p>To really make this fast you might also use <span style="background-color:yellow;">eventlet</span> which uses non-blocking IO to speed things up . </p>
<p>You can use a head <span style="background-color:yellow;">request</span> like this : </p>
<pre><code>from eventlet import httpc
try:
    res = httpc.head(url)
except httpc.NotFound:
    # handle 404
</code></pre>
<p>You can then put this into some simple script like that example script here . With that you should get pretty much concurrency by using a coroutines pool . </p>
<br /><b># 2 </b><br /><p>Using <span style="background-color:yellow;">httplib</span> and <span style="background-color:yellow;">urlparse</span> : </p>
<pre><code>def checkURL(url):
    import httplib
    import urlparse

    protocol, host, path, query, fragment = urlparse.urlsplit(url)

    if protocol == "http":
        conntype = httplib.HTTPConnection
    elif protocol == "https":
        conntype = httplib.HTTPSConnection
    else:
        raise ValueError("unsupported protocol: " + protocol)

    conn = conntype(host)
    conn.request("HEAD", path)
    resp = conn.getresponse()
    conn.close()

    if resp.status < 400:
        return true

    return false
</code></pre>
<br /><b># 3 </b><br /><p>Just send HTTP HEAD <span style="background-color:yellow;">requests</span> as shown in the accepted answer to this question . </p>
<br /><b># 4 </b><br /><p>Instead of sending an HTTP GET <span style="background-color:yellow;">request</span> for each URL you can try sending an HTTP HEAD <span style="background-color:yellow;">request.</span> They are described in this document . </p>
<br /><b># 5 </b><br /><p>This is a trivial case for twisted . There are a couple of concurrency tools you can use to slow it down , otherwise , it 'll pretty much do it all at once . </p>
<p>Twisted is definitely my favorite thing about python. : ) </p>
<br /><b># 6 </b><br /><p>This might help you to start. The file sitelist.txt contains a list of URIs. You might have to install <span style="background-color:yellow;">httplib2</span> , highly recommended. I put a sleep between each <span style="background-color:yellow;">request</span> so if you have many URIs on the same site , your client will not be blacklisted for abusing resources . </p>
<pre><code>   import httplib2
   import time

   h = httplib2.Http(".cache")

   f = open("sitelist.txt", "r")
   urllist = f.readlines()
   f.close()

   for url in urllist:
      # wait 10 seconds before the next request - be nice with the site
      time.sleep(10)
      resp= {}
      urlrequest = url.strip()
      try:
         resp, content = h.request(urlrequest, "HEAD")
         if resp['status'] == "200":
            print url, "200 - Good"
         else:
            print url, resp['status'], " you might want to double check"
      except:
         pass
</code></pre>
<br /><b># 7 </b><br /><p>A Python program which does a similar work ( for a list of URL stored at del.icio.us ) is disastrous . </p>
<p>And , yes , it uses HEAD and not GET but do note some ( not HTTP standard ) servers send different results for HEAD and for GET : the Python environment Zope is a typical culprit. ( Also , in some case , network problems , for instance tunnels + broken firewalls which block ICMP , prevent big packets to get through so HEAD works and not GET. ) </p>
<br />