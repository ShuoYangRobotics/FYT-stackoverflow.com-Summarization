<h3>Question ( ID-691148 ) : </h3><h2>Pythonic way to implement a tokenizer </h2><p>I 'm going to implement a tokenizer in Python and I was wondering if you could offer some style advice ? </p>
<p>I 've implemented a tokenizer before in C and in Java so I 'm fine with the theory , I 'd just like to ensure I 'm following pythonic styles and best practices . </p>
<p>Listing Token Types : </p>
<p>In Java , for example , I would have a list of fields like so : </p>
<pre><code>public static final int TOKEN_INTEGER = 0
</code></pre>
<p>But , obviously , there 's no way ( I think ) to declare a constant variable in Python , so I could just <span style="background-color:yellow;">replace</span> this with normal variable declarations but that does n't strike me as a great solution since the declarations could be altered . </p>
<p>Returning Tokens From The <span style="background-color:yellow;">Tokenizer</span> : </p>
<p>Is there a better alternative to just simply <span style="background-color:yellow;">returning</span> a list of tuples e.g . </p>
<pre><code>[ (TOKEN_INTEGER, 17), (TOKEN_STRING, "Sixteen")]?
</code></pre>
<p>Cheers, </p>
<p>Pete </p>
<br /><h3>Answers ( Total-11 ) : </h3><b># 0 </b><br /><p>Python takes a " we 're all consenting adults " approach to information hiding. It 's OK to use variables as though they were constants , and trust that users of your code wo n't do something stupid . </p>
<br /><b># <span style="background-color:yellow;">1</span> </b><br /><p>In many situations , exp. when parsing long input streams , you may find it more useful to implement you tokenizer as a generator function. This way you can easily iterate over all the tokens without the need for lots of memory to build the list of tokens first . </p>
<p>For generator see the original proposal or other online docs </p>
<br /><b># 2 </b><br /><p>There 's an undocumented class in the <span style="background-color:yellow;">re</span> module called <span style="background-color:yellow;">re.Scanner</span> . It 's very straightforward to use for a tokenizer : </p>
<pre><code>import re
scanner=re.Scanner([
  (r"[0-9]+",   	lambda scanner,token:("INTEGER", token)),
  (r"[a-z_]+",  	lambda scanner,token:("IDENTIFIER", token)),
  (r"[,.]+",    	lambda scanner,token:("PUNCTUATION", token)),
  (r"\s+", None), # None == skip token.
])

results, remainder=scanner.scan("45 pigeons, 23 cows, 11 spiders.")
print results
</code></pre>
<p>will <span style="background-color:yellow;">result</span> in </p>
<pre><code>[('INTEGER', '45'),
 ('IDENTIFIER', 'pigeons'),
 ('PUNCTUATION', ','),
 ('INTEGER', '23'),
 ('IDENTIFIER', 'cows'),
 ('PUNCTUATION', ','),
 ('INTEGER', '11'),
 ('IDENTIFIER', 'spiders'),
 ('PUNCTUATION', '.')]
</code></pre>
<p>I used <span style="background-color:yellow;">re.Scanner</span> to write a pretty nifty configuration/structured data format parser in only a couple hundred lines . </p>
<br /><b># 3 </b><br /><p>" Is there a better alternative to just simply <span style="background-color:yellow;">returning</span> a list of tuples ? " </p>
<p>Nope. It works <span style="background-color:yellow;">really</span> well . </p>
<br /><b># 4 </b><br /><p>Thanks for your help , I 've started to bring these ideas together , and I 've come up with the following. Is there anything terribly wrong with this implementation ( particularly I 'm concerned about passing a file <span style="background-color:yellow;">object</span> to the tokenizer ) : </p>
<pre><code>class Tokenizer(object):

  def __init__(self,file):
     self.file = file

  def __get_next_character(self):
      return self.file.read(1)

  def __peek_next_character(self):
      character = self.file.read(1)
      self.file.seek(self.file.tell()-1,0)
      return character

  def __read_number(self):
      value = ""
      while self.__peek_next_character().isdigit():
          value += self.__get_next_character()
      return value

  def next_token(self):
      character = self.__peek_next_character()

      if character.isdigit():
          return self.__read_number()
</code></pre>
<br /><b># 5 </b><br /><p>" Is there a better alternative to just simply <span style="background-color:yellow;">returning</span> a list of tuples ? " </p>
<p>That 's the approach used by the " tokenize " module for parsing Python <span style="background-color:yellow;">source</span> code. Returning a simple list of tuples can work very well . </p>
<br /><b># 6 </b><br /><p>I have <span style="background-color:yellow;">recently</span> built a tokenizer , too , and passed through some of your issues . </p>
<p>Token types are declared as " constants " , i.e. variables with ALL_CAPS names , at the module level. For example, </p>
<pre><code>_INTEGER = 0x0007
_FLOAT = 0x0008
_VARIABLE = 0x0009
</code></pre>
<p>and so on. I have used an underscore in front of the name to point out that somehow those fields are " private " for the module , but I <span style="background-color:yellow;">really</span> do n't know if this is typical or advisable , not even how much Pythonic. ( Also , I 'll probably ditch numbers in favour of strings , because during debugging they are much more <span style="background-color:yellow;">readable.</span> ) </p>
<p>Tokens are <span style="background-color:yellow;">returned</span> as named tuples . </p>
<pre><code>from collections import namedtuple
Token = namedtuple('Token', ['value', 'type'])
# so that e.g. somewhere in a function/method I can write...
t = Token(n, _INTEGER)
# ...and return it properly
</code></pre>
<p>I have used named tuples because the tokenizer 's client code ( e.g. the parser ) seems a little clearer while using names ( e.g. token.value ) instead of indexes ( e.g. token [ 0 ] ) . </p>
<p>Finally , I 've noticed that sometimes , especially writing tests , I prefer to pass a string to the tokenizer instead of a file <span style="background-color:yellow;">object.</span> I call it a " <span style="background-color:yellow;">reader</span> " , and have a specific method to open it and let the tokenizer access it through the same interface . </p>
<pre><code>def open_reader(self, source):
    """
    Produces a file object from source.
    The source can be either a file object already, or a string.
    """
    if hasattr(source, 'read'):
        return source
    else:
        from io import StringIO
        return StringIO(source)
</code></pre>
<br /><b># 7 </b><br /><p>When I start something new in Python I usually look first at some modules or libraries to use. There 's 90 % + chance that there already is somthing available . </p>
<p>For tokenizers and parsers this is certainly so. Have you looked at PyParsing ? </p>
<br /><b># 8 </b><br /><p>I 've implemented a tokenizer for a C-like programming language. What I did was to split up the creation of tokens into two layers : </p>
<ul>a surface <span style="background-color:yellow;">scanner</span> : This one actually <span style="background-color:yellow;">reads</span> the text and uses <span style="background-color:yellow;">regular</span> expression to split it up into only the most primitve tokens ( operators , identifiers , numbers,... ) ; this one yields tuples ( tokenname , scannedstring , startpos , endpos ) . a tokenizer : This consumes the tuples from the first layer , turning them into token <span style="background-color:yellow;">objects</span> ( named tuples would do as well , I think ) . Its purpose is to detect some long-range dependencies in the token stream , particularly strings ( with their opening and closing quotes ) and comments ( with their opening an closing lexems ; - yes , I wanted to <span style="background-color:yellow;">retain</span> comments ! ) and coerce them into single tokens. The <span style="background-color:yellow;">resulting</span> stream of token <span style="background-color:yellow;">objects</span> is then <span style="background-color:yellow;">returned</span> to a consuming parser . </ul>
<p>Both are generators. The benefits of this approach were : </p>
<ul>Reading of the raw text is done only in the most primitive way , with simple <span style="background-color:yellow;">regexps</span> - fast and clean . The second layer is already implemented as a primitive parser , to detect string literals and comments - <span style="background-color:yellow;">re-use</span> of parser technology . You do n't have to strain the surface <span style="background-color:yellow;">scanner</span> with complex detections . But the <span style="background-color:yellow;">real</span> parser gets tokens on the semantic level of the language to be parsed ( again strings , comments ) . </ul>
<p>I feel quite happy with this layered approach . </p>
<br /><b># 9 </b><br /><p>" Is there a better alternative to just simply <span style="background-color:yellow;">returning</span> a list of tuples " </p>
<p>I had to implement a tokenizer , but it <span style="background-color:yellow;">required</span> a more complex approach than a list of tuples , therefore I implemented a class for each token. You can then <span style="background-color:yellow;">return</span> a list of class instances , or if you want to save <span style="background-color:yellow;">resources</span> , you can <span style="background-color:yellow;">return</span> something implementing the iterator interface and generate the next token while you progress in the parsing . </p>
<br /><b># <span style="background-color:yellow;">10</span> </b><br /><p>I 'd turn to the excellent Text Processing in Python by David Mertz </p>
<br />