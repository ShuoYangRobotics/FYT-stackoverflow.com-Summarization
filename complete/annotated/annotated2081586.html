<h3>Question ( ID-2081586 ) : </h3><h2>Web scraping with Python </h2><p>I 'd like to grab daily sunrise/sunset times from here . Is it possible to scrape web content with Python ? what are the modules used ? Is there any tutorial available ? </p>
<p>Thanks </p>
<br /><h3>Answers ( Total-7 ) : </h3><b># 0 </b><br /><p>To help you choose from the various modules available , I think this talk ( at pycon 2009 ) , is what you 're looking for ( the guy has lots of experience on the matter ) . And he points out the good and the bad from most of available modules . </p>
<ul>Scrape the Web : Strategies for programming websites that do n't expect it ( Part 1 of 3 ) Scrape the Web : Strategies for programming websites that do n't expect it ( Part 2 of 3 ) Scrape the Web : Strategies for programming websites that do n't expect it ( Part 3 of 3 ) </ul>
<p>From the PYCON 2009 schedule : </p>
<blockquote>Do you find yourself faced with websites that have data you need to extract ? Would your life be simpler if you could programmatically input data into web applications , even those tuned to resist interaction by bots ? We 'll discuss the basics of web scraping , and then dive into the details of different methods and where they are most applicable . You 'll leave with an understanding of when to apply different tools , and learn about a " heavy hammer " for screen scraping that I picked up at a project for the Electronic Frontier Foundation . Atendees should bring a laptop , if possible , to try the examples we discuss and optionally take notes . </blockquote>
<p>Update : </p>
<p>Asheesh Laroia has updated his presentation for pycon 2010 </p>
<ul>PyCon 2010 : Scrape the Web : Strategies for programming websites that do n't expected it * My motto : " The website is the API. " * Choosing a parser : <span style="background-color:yellow;">BeautifulSoup</span> , lxml , HTMLParse , and html5lib . * Extracting information , even in the face of bad HTML : Regular expressions , <span style="background-color:yellow;">BeautifulSoup</span> , SAX , and XPath . * Automatic template reverse-engineering tools . * Submitting to forms . * Playing with XML-RPC * DO NOT BECOME AN EVIL COMMENT SPAMMER . * Countermeasures , and circumventing them : o IP address limits o Hidden form fields o User-agent detection o JavaScript o CAPTCHAs * Plenty of full source code to working examples : o Submitting to forms for text-to-speech . o Downloading music from web stores . o Automating Firefox with Selenium RC to navigate a pure-JavaScript service . * Q&amp ; A ; and workshopping * Use your power for good , not evil . </ul>
<br /><b># 1 </b><br /><p>just use <span style="background-color:yellow;">urllib2</span> in combination with the brilliant <span style="background-color:yellow;">BeautifulSoup</span> library : </p>
<pre><code>
import urllib2
from BeautifulSoup import BeautifulSoup

soup = BeautifulSoup(urllib2.urlopen('http://www.timeanddate.com/worldclock/astronomy.html?n=78').read())

for row in soup('table', {'class' : 'spad'})[0].tbody('tr'):
  tds = row('td')
  print tds[0].string, tds[1].string
  # will print date and sunrise
</code></pre>
<br /><b># 2 </b><br /><p>You can use <span style="background-color:yellow;">urllib2</span> to make the HTTP requests , and then you 'll have web content . </p>
<p>You can get it like this : </p>
<pre><code>import urllib2
response = urllib2.urlopen('http://www.timeanddate.com/worldclock/astronomy.html?n=78')
html = response.read()
</code></pre>
<p>Beautiful Soup is a python HTML parser that is supposed to be good for screen scraping . </p>
<p>In particular , here is their tutorial on parsing an HTML document . </p>
<p>Good luck ! </p>
<br /><b># 3 </b><br /><p>Python has several options for web scraping . I enumerated some of the options here in response to a similar question . </p>
<br /><b># 4 </b><br /><p>I 'd really recommend Scrapy , for reasons being elaborated in this question - " Is it worth learning Scrapy ? " . </p>
<p>Quote from the answer : </p>
<blockquote>Scrapy crawling is fastest than mechanize because uses asynchronous operations ( on top of Twisted ) . Scrapy has better and fastest support for parsing ( x ) html on top of libxml2 . Scrapy is a mature framework with full unicode , handles redirections , gzipped responses , odd encodings , integrated http cache , etc . Once you are into Scrapy , you can write a spider in less than 5 minutes that <span style="background-color:yellow;">download</span> images , creates thumbnails and export the extracted data directly to csv or json . </blockquote>
<br /><b># 5 </b><br /><p>All this is cool but how do we interact with the target web page to get it to the real place we want to extract the data from. Many web pages encode session ids within their dynamically-generated urls , for example. As well , many pages can sense whether the visitor is a bot or a real person ( I do n't know how ) - so what do we do ? </p>
<br /><b># 6 </b><br /><p>I collected together scripts from my web scraping work into this library . </p>
<p>Example script for your case : </p>
<pre><code>from webscraping import download, xpath
D = download.Download()

html = D.get('http://www.timeanddate.com/worldclock/astronomy.html?n=78')
for row in xpath.search(html, '//table[@class="spad"]/tbody/tr'):
    cols = xpath.search(row, '/td')
    print 'Sunrise: %s, Sunset: %s' % (cols[1], cols[2])
</code></pre>
<p>Output : </p>
<pre><code>Sunrise: 08:39, Sunset: 16:08
Sunrise: 08:39, Sunset: 16:09
Sunrise: 08:39, Sunset: 16:10
Sunrise: 08:40, Sunset: 16:10
Sunrise: 08:40, Sunset: 16:11
Sunrise: 08:40, Sunset: 16:12
Sunrise: 08:40, Sunset: 16:13
</code></pre>
<br />