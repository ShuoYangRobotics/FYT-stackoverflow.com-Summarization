Question (ID-1538589): How to sort all possible words out of a string? I'm wondering how to proceed with this task, take this string for example "thingsandstuff". 

 How could I generate all possible strings out of this string as to look them up individually against an english dictionary? 

 The goal is to find valid english words in a string that does not contain space. 

 Thanks 
 Answers (Total-11): #0 People talk about this as though the Order of the problem is the number of possible substrings. This is incorrect. The correct order of this problem is: 

 O( min ( number-of-words-in-dict, number-of-substring-combinations) * comparison_cost) 

 So, another approach to the problem, to build on Vinko, is to index the heck out of the dictionary (e.g., for each work in the dict, determine the letters in that word, the length of the word, etc). This can speed things up dramatically. As an example, we know that target "queen" can't match "zebra" (no z's!) ( or any word containing z,r,b,a...), and the like. Also, store each word in the dict as a sorted string ('zebra' -> 'aberz') and do "string in string" (longest common substring) matching. 'eenuq' vs 'abarz' (no match). 

 (Note: I am assuming that the order of the letters in the original word don't matter -- it's a 'bag of letters', if they do, then adjust accordingly) 

 If you have lots of words to compare at once, the comparison cost can be lowered further using something like KMP . 

 (Also, I dove right in, and made some assumptions that Alex didn't, so if they're wrong, then shut my mouth!) 
 #1 Another possibility is going the other way around, instead of generating substrings from a string, grab all your candidate words and match them against your string. 

 You can store as a result (start,end) pairs of indexes of the words in the original string. 

 This could be easily done in regex, or, if not performant enough, with str.find(), or if even not performant enough with more complex dictionary index schemes or smarts about what can and can't match (see Gregg's answer for ideas) 

 Here you have a sample of what I mean 

 candidate = "thingsandstuffmydarlingpretty"
words = file('/usr/share/dict/words').read()
#This generator calls find twice, it should be rewritten as a normal loop
generate_matches = ((candidate.find(word),word) for word in words.split('\n')
      if candidate.find(word) != -1 and word != '')

for match in generate_matches:
 print "Found %s at (%d,%d)" % (match[1],match[0],match[0] + len(match[1]))
 
 #2 The brute force approach, i.e. checking every substring, is computationally unfeasible even for strings of middling lengths (a string of length N has O(N**2) substrings). Unless there is a pretty tight bound on the length of strings you care about, that doesn't scale well. 

 To make things more feasible, more knowledge is required -- are you interested in overlapping words (eg 'things' and 'sand' in your example) and/or words which would leave unaccounted for characters (eg 'thing' and 'and' in your example, leaving the intermediate 's' stranded), or you do you want a strict partition of the string into juxtaposed (not overlapping) words with no residue? 

 The latter would be the simplest problem, because the degrees of freedom drop sharply -- essentially to trying to determine a sequence of "breaking points", each between two adjacent characters, that would split the string into words. If that's the case, do you need every possible valid split (i.e. do you need both "thing sand" and "things and"), or will any single valid split do, or are there criteria that your split must optimize? 

 If you clarify all of these issues it may be possible to give you more help! 
 #3 norving wrote a great article on how to write a spell checker in python. 

 http://norvig.com/spell-correct.html 

 it will give you an good idea on how to detect words. (i.e. just go testing each group of chars until you get a valid word... beware that to be deterministic you would need to do the reverse. Test all the string, and then go removing chars at the end. that way you get composite words as they are intended... or not intended, who knows. spaces have a reason :) 

 after that, it's basic CS 101. 
 #4 Well here is my idea 

 
 Find all possible strings containing 1 character from the original 
 Find all possible strings containing 2 characters from the original 
 ... Same thing up to the length of the original string 
 

 Then add all up and go match with your dictionary 
 #5 What if you break it up into syllables and then use those to construct words to compare to your dictionary. It's still a brute force method, but it would surely speed things up a bit. 
 #6 I looked at a powerset implementation. Too many possibilities. 

 Try encoding your string and all candidates from your dictionary and see if the candidate from the dictionary could be made from the candidate string. That is, do the letters in the dictionary word appear no more frequently than in your candidate string? 

 from __future__ import with_statement
import collections

def word_dict(word):
 d = collections.defaultdict(int)
 for c in word:
 	d[c] += 1
 return d

def compare_word_dict(dict_cand, cand):
 return all(dict_cand[k] &lt;= cand[k] for k in dict_cand)


def try_word(candidate):
 s = word_dict(candidate)
 dictionary_file = r"h:\words\WORDs(3).txt"
 i = 0
 with open(dictionary_file) as f:
 	for line in f:
 		line = line.strip()
 		dc = word_dict(line)
 		if compare_word_dict(dc,s):
 			print line
 			i += 1
 return i


print try_word("thingsandstuff")
 

 I get 670 words with my dictionary. Seems a bit small. Takes about 3 seconds on 200k words in the dictionary. 

 This works for python 2.5 and above because of the addition of collections.defaultdict . In python 3.1, collections.Counter was added that works like collections.defaultdict(int). 
 #7 Take a look to this post , it addresses the same problem, both in Python and OCaml, with a solution based in normalizing the strings first instead of doing brute-force search. 

 By the way, the automatic translation removes the indenting, so to get the working Python code you should look at the untranslated Spanish version (that in fact is much proper than the crappy English generated by Google translator)... 

 Edit: 

 Re-reading your question, I understand now that you could want only those words that are unscrambled, right? If so, you don't need to do all the stuff described in the post, just: 

 maxwordlength = max(map(len, english_words))
for i in range(len(word)):
 for j in range(i+1, min(maxwordlength+i, len(word))):
   if word[i:j] in english_words:
    print word[i:j]
 

 The complexity should be O(N) now, given that the size of the largest word in English is finite. 
 #8 Code: 

 def all_substrings(val):
 return [val[start:end] for start in range(len(val)) for end in range(start + 1, len(val))]

val = "thingsandstuff"
for result in all_substrings(val):
 print result
 

 Output: 

 t
th
thi
thin
thing
 

 [ ... ] 

 tu
tuf
u
uf
f
 
 #9 This will find whether or not a candidate can be formed out of the letters in a given word; it's assumed that word (but not candidate ) is sorted prior to the call. 

 &gt;&gt;&gt; def match(candidate, word):

  def next_char(w):
   for ch in sorted(w):
    yield ch

  g = next_char(word)
  for cl in sorted(candidate):
   try:
    wl = g.next()
   except StopIteration:
    return False
   if wl &gt; cl:
    return False
   while wl &lt; cl:
    try:
     wl = g.next()
    except StopIteration:
     return False
    if wl &gt; cl:
     return False
  return True

&gt;&gt;&gt; word = sorted("supernatural")
&gt;&gt;&gt; dictionary = ["super", "natural", "perturb", "rant", "arrant"]
&gt;&gt;&gt; for candidate in dictionary:
  print candidate, match(candidate, word)

super True
natural True
perturb False
rant True
arrant True
 

 When I load the BSD words file (235,000+ words) and run this using plenipotentiary as my word, I get about 2500 hits in under a second and a half. 

 If you're going to run many searches, it's a good idea to remove the sort from next_char , build a dictionary keyed on the sorted version of each word - 

 d = dict([(sorted(word), word) for word in dictionary])
 

 and produce results via logic like this: 

 result = [d[k] for k in d.keys() if match(k, word)]
 

 so that you have to perform 250,000+ sorts over and over again. 
 #10 If you know the full dictionary well in advance, and it doesn't change between searches, you might try the following... 

 Index the dictionary. Each word (e.g. "hello") becomes a (key, data) tuple such as ("ehllo", "hello"). In the key, the letters are sorted alphabetically. 

 Good index data structures would include a trie (aka digital tree) or a ternary tree . A conventional binary tree could be made to work. A hash table wouldn't work. I'm going to assume a trie or a ternary tree. Note - the data structure must act as a multimap (you probably need a linked list of matched data items at each key-matched leaf). 

 Before evaluating for a particular string, sort the letters in the string. Then do a key search in the data structure. BUT a simple key search will only find words that use all letters from the original string. 

 Basically, a trie search matches one letter at a time, choosing a child node based on the next letter of the input. However, at each step, we have an extra option - skip a letter of the sorted input string and remain at the same node (ie, don't use that letter in the output). The obvious thing to do is a depth-first backtracking search. Note that both our keys and our input have the letters sorted, so we can probably optimise the search a bit. 

 A ternary tree version follows similar principles to a trie, but instead of multiple children per node, you basically have next-letter binary tree logic built into the structure. The search can be easily adapted - the options for each next-letter search being match the next input letter or discard it. 

 When you get runs of the same letter in the sorted input string, the 'skip a letter' option in the search should be 'skip to the next different letter'. Otherwise, you end up doing duplicate searches (during backtracking) - e.g. there are 3 different ways to use two out of three duplicate letters - you could ignore the first, the second, or the third duplicate - and you only need to check one case. 

 Optimisations might have extra details in the data structure nodes in order to help prune the search tree. E.g. keeping the maximum length of word tails in the subtree allows you to check whether your remaining part of your search string contains enough letters to bother continuing the search. 

 Time complexity isn't immediately obvious due to the backtracking.