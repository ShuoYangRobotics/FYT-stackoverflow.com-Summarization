Question (ID-1632902): lambda versus list comprehension performance I recently posted a question using a lambda function and in a reply someone had mentioned lambda is going out of favor, to use list comprehensions instead. I am relatively new to Python. I ran a simple test: 

 import time

S=[x for x in range(1000000)]
T=[y**2 for y in range(300)]
#
#
time1 = time.time()
N=[x for x in S for y in T if x==y]
time2 = time.time()
print 'time diff [x for x in S for y in T if x==y]=', time2-time1
#print N
#
#
time1 = time.time()
N=filter(lambda x:x in S,T)
time2 = time.time()
print 'time diff filter(lambda x:x in S,T)=', time2-time1
#print N
#
#
#http://snipt.net/voyeg3r/python-intersect-lists/
time1 = time.time()
N = [val for val in S if val in T]
time2 = time.time()
print 'time diff [val for val in S if val in T]=', time2-time1
#print N
#
#
time1 = time.time()
N= list(set(S) &amp; set(T))
time2 = time.time()
print 'time diff list(set(S) &amp; set(T))=', time2-time1
#print N #the results will be unordered as compared to the other ways!!!
#
#
time1 = time.time()
N=[]
for x in S:
 for y in T:
  if x==y:
   N.append(x)
time2 = time.time()
print 'time diff using traditional for loop', time2-time1
#print N
 

 They all print the same N so I commented that print stmt out (except the last way it's unordered), but the resulting time differences were interesting over repeated tests as seen in this one example: 

 time diff [x for x in S for y in T if x==y]= 54.875
time diff filter(lambda x:x in S,T)= 0.391000032425
time diff [val for val in S if val in T]= 12.6089999676
time diff list(set(S) &amp; set(T))= 0.125
time diff using traditional for loop 54.7970001698
 

 So while I find list comprehensions on the whole easier to read, there seems to be some performance issues at least in this example. 

 So, two questions: 

 
 Why is lambda etc being pushed aside? 
 For the list comprehension ways, is there a more efficient implementation and how would you KNOW it's more efficient without testing? I mean, lambda/map/filter was supposed to be less efficient because of the extra function calls, but it seems to be MORE efficient. 
 

 Paul 
 Answers (Total-10): #0 Your tests are doing very different things. With S being 1000000 elements and T being 300: 

 [x for x in S for y in T if x==y]= 54.875 

 This option does 300000000 equality comparisons. 

 filter(lambda x:x in S,T)= 0.391000032425 

 This option does 300 linear searches through S. 

 [val for val in S if val in T]= 12.6089999676 

 This option does 1000000 linear searches through T. 

 list(set(S) &amp; set(T))= 0.125 

 This option does two set constructions and one set intersection. 

 

 The differences in performance between these options is much more related to the algorithms each one is using, rather than any difference between list comprehensions and lambda . 
 #1 When I fix your code so that the list comprehension and the call to filter are actually doing the same work things change a whole lot: 

 import time

S=[x for x in range(1000000)]
T=[y**2 for y in range(300)]
#
#
time1 = time.time()
N=[x for x in T if x in S]
time2 = time.time()
print 'time diff [x for x in T if x in S]=', time2-time1
#print N
#
#
time1 = time.time()
N=filter(lambda x:x in S,T)
time2 = time.time()
print 'time diff filter(lambda x:x in S,T)=', time2-time1
#print N
 

 Then the output is more like: 

 time diff [x for x in T if x in S]= 0.414485931396
time diff filter(lambda x:x in S,T)= 0.466315984726
 

 So the list comprehension has a time that's generally pretty close to and usually less than the lambda expression. 

 The reason lambda expressions are being phased out is that many people think they are a lot less readable than list comprehensions. I sort of reluctantly agree. 
 #2 Q: Why is lambda etc being pushed aside? 

 A: List comprehensions and generator expressions are generally considered to be a nice mix of power and readability. The pure functional-programming style where you use map() , reduce() , and filter() with functions (often lambda functions) is considered not as clear. Also, Python has added built-in functions that nicely handle all the major uses for reduce() . 

 Suppose you wanted to sum a list. Here are two ways of doing it. 

 lst = range(10)
print reduce(lambda x, y: x + y, lst)

print sum(lst)
 

 Sign me up as a fan of sum() and not a fan of reduce() to solve this problem. Here's another, similar problem: 

 lst = range(10)
print reduce(lambda x, y: bool(x or y), lst)

print any(lst)
 

 Not only is the any() solution easier to understand, but it's also much faster; it has short-circuit evaluation, such that it will stop evaluating as soon as it has found any true value. The reduce() has to crank through the entire list. This performance difference would be stark if the list was a million items long, and the first item evaluated true. By the way, any() was added in Python 2.5; if you don't have it, here is a version for older versions of Python: 

 def any(iterable):
 for x in iterable:
  if x:
   return True
 return False
 

 Suppose you wanted to make a list of squares of even numbers from some list. 

 lst = range(10)
print map(lambda x: x**2, filter(lambda x: x % 2 == 0, lst))

print [x**2 for x in lst if x % 2 == 0]
 

 Now suppose you wanted to sum that list of squares. 

 lst = range(10)
print sum(map(lambda x: x**2, filter(lambda x: x % 2 == 0, lst)))

# list comprehension version of the above
print sum([x**2 for x in lst if x % 2 == 0])

# generator expression version; note the lack of '[' and ']'
print sum(x**2 for x in lst if x % 2 == 0)
 

 The generator expression actually just returns an iterable object. sum() takes the iterable and pulls values from it, one by one, summing as it goes, until all the values are consumed. This is the most efficient way you can solve this problem in Python. In contrast, the map() solution, and the equivalent solution with a list comprehension inside the call to sum() , must first build a list; this list is then passed to sum() , used once, and discarded. The time to build the list and then delete it again is just wasted. (EDIT: and note that the version with both map and filter must build two lists, one built by filter and one built by map ; both lists are discarded.) (EDIT: But in Python 3.0 and newer, map() and filter() are now both "lazy" and produce an iterator instead of a list; so this point is less true than it used to be. Also, in Python 2.x you were able to use itertools.imap() and itertools.ifilter() for iterator-based map and filter. But I continue to prefer the generator expression solutions over any map/filter solutions.) 

 By composing map() , filter() , and reduce() in combination with lambda functions, you can do many powerful things. But Python has idiomatic ways to solve the same problems which are simultaneously better performing and easier to read and understand. 
 #3 First of all, test like this: 

 import timeit

S=[x for x in range(10000)]
T=[y**2 for y in range(30)]

print "v1", timeit.Timer('[x for x in S for y in T if x==y]',
 		 'from __main__ import S,T').timeit(100)
print "v2", timeit.Timer('filter(lambda x:x in S,T)',
 		 'from __main__ import S,T').timeit(100)
print "v3", timeit.Timer('[val for val in T if val in S]',
 		 'from __main__ import S,T').timeit(100)
print "v4", timeit.Timer('list(set(S) &amp; set(T))',
 		 'from __main__ import S,T').timeit(100)
 

 And basically you are doing different things each time you test. When you would rewrite the list-comprehension for example as 

 [val for val in T if val in S]
 

 performance would be on par with the 'lambda/filter' construct. 
 #4 Sets are the correct solution for this. However try swapping S and T and see how long it takes! 

 filter(lambda x:x in T,S)

$ python -m timeit -s'S=[x for x in range(1000000)];T=[y**2 for y in range(300)]' 'filter(lambda x:x in S,T)'
10 loops, best of 3: 485 msec per loop
$ python -m timeit -r1 -n1 -s'S=[x for x in range(1000000)];T=[y**2 for y in range(300)]' 'filter(lambda x:x in T,S)'
1 loops, best of 1: 19.6 sec per loop
 

 So you see that the order of S and T are quite important 

 Changing the order of the list comprehension to match the filter gives 

 $ python -m timeit -s'S=[x for x in range(1000000)];T=[y**2 for y in range(300)]' '[x for x in T if x in S]'
10 loops, best of 3: 441 msec per loop
 

 So if fact the list comprehension is slightly faster than the lambda on my computer 
 #5 Your list comprehension and lambda are doing different things, the list comprehension matching the lambda would be [val for val in T if val in S] . 

 Efficiency isn't the reason why list comprehension are preferred (while they actually are slightly faster in almost all cases). The reason why they are preferred is readability. 

 Try it with smaller loop body and larger loops, like make T a set, and iterate over S. In that case on my machine the list comprehension is nearly twice as fast. 
 #6 Your profiling is done wrong. Take a look the timeit module and try again. 

 lambda defines anonymous functions. Their main problem is that many people don't know the whole python library and use them to re-implement functions that are already in the operator , functools etc module ( and much faster ). 

 List comprehensions have nothing to do with lambda . They are equivalent to the standard filter and map functions from functional languages. LCs are preferred because they can be used as generators too, not to mention readability. 
 #7 Many people have already pointed out that you're comparing apples with oranges, etc, etc. But I think nobody's shown how to a really simple comparison -- list comprehension vs map plus lambda with little else to get in the way -- and that might be: 

 $ python -mtimeit -s'L=range(1000)' 'map(lambda x: x+1, L)'
1000 loops, best of 3: 328 usec per loop
$ python -mtimeit -s'L=range(1000)' '[x+1 for x in L]'
10000 loops, best of 3: 129 usec per loop
 

 Here, you can see very sharply the cost of lambda -- about 200 microseconds, which in the case of a sufficiently simple operation such as this one swamps the operation itself. 

 Numbers are very similar with filter of course, since the problem is not filter or map, but rather the lambda itself: 

 $ python -mtimeit -s'L=range(1000)' '[x for x in L if not x%7]'
10000 loops, best of 3: 162 usec per loop
$ python -mtimeit -s'L=range(1000)' 'filter(lambda x: not x%7, L)'
1000 loops, best of 3: 334 usec per loop
 

 No doubt the fact that lambda can be less clear, or its weird connection with Sparta (Spartans had a Lambda, for "Lakedaimon", painted on their shields -- this suggests that lambda is pretty dictatorial and bloody;-) have at least as much to do with its slowly falling out of fashion, as its performance costs. But the latter are quite real. 
 #8 This is pretty fast: 

 def binary_search(a, x, lo=0, hi=None):
 if hi is None:
  hi = len(a)
 while lo &lt; hi:
  mid = (lo+hi)//2
  midval = a[mid]
  if midval &lt; x:
   lo = mid+1
  elif midval &gt; x: 
   hi = mid
  else:
   return mid
 return -1

time1 = time.time()
N = [x for x in T if binary_search(S, x) &gt;= 0]
time2 = time.time()
print 'time diff binary search=', time2-time1
 

 Simply: less comparisions, less time. 
 #9 List comprehensions can make a bigger difference if you have to process your filtered results. In your case you just build a list, but if you had to do something like this: 

 n = [f(i) for i in S if some_condition(i)]
 

 you would gain from LC optimization over this: 

 n = map(f, filter(some_condition(i), S))
 

 simply because the latter has to build an intermediate list (or tuple, or string, depending on the nature of S). As a consequence you will also notice a different impact on the memory used by each method, the LC will keep lower. 

 The lambda in itself does not matter.