Question (ID-4220601): Writing a small yet flexible HTTP client I'm looking to find out how people would go about writing a quick (small) yet flexible HTTP client. By quick I mean not much code, (I'll leave it up to you to decide what that means), and preferably using built-in language functions as opposed to downloaded or custom libraries, such that a basic knowledge of socket programming should be sufficient to understand how the code works. By flexible I mean that you should be able to manipulate the incoming data easily. My own version of something like this would be 

 #!/usr/bin/perl
use Socket;
use HTML::Parse;

socket(SOCKH, PF_INET, SOCK_STREAM, getprotobyname('tcp')) || die $!;
connect(SOCKH,sockaddr_in(80,inet_aton('www.example.com'))) || die $!;

$old_fh = select(SOCKH);
$|=1;
select($old_fh);

print SOCKH "GET / HTTP/1.0\n\n";

while (&lt;SOCKH&gt;) {
 $response .= $_;
}

print parse_html($response)-&gt;format;

close(&lt;SOCKH&gt;);
 

 This is just a quick client which I'll be editing for HTTP/1.1 shortly, or if you have suggestions on how to better it for compliancy please share! 

 EDIT: An update to my code, using LWP as Sinan Unur suggested: 

 #!/usr/bin/perl
use LWP::Simple;
use HTML::Parse;

$data = parse_html( get( 'www.example.com' ) )-&gt;format;
foreach $line ( $data ) {
 print $line; // or any other line-based operation
}
 
 Answers (Total-7): #0 Perl has LWP . I suggest you use it. 
 #1 Similar to @Santa's example, only in Ruby: 

 require 'open-uri'
print open('http://www.example.com').read
 

 If you want to parse the content, Ruby's Nokogiri gem is awesome. It's built on top of libXML. 

 Many other HTTP client gems are available, including HTTParty and Typhoeus . HTTParty makes it trivial to add REST capability to a class, along with the ability to parse JSON and XML. Typhoeus makes it easy to write multiple requests all at once for parallel gets/heads/whatevers. 
 #2 Python has "batteries included" 

 You don't need to work at the socket level (although you can). Python has several higher level web/http libraries built in its standard library. 

 for example, in Python 2, you can use urllib2 : 

 import urllib2

response = urllib2.urlopen('http://www.example.com/')
html = response.read()
 

 also check out httplib , for slightly lower level access: 

 &gt;&gt;&gt; import httplib
&gt;&gt;&gt; conn = httplib.HTTPConnection("www.python.org")
&gt;&gt;&gt; conn.request("GET", "/index.html")
&gt;&gt;&gt; r1 = conn.getresponse()
&gt;&gt;&gt; print r1.status, r1.reason
200 OK
&gt;&gt;&gt; data1 = r1.read()
&gt;&gt;&gt; conn.request("GET", "/parrot.spam")
&gt;&gt;&gt; r2 = conn.getresponse()
&gt;&gt;&gt; print r2.status, r2.reason
404 Not Found
&gt;&gt;&gt; data2 = r2.read()
&gt;&gt;&gt; conn.close()
 
 #3 #!/usr/bin/env python

import urllib

f = urllib.urlopen('http://www.example.com')
print f.read()
f.close()
 
 #4 Plenty of examples seem trivial until you start needing to do anything heavy, like streaming gigabytes of data in both directions. My favorite is ruby's net/http , which is part of every ruby install. 

 Here's an example that streams the data, and leaves the connection open when you're finished for more requests. 

 require 'net/http'

http = Net::HTTP.new('my.server.org')
http.start

req = Net::HTTP::Post.new('/path/to/huge/file')
req.basic_auth('user', 'pass')
req.content_length = File.size 'huge_input_file.txt'
req.body_stream = File.open('huge_input_file.txt', 'rb')

http.request(req){|res|
 File.open('huge_output_file.txt', 'wb'){|f|
 res.read_body {|chunk| f.write chunk}
 }
}
 

 

 Perl doesn't have anything built-in. Net::HTTP and LWP::UserAgent are not part of perl core. A while back I needed a perl http client that functions like the above ruby example, without depending on anything from CPAN. It took under 200 lines, using only IO::Socket::INET and syswrite / sysread (which eliminates a lot of the lower-layer socket nonsense), and MIME::Base64 for authentication. 
 #5 I take it that you don't actually want to write your own http client, but rather a script that fetches stuff using http? 

 Others mentioned HTTP::Lite and LWP::Simple .. 

 Here's an example using Mojo::Client , from the docs. 

 # Quick JSON request
my $trends = 'http://search.twitter.com/trends.json';
print $client-&gt;get($trends)-&gt;res-&gt;json-&gt;{trends}-&gt;[0]-&gt;{name};

# Extract data from HTML and XML resources
print $client-&gt;get('mojolicious.org')-&gt;res-&gt;dom-&gt;at('title')-&gt;text;

# Scrape the latest headlines from a news site
my $news = 'http://digg.com';
$client-&gt;max_redirects(3);
$client-&gt;get($news)-&gt;res-&gt;dom('h3 &gt; a.story-title')-&gt;each(sub {
 print shift-&gt;text . "\n";
});
 

 Cool, but not the most mature module available.. 
 #6 I can't see what possible benefit you would get from any answers to such a question, but I'll give in to the peer pressure. Since you don't seem to bothered which language to use... 

 If you want to manipulate it in interesting ways: 

 &lt;?php
$d=new DOMDocument();
$d-&gt;loadHTMLFile('http://www.example.com/');
...
 

 But to just return a page: 

 &lt;?php
print file_get_contents('http://www.example.com/');