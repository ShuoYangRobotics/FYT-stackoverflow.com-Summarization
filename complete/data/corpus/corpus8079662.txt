Question (ID-8079662): How to speed up Python execution? I have a project written in Python which work with a big size of data.
I would like to speed up the execution time. 

 In simple words, let's say I have this sample fully optimized code: 

 def foo(x):
 doSomething

main():
 for i in range(1,10000000):
  foo(i)
 

 Is there some way to speed up this? For example by using multiprocessing or other stuff?
And most important, is it worth? 

 Thanks for replies. I think I will try the route of multiprocessing. Someone can suggest me a guide or some example for helping me? 
 Answers (Total-6): #0 The only real way to know would be to profile and measure. Your code could be doing anything. "doSomething" might be a time.sleep(10) in which case, forking off 10000000 processes would make the whole program run in approximately 10 seconds (ignoring the forking overhead and resulting slowdowns). 

 Use http://docs.python.org/library/profile.html and check to see where the bottle necks are, see if you can optimise the "fully optimised" program using better coding. If it's already fast enough, stop. 

 Then, depending on whether it's CPU or I/O bound and the hardware you have, you might want to try multiprocessing or threading. You can also try distributing to multiple machines and doing some map/reduce kind of thing if your problem can be broken down. 
 #1 To answer your last question first, if you have a problem with performance, then it's worth it. That's the only criterion, really. 

 As for how: 

 If your algorithm is slow because it's computationally expensive, consider rewriting it as a C extension , or use Cython , which will let you write fast extensions in a Python-esque language. Also, PyPy is getting faster and faster and may just be able to run your code without modification. 

 If the code is not computationally expensive, but it just loops a huge amount, it may be possible to break it down with Multiprocessing, so it gets done in parallel. 

 Lastly, if this is some kind of basic data splatting task, consider using a fast data store. All the major relational databases are optimised up the wazoo, and you may find that your task can be sped up simply by getting the database to do it for you. You may even be able to shape it to fit a Redis store, which can aggregate big data sets brilliantly. 
 #2 You could use the PyPy interpreter which has a JIT compiler built into it, it might actually improve performance over loops like this. Here is a link that explains some of the speed ups that the PyPy interpreter offers over regular CPython. 

 Or you could write your code using Cython which allows native c extensions inside python. Huge chunks of numpy are written this way to get good speed ups. 

 Or you could forget using python and just write it in ASM. Sure it will be harder to do but when you see that your program runs ~1% faster than everyone elses? You will be happy you went that extra mile. 
 #3 Besides what was already said you could check out cython . But profile before you do. Also, pypy might be worth checking out. There shouldn't be any work needed to make it work. 
 #4 If the tasks can be worked upon in parallel, you could investigate in using a process pool using the multiprocessing module and have the jobs distributed among the subprocesses. 
 #5 It really depends on your application, but if doSomething can be threaded nicely (e.g. has a lot of heavy C code) then you might be interested to spawn multiple threads to do the work. The usual pattern goes something like this: 

 def worker():
 while True:
  item = q.get()
  do_work(item)
  q.task_done()

q = Queue()
for i in range(num_worker_threads):
  t = Thread(target=worker)
  t.daemon = True
  t.start()

for item in source():
 q.put(item)

q.join()  # block until all tasks are done