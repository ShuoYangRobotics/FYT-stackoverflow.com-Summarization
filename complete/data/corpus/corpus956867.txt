Question (ID-956867): How to get string Objects instead Unicode ones from JSON in Python? I'm using Python (Python 2.5.2 on Ubuntu 8.10) to parse JSON from (ASCII encoded) text files. When loading these files with json ( simplejson ), all my string values are cast to Unicode objects instead of string objects. 

 The problem is, I have to use the data with some libraries that only accept string objects. 

 Is it possible to get string objects instead unicode ones from simplejson ? 
 Any hints on how I can achieve this automatically? 

 Edit: I can't change the libraries nor update them. One - the csv module - is even in the Python standard library (the documentation says it will support Unicode in the future). I could write wrappers of course, but maybe there is a more convenient way? 

 The actual data I parse from the JSON files is rather nested and complex, so it would be a pain to look for every Unicode object therein and cast it manually... 

 Here's a small example: 

 >>> import simplejson as json
>>> l = ['a', 'b']
>>> l
['a', 'b']
>>> js = json.dumps(l)
>>> js
'["a", "b"]'
>>> nl = json.loads(js)
>>> nl
[u'a', u'b'] 

 Update : I completely agree with Jarret Hardie and nosklo: Since the JSON specs specifically state strings as Unicode simplejson should return Unicode objects. 

 But while searching the net, I came across some post, where people complain about simplejson actually returning string objects... I couldn't reproduce this behavior but it seems it is possible. Any hints? 

 Workaround 

 Right now I use PyYAML to parse the files, it gives me string objects. 
Since JSON is a subset of YAML it works nicely. 
 Answers (Total-7): #0 That's because json has no difference between string objects and unicode objects. They're all strings in javascript. 

 I think JSON is right to return unicode objects . In fact, I wouldn't accept anything less, since javascript strings are in fact unicode objects (i.e. JSON (javascript) strings can store any kind of unicode character) so it makes sense to create unicode objects when translating strings from JSON. Plain strings just wouldn't fit since the library would have to guess the encoding you want. 

 It's better to use unicode string objects everywhere. So your best option is to update your libraries so they can deal with unicode objects. 

 But if you really want bytestrings, just encode the results to the encoding of your choice: 

 &gt;&gt;&gt; nl = json.loads(js)
&gt;&gt;&gt; nl
[u'a', u'b']
&gt;&gt;&gt; nl = [s.encode('utf-8') for s in nl]
&gt;&gt;&gt; nl
['a', 'b']
 
 #1 You can use the object_hook parameter for json.loads to pass in a converter. You don't have to do the conversion after the fact. The json module will always pass the object_hook dicts only, and it will recursively pass in nested dicts, so you don't have to recurse into nested dicts yourself. I don't think I would convert unicode strings to numbers like Wells shows. If it's a unicode string, it was quoted as a string in the JSON file, so it is supposed to be a string (or the file is bad). 

 Also, I'd try to avoid doing something like str(val) on an unicode object. You should use value.encode(encoding) with a valid encoding, depending on what your external lib expects. 

 So, for example: 

 def _decode_list(data):
 rv = []
 for item in data:
  if isinstance(item, unicode):
   item = item.encode('utf-8')
  elif isinstance(item, list):
   item = _decode_list(item)
  elif isinstance(item, dict):
   item = _decode_dict(item)
  rv.append(item)
 return rv

def _decode_dict(data):
 rv = {}
 for key, value in data.iteritems():
  if isinstance(key, unicode):
   key = key.encode('utf-8')
  if isinstance(value, unicode):
   value = value.encode('utf-8')
  elif isinstance(value, list):
   value = _decode_list(value)
  elif isinstance(value, dict):
   value = _decode_dict(value)
  rv[key] = value
 return rv

obj = json.loads(s, object_hook=_decode_dict)
 
 #2 I'm afraid there's no way to achieve this automatically within the simplejson library. 

 The scanner and decoder in simplejson are designed to produce unicode text. To do this, the library uses a function called c_scanstring (if it's available, for speed), or py_scanstring if the C version is not available. The scanstring function is called several times by nearly every routine that simplejson has for decoding a structure that might contain text. You'd have to either monkeypatch the scanstring value in simplejson.decoder, or subclass JSONDecoder and provide pretty much your own entire implementation of anything that might contain text. 

 The reason that simplejson outputs unicode, however, is that the json spec specifically mentions that "A string is a collection of zero or more Unicode characters"... support for unicode is assumed as part of the format itself. Simplejson's scanstring implementation goes so far as to scan and interpret unicode escapes (even error-checking for malformed multi-byte charset representations), so the only way it can reliably return the value to you is as unicode. 

 If you have an aged library that needs an str , I recommend you either laboriously search the nested data structure after parsing (which I acknowledge is what you explicitly said you wanted to avoid... sorry), or perhaps wrap your libraries in some sort of facade where you can massage the input parameters at a more granular level. The second approach might be more manageable than the first if your data structures are indeed deeply nested. 
 #3 This is late to the game, but I built this recursive caster. It works for my needs and I think it's relatively complete. It may help you. 

 def _parseJSON(self, obj):
	newobj = {}

	for key, value in obj.iteritems():
		key = str(key)

		if isinstance(value, dict):
			newobj[key] = self._parseJSON(value)
		elif isinstance(value, list):
			if key not in newobj:
				newobj[key] = []
				for i in value:
					newobj[key].append(self._parseJSON(i))
		elif isinstance(value, unicode):
			val = str(value)
			if val.isdigit():
				val = int(val)
			else:
				try:
					val = float(val)
				except ValueError:
					val = str(val)
			newobj[key] = val

	return newobj
 

 Just pass it a JSON object like so: 

 obj = json.loads(content, parse_float=float, parse_int=int)
obj = _parseJSON(obj)
 

 I have it as a private member of a class, but you can repurpose the method as you see fit. 
 #4 So, I've run into the same problem. Guess what was the first Google result. 

 Because I need to pass all data to PyGTK, unicode strings aren't very useful to me either. So I have another recursive conversion method. It's actually also needed for typesafe JSON conversion - json.dump() would bail on any non-literals, like Python objects. Doesn't convert dict indexes though. 

 # removes any objects, turns unicode back into str
def filter_data(obj):
  if type(obj) in (int, float, str, bool):
    return obj
  elif type(obj) == unicode:
    return str(obj)
  elif type(obj) in (list, tuple, set):
    obj = list(obj)
    for i,v in enumerate(obj):
      obj[i] = filter_data(v)
  elif type(obj) == dict:
    for i,v in obj.iteritems():
      obj[i] = filter_data(v)
  else:
    print "invalid object in data, converting to string"
    obj = str(obj) 
  return obj
 
 #5 The gotcha is that simplejson and json are two different modules, at least in the manner they deal with unicode. You have json in py 2.6+, and this gives you unicode values, whereas simplejson returns string objects. Just try easy_install-ing simplejson in your environment and see if that works. It did for me. 
 #6 I ran into this problem too, and having to deal with JSON, I came up with a small loop that converts the unicode keys to strings. (Simplejson on GAE does not return string keys) 

 obj is the object decoded from json 

    if NAME_CLASS_MAP.has_key(cls):
    kwargs = {}
    for i in obj.keys():
     kwargs[str(i)] = obj[i]
    o = NAME_CLASS_MAP[cls](**kwargs)
    o.save()
 

 kwargs is what I pass to the constructor of the GAE application (which does not like unicode keys in **kwargs) 

 Not as robust as the solution from Wells, but much smaller.