Question (ID-6728719): How do I make this list function faster? def removeDuplicatesFromList(seq): 
 # Not order preserving 
 keys = {}
 for e in seq:
  keys[e] = 1
 return keys.keys()

def countWordDistances(li):
 '''
 If li = ['that','sank','into','the','ocean'] 
 This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }
 However, if there is a duplicate term, take the average of their positions
 '''
 wordmap = {}
 unique_words = removeDuplicatesFromList(li)
 for w in unique_words:
  distances = [i+1 for i,x in enumerate(li) if x == w]
  wordmap[w] = float(sum(distances)) / float(len(distances)) #take average
 return wordmap
 

 How do I make this function faster? 
 Answers (Total-8): #0 import collections
def countWordDistances(li):
 wordmap = collections.defaultdict(list)
 for i, w in enumerate(li, 1):
  wordmap[w].append(i)
 for k, v in wordmap.iteritems():
  wordmap[k] = sum(v)/float(len(v))

 return wordmap
 

 This makes only one pass through the list, and keeps operations to a minimum. I timed this on a word list with 1.1M entries, 29k unique words, and it was almost twice as fast as Patrick's answer. On a list of 10k words, 2k unique, it was more than 300x faster than the OP's code. 

 To make Python code go faster, there are two rules to keep in mind: use the best algorithm, and avoid Python. 

 On the algorithm front, iterating the list once instead of N+1 times (N= number of unique words) is the main thing that will speed this up. 

 On the "avoid Python" front, I mean: you want your code to be executing in C as much as possible. So using defaultdict is better than a dict where you explicitly check if the key is present. defaultdict does that check for you, but does it in C, in the Python implementation. enumerate is better than for i in range(len(li)) , again because it's fewer Python steps. And enumerate(li, 1) makes the counting start at 1 instead of having to have a Python +1 somewhere in the loop. 

 Edited: Third rule: use PyPy. My code goes twice as fast on PyPy as on 2.7. 
 #1 Based off @Ned Batchelder's solution, but without creating dummy lists: 

 import collections
def countWordDistances(li):
 wordmap = collections.defaultdict(lambda:[0.0, 0.0])
 for i, w in enumerate(li, 1):
  wordmap[w][0] += i
  wordmap[w][1] += 1.0
 for k, (t, n) in wordmap.iteritems():
  wordmap[k] = t / n
 return wordmap
 
 #2 I'm not sure if this will be faster than using a set, but it requires only one pass through the list: 

 def countWordDistances(li):
 wordmap = {}
 for i in range(len(li)):
  if li[i] in wordmap:
   avg, num = wordmap[li[i]]
   new_avg = avg*(num/(num+1.0)) + (1.0/(num+1.0))*i
   wordmap[li[i]] = new_avg, num+1
  else:
   wordmap[li[i]] = (i, 1)

 return wordmap
 

 This returns a modified version of wordmap, with the values associated with each key being a tuple of the average position and the number of occurences. You could obviously easily transform this to the form of the original output, but this would take some time. 

 The code basically keeps a running average while iterating through the list, recalculating each time by taking a weighted average. 
 #3 Use a set: 

 def countWordDistances(li):
 '''
 If li = ['that','sank','into','the','ocean'] 
 This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }
 However, if there is a duplicate term, take the average of their positions
 '''
 wordmap = {}
 unique_words = set(li)
 for w in unique_words:
  distances = [i+1 for i,x in enumerate(li) if x == w]
  wordmap[w] = float(sum(distances)) / float(len(distances)) #take average
 return wordmap
 
 #4 The first thing that springs to mind is to use a set to remove duplicate words: 

 unique_words = set(li)
 

 In general, though, if you're worried about speed you need to profile the function to see where the bottleneck is, and then try to reduce that bottleneck. 
 #5 Use a frozenset instead of a dict , since you're not doing anything with the values: 

 def removeDuplicatesFromList(seq):
 return frozenset(seq)
 
 #6 Using list comprehension: 

 def countWordDistances(l):
 unique_words = set(l)
 idx = [[i for i,x in enumerate(l) if x==item]
   for item in unique_words]
 return {item:1.*sum(idx[i])/len(idx[i]) + 1.
   for i,item in enumerate(unique_words)}

li = ['that','sank','into','the','ocean']
countWordDistances(li)
# {'into': 3.0, 'ocean': 5.0, 'sank': 2.0, 'that': 1.0, 'the': 4.0}

li2 = ['that','sank','into','the','ocean', 'that']
countWordDistances(li2)
# {'into': 3.0, 'ocean': 5.0, 'sank': 2.0, 'that': 3.5, 'the': 4.0}
 
 #7 Oneliner - 

 from __future__ import division # no need for this if using py3k

def countWordDistances(li):
 '''
 If li = ['that','sank','into','the','ocean'] 
 This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }
 However, if there is a duplicate term, take the average of their positions
 '''
 return {w:sum(dist)/len(dist) for w,dist in zip(set(li), ([i+1 for i,x in enumerate(li) if x==w] for w in set(li))) }
 

 What I am making in the last line is a dictionary comprehension, similar to a list comprehension.