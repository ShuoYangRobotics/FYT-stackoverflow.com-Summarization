Question (ID-6971819): Find the numbers that can be made with addition and subtraction, using all numbers given I've profiled my application, and it spends 90% of its time in plus_minus_variations . 

 The function finds ways to make various numbers given a list of numbers using addition and subtraction. 

 For example: 
Input 

 1, 2
 

 Output 

 1+2=3
1-2=-1
-1+2=1
-1-2=-3
 

 This is my current code. I think it could be improved a lot in terms of speed. 

 def plus_minus_variations(nums):
 result = dict()
 for i, ops in zip(xrange(2 ** len(nums)), \
   itertools.product([-1, 1], repeat=len(nums))):
  total = sum(map(operator.mul, ops, nums))
  result[total] = ops
 return result
 

 I'm mainly looking for a different algorithm to approach this with. My current one seems pretty inefficient. However, if you have optimization suggestions about the code itself, I'd be happy to hear those too. 

 Additional: 

 
 It's okay if the result is missing some of the answers (or has some extraneous answers) if it finishes a lot faster. 
 If there are multiple ways to get a number, any of them are fine. 
 For the list sizes I'm using, 99.9% of the ways produce duplicate numbers. 
 It's okay if the result doesn't have the way that the numbers were produced, if, again, it finishes a lot faster. 
 
 Answers (Total-6): #0 EDITED: 

 Aha! 
Code is in Python 3,
inspired by tyz: 

 from functools import reduce # only in Python 3

def process(old, num):
 new = set(map(num.__add__, old)) # use itertools.imap for Python 2
 new.update(map((-num).__add__, old))
 return new

def pmv(nums):
 n = iter(nums)
 x = next(n)
 result = {x, -x} # set([x, -x]) for Python 2
 return reduce(process, n, result)
 

 Instead of split half and recursive, I use reduce to compute it one by one. that extremely reduced the times of function calls. 

 Take less than 1 sec to compute 256 numbers. 

 

 Why product then mult? 

 def pmv(nums):
 return {sum(i):i for i in itertools.product(*((num, -num) for num in nums))}
 

 Can be faster without how the numbers were produced: 

 def pmv(nums):
 return set(map(sum, itertools.product(*((num, -num) for num in nums))))
 
 #1 If it is ok not to get trace of number producing there is no reasons to recalculate sum of number combination every time. You can store intermediate results: 

 def combine(l,r):
 res = set()
 for x in l:
  for y in r:
   res.add( x+y )
   res.add( x-y )
   res.add( -x+y )
   res.add( -x-y )
 return list(res)

def pmv(nums):
 if len(nums) &gt; 1:
  l = pmv( nums[:len(nums)/2] )
  r = pmv( nums[len(nums)/2:] )
  return combine( l, r )
 return nums
 

 EDIT : if the way of number generation is important you can use this variant: 

 def combine(l,r):
 res = dict()
 for x,q in l.iteritems():
  for y,w in r.iteritems():
   if not res.has_key(x+y):
    res[x+y] = w+q
    res[-x-y] = [-i for i in res[x+y]]
   if not res.has_key(x-y):
    res[x-y] = w+[-i for i in q]
    res[-x+y] = [-i for i in res[x-y]]
 return res

def pmv(nums):
 if len(nums) &gt; 1:
  l = pmv( nums[:len(nums)/2] )
  r = pmv( nums[len(nums)/2:] )
  return combine( l, r )
 return {nums[0]:[1]}
 

 My tests shows that it is still faster than the other solutions. 
 #2 This seems to be significantly faster for large random lists, I guess you could further micro-optimize it, but I prefer readability. 

 I chunk the list into smaller pieces and create variations for it. Since you get a lot less than 2 ** len(chunk) variatons it's going to be faster. Chunk length is 6, you can play with it to see what's the optimal chunk length. 

 def pmv(nums):
 chunklen=6
 res = dict()
 res[0] = ()
 for i in xrange(0, len(nums), chunklen):
  part = plus_minus_variations(nums[i:i+chunklen])
  resnew = dict()
  for (i,j) in itertools.product(res, part):
   resnew[i + j] = tuple(list(res[i]) + list(part[j]))
  res = resnew
 return res
 
 #3 You can get something like a 50% speedup (at least for short lists) just by doing: 

 from itertools import product, imap
from operator import mul

def plus_minus_variations(nums):
 result = {}
 for ops in product((-1, 1), repeat=len(nums)):
  result[sum(imap(mul, ops, nums))] = ops
 return result
 

 imap won't create intermediate lists you don't need. Importing into the local namespace saves the time attribute lookup takes. Tuples are faster than lists. Don't store unneeded intermediate items. 

 I tried this with a dict comprehension but it was a bit slower. I tried it with a set comprehension (not saving the ops ) and it was the same speed. 

 I don't know why you were using zip and xrange at all... you weren't using the result in your calculation. 

 Edit: I get significant speedups with this version all the way up to the point where your version gives a memory error, not just for short lists. 
 #4 From a mathematical point of view you finally arive at all multiples of the greatest common divisor of your startvalues. 

 For example: 

 
 startvalues 2,4. then the gcd(2,4) is 2, so the generated numbers are .. -4, -2, 0, 2, 4, ... 
 startvalues 3,5. then the gcd(3,5) is 1, you get all integers. 
 startvalues 12, 18, 15. the gcd(12,15,18) is 3, you get .. -6, -3, 0, 3, 6, .... 
 
 #5 This simple iterative method computes all possible sums. It could be about 5 times faster than the recursive method by @tyz. 

 def pmv(nums):
 sums = set()
 sums.add(0)
 for i in range(0, len(nums)):
  partialsums = set()
  for s in sums:
   partialsums.add(s + nums[i])
   partialsums.add(s - nums[i])
  sums = partialsums
 return sums