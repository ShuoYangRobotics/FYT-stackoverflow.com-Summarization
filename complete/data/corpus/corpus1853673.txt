Question (ID-1853673): Writing a Faster Python Spider I'm writing a spider in Python to crawl a site. Trouble is, I need to examine about 2.5 million pages, so I could really use some help making it optimized for speed. 

 What I need to do is examine the pages for a certain number, and if it is found to record the link to the page. The spider is very simple, it just needs to sort through a lot of pages. 

 I'm completely new to Python, but have used Java and C++ before. I have yet to start coding it, so any recommendations on libraries or frameworks to include would be great. Any optimization tips are also greatly appreciated. 

 Thanks for all the help :) 
 Answers (Total-6): #0 You could use MapReduce like Google does, either via Hadoop (specifically with Python: 1 and 2 ), Disco , or Happy . 

 The traditional line of thought, is write your program in standard Python, if you find it is too slow, profile it , and optimize the specific slow spots. You can make these slow spots faster by dropping down to C, using C/C++ extensions or even ctypes . 

 If you are spidering just one site, consider using wget -r ( an example ). 
 #1 Spidering somebody's site with millions of requests isn't very polite. Can you instead ask the webmaster for an archive of the site? Once you have that, it's a simple matter of text searching. 
 #2 You waste a lot of time waiting for network requests when spidering, so you'll definitely want to make your requests in parallel. I would probably save the result data to disk and then have a second process looping over the files searching for the term. That phase could easily be distributed across multiple machines if you needed extra performance. 
 #3 As you are new to Python, I think the following may be helpful for you :) 

 
 if you are writing regex to search for certain pattern in the page, compile your regex wherever you can and reuse the compiled object 
 BeautifulSoup is a html/xml parser that may be of some use for your project. 
 
 #4 Where are you storing the results? You can use PiCloud 's cloud library to parallelize your scraping easily across a cluster of servers. 
 #5 What Adam said. I did this once to map out Xanga's network. The way I made it faster is by having a thread-safe set containing all usernames I had to look up. Then I had 5 or so threads making requests at the same time and processing them. You're going to spend way more time waiting for the page to DL than you will processing any of the text (most likely), so just find ways to increase the number of requests you can get at the same time.