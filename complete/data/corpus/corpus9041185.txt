Question (ID-9041185): Could someone kindly explain to me exactly how "binary" numbers relate to my everyday programming? I'll admit it - i'm a "lowly web programmer" who is trying very hard to develop a much deeper understanding of programming as a whole. I understand the textbook definition of "Binary", but what I don't understand is exactly how it applies to my day to day programming? 

 The concept of "binary numbers" vs .. well... "regular" numbers, is completely lost on me despite my best attempts to research and understand the concept. 

 I am someone who originally taught myself to program by building stupid little adventure games in early DOS Basic and C, and now currently does most (er, all) of my work in PHP, javascript, Rails, and other "Web" languages. I find that so much of this logic is abstracted out in these higher level languages that I ultimately feel I am missing many of the tools I need to continue progressing and writing better code. 

 If anyone could point me in the direction of a good, solid practical learning resource... or explain it here, it would be massively appreciated. 

 I'm not so much looking for the 'definition' (i've read the wikipedia page a few times now), but more some direction on how I can incorporate this newfound knowledge of exactly what binary numbers are into my day to day programming, if at all. I'm primarily writing in PHP these days, so references to that language specifically would be very helpful. 

 Thanks in advance to everyone. 

 EDIT As pointed out.. binary is a representation of a number, not a different system altogether.. So to revise my question, what are the benefits (if any) of using binary representation of numbers rather than just... numbers. 
 Answers (Total-8): #0 Binary trees (one of your tags), particularly binary search trees , are practical for some everyday programming scenarios (e.g. sorting). 

 Binary numbers are essential to computing fundamentals but more rarely used in higher-level languages. 

 Binary numbers are useful in understanding bounds, such as the largest unsigned number of various widths (e.g. 2^32 - 1 for 32-bit), or the largest and smallest signed numbers for two's complement (the system normally used). For example, why is the smallest signed two's complement 32-bit number -2^31 but the largest 2^31 - 1? Even odder at first glance, -(-2^31) (negating the smallest number), yields itself. (Hint, try it with 2-bit numbers, since the analysis is the same). 

 Another is basic information theory . How many bits do I need to represent 10000 possibilities (log 2 10000, rounded up)? It's also applicable to cryptography, but you're probably not getting into that much yet. 

 Don't expect to use binary everyday, but do develop a basic understanding for these and other reasons. 

 If you explore pack and bitwise operators, you may find other uses. In particular, many programmers don't know when they can use XOR (which can be understood by looking at a truth table involving the two binary digits). 
 #1 Here is a brief history to help your understanding and I will get to your question at the end. 

 Binary is a little weird because we are so used to using a base 10 number system. This is because humans have 10 fingers, when they ran out they had to use a stick, toe or something else to represent 10 fingers. This it not true for all cultures though, some of the hunter gatherer populations (such as the Australian Aboriginal) used a base 5 number system (one hand) as producing large numbers were not necessary. 

 Anyway, the reason base 2 is important in computing is because a circuit can have two states, low voltage and high voltage; think of this like a switch (on and off). Place 8 of these switches together and you have 1 byte (8 bits). The best way to think of a bit is 1=on and 0=off which is exactly how it is represented in binary. You might then have something like this 10011100 where 1's are high volts and 0 are low volts. In early computers, physical switches were used which the the operator could turn on and off to create a program. 

 Nowadays, you will rarely need to use binary number in modern programming. The only exceptions I can think of is bitwise arithmetic which are very fast and efficient ways of solving certain problems or maybe some form of computer hacking. All I can suggest is learn the basics of it but don't worry about actually using it in everyday programming. 
 #2 There are two usages of binary (versus regular ) numbers. 

 Because of the word regular , probably not: 

 
 Binary stored as compact bytes, say 4 bytes for an integer, 8 B for a double. Is SQL INT or DOUBLE . Regular stored as text, byte per digit. SQL VARCHAR . 
 

 But in our case: 

 
 Representation in different numbering base: 101 binary = 1*4 + 0*2 + 1*1 = 5. 
 

 This lends itself for complex codings of yes/no states: 

 Given 1 | x = 1 and 0 | x = x (or, binary +) and 0 &amp; x = 0 and 1 &amp; x = x (and, binary *) 

 $sex_male = 0:
$sex_female = 1;
$employee_no = 0*2;
$employee_yes = 1*2;
$has_no_email = 0*4;
$has_email = 1*4;

$code = $sex_female | $employee_no | $has_email;

if (($code &amp; $sex_female) != 0) print "female";
 
 #3 To me, one of the biggest impacts of a binary representation of numbers is the difference between floating point values and our "ordinary" (base-10 or decimal) notion of fractions, decimals, and real numbers. 

 The vast majority of fractions cannot be exactly represented in binary. Something like 0.4 seems like it's not a hard number to represent; it's only got one place after the decimal, it's the same as two fifths or 40%, what's so tough? But most programming environments use binary floating point, and cannot represent this number exactly ! Even if the computer displays 0.4, the actual value used by the computer is not exactly 0.4. So you get all kinds of unintuitive behavior when it comes to rounding and arithmetic. 

 Note that this "problem" is not unique to binary. For example, using our own base-10 decimal notation, how do we represent one third? Well, we can't do it exactly. 0.333 is not exactly the same as one third. 0.333333333333 is not exactly one third either. We can get pretty close, and the more digits you let us use, the closer we can get. But we can never, ever be exactly right, because it would require an infinite number of digits. This is fundamentally what's happening when binary floating point does something we don't expect: The computer doesn't have an infinite number of binary digits (bits) to represent our number, and so it can't get it exactly right, but gives us the closest thing it can. 
 #4 rather more of an experience rather than a solid answer: 

 actually, you don't actually need binary because it's pretty much abstracted in programming nowadays (depending on what you program). binary has more use in the systems design and networking. 

 some things my colleagues at school do in their majors: 

 
 processor instruction sets and operations (op codes) 
 networking and data transmission 
 hacking (especially memory "tampering". more of hex but still related) 
 memory allocation (in assembly, we use hex but sometimes binary) 
 

 you need to know how these "regular numbers" are represented and understood by the machine - hence all those "conversion lessons" like hex to binary, binary to octal etc. machines only read binary. 
 #5 With Python you can explore bitwise operations and manipulations with the command line. Personally I've used bit operations to examine an obscure compression algorithm used in packet radio. 

 
 Bitwise Operators 
 Bit Manipulations 
 
 #6 Interesting question. Although you are a "lowly web guy" I would have to say that it is great that you are curious about how binary affects you. Well to help I would suggest picking up a low-level language and playing around with it. Something along the likes of C programming and/or Assembly. As far as using PHP try looking through the source code of PHP and how its implemented.
Here's a Quality links on binary/Hexadecimal http://maven.smith.edu/~thiebaut/ArtOfAssembly/artofasm.html 
Good luck and happy learning :) 
 #7 As a web guy, you no doubt understand the importance of unicode. Unicode is represented in hexidecimal format when viewing character sets not supported by your system. Hexidecimal also appears in RGB values, and memory addresses. Hexideciaml is, among many things, a shorthand for writing out long binary characters. 

 Finally, binary numbers work as the basis of truthiness: 1 is true, while 0 is always false. 

 Go check out a book on digital fundementals, and try your hand at boolean logic. You'll never look at if a and not b or c the same way again!