Question (ID-5002783): Best Python clustering library to use for product data analysis I have a collection of alphanumeric product codes of various products. Similar products have no intrinsic similarity in their codes, ie product code "A123" might mean "Harry Potter Volume 1 DVD" and "B123" might mean "Kellogs Corn Flakes". I also do not actually have the description or identify of the product. All I have is an "owner" of this code. My data, therefore, looks (in a non-normal way) something like this: 

 Owner1: ProductCodes A123,B124,W555,M221,M556,127,102 

 Owner2: ProductCode D103,Z552,K112,L3254,223,112 

 Owner3: ProductCode G123 

 .... 

 I have huge (ie Terabytes) sets of this data. 

 I assume that an owner would - for the majority - have an undetermined number of groups of similar products - ie an owner might have just 2 groups - all the DVDs and books of Harry Potter, but also a collection of "Iron Maiden" cds. I would like to analyse this data and determine distance functions between product codes so I can start making assumptions about "how close" product codes are to each other and also cluster product codes (so I can also identify how many groups an owner has). I have started doing some research on textual clustering algorithms but there are numerous ones to choose from and I'm not sure on which one(s) work best with this scenario. 

 Can someone point me towards the most appropriate python based clustering functions / libraries to use please ?! 
 Answers (Total-7): #0 What you have is a bipartite graph. As an initial stab, it sounds like you are going to treat neighbour lists as zero-one vectors between which you define some kind of similarity/correlation. This could be a normalised Hamming distance for example. Depending on which way you do that you will obtain a graph on a single domain -- either product codes or owners. It will shortly become clear why I've cast everything in the language of graphs, bear with me. Now why do you insist on a Python implementation? Clustering large scale data is time and memory consuming. To pull the cat out of the bag, I have written and still maintain a graph clustering algorithm, used quite widely in bioinformatics. Is is threaded, accepts weighted graphs, and has been used for graphs with millions of nodes and towards a billion of edges. Refer to http://micans.org/mcl/ for more information. Of course, if you trawl stackoverflow and stackexchange there is quite a few threads that may be of interest to you. I would recommend the Louvain method as well, except that I am not sure whether it accepts weighted networks, which you will probably produce. 
 #1 R language has many packages for finding groups in data , and there are python bindings to R, called RPy . R provides several algorithms already mentioned here and also known for good performance on large datasets. 
 #2 I think you can use pycluster also change algorithm for your problem 

 
 http://bonsai.hgc.jp/~mdehoon/software/cluster/software.htm 
 http://cran.r-project.org/web/views/Cluster.html 
 

 also i think you better see this http://www.dennogumi.org/2007/11/data-clustering-with-python 
 #3 I don't know much about your problem domain. But PyCluster is pretty decent clustering package which works good on large datasets:
 http://bonsai.hgc.jp/~mdehoon/software/cluster/software.htm 

 Hope it helps. 
 #4 I don't know of an off-the-shelf lib, sorry. There are big libs for full-text search and similarity,
but for bit sets you'll have to roll your own (as far as i know).
A couple of suggestions anyway: 

 
 bitset approach: first get say 10k owners x 100k products, or 100k x 10k, in memory, to play with.
You could use bitarray to make a big array of 10k x 100k bits.
But then, what do you want to do with it ? 
To find similar pairs among N objects (either owners or products),
you have to look at all N*(N-1)/2 pairs, which is a lot; 
or, there must be some structure in tha data that allows early pruning / hierarchical similarity; 
or, google "greedy clustering" Python &mdash; don't see an off-the-shelf lib. 
 how do you define "similarity" of owners / of products ? There are lots of possibilities &mdash; number in common, ratio in common, tf-idf ... 
 

 (Added): have you looked at Mahout 's recommendation system API,
is that about what you're looking for ? 
 This SO question
says there's no Python equivalent, which leaves two choices: 
a) ask if anyone has used Mahout from Jython,
or b) if you can't lick 'em, join 'em. 
 #5 You can try to do clustering using the k-means clustering algorithm and its scipy implementation available in scikits.learn.cluster.KMeans . 
 #6 Try Celery , it's a distributed job queue system. You can put all your task in the queue, and add work nodes as much as you want. 

 

 It should fit your needs.