<h3>Question (ID-1545606):</h3><h2>Python k-means algorithm</h2><p>I am looking for Python implementation of k-means algorithm with examples to cluster and cache my database of coordinates.</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><p><a href="http://docs.scipy.org/doc/scipy/reference/cluster.html" rel="nofollow">Scipy's clustering</a> implementations work well, and they include a <a href="http://docs.scipy.org/doc/scipy/reference/cluster.vq.html" rel="nofollow">k-means</a> implementation.  There's a scipy kmeans <a href="http://hackmap.blogspot.com/2007/09/k-means-clustering-in-scipy.html" rel="nofollow">example</a> here.</p>

<p>There's also <a href="http://code.google.com/p/scipy-cluster/" rel="nofollow">scipy-cluster</a>, which does agglomerative clustering; ths has the advantage that you don't need to decide on the number of clusters ahead of time.</p>
<br /><b>#1</b><br /><p>SciPy's <a href="http://docs.scipy.org/doc/scipy/reference/cluster.vq.html">kmeans2()</a> has some numerical problems: others have <a href="http://mail.scipy.org/pipermail/scipy-user/2009-February/019777.html">reported</a> error messages such as "Matrix is not positive definite - Cholesky decomposition cannot be computed" in version 0.6.0, and I just encountered the same in version 0.7.1.</p>

<p>For now, I would recommend using <a href="http://bonsai.ims.u-tokyo.ac.jp/~mdehoon/software/cluster/software.htm#pycluster">PyCluster</a> instead.  Example usage:</p>

<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; import Pycluster
&gt;&gt;&gt; points = numpy.vstack([numpy.random.multivariate_normal(mean, 
                                                            0.03 * numpy.diag([1,1]),
                                                            20) 
                           for mean in [(1, 1), (2, 4), (3, 2)]])
&gt;&gt;&gt; labels, error, nfound = Pycluster.kcluster(points, 3)
&gt;&gt;&gt; labels  # Cluster number for each point
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)
&gt;&gt;&gt; error   # The within-cluster sum of distances for the solution
1.7721661785401261
&gt;&gt;&gt; nfound  # Number of times this solution was found
1
</code></pre>
<br /><b>#2</b><br /><p>From <a href="http://en.wikipedia.org/wiki/K-means%5Fclustering" rel="nofollow">wikipedia</a>, you could use scipy,  <a href="http://docs.scipy.org/doc/scipy/reference/cluster.vq.html" rel="nofollow">K-means clustering an vector quantization</a></p>

<p>Or, you could use a Python wrapper for OpenCV, <a href="http://code.google.com/p/ctypes-opencv/" rel="nofollow">ctypes-opencv</a>.</p>

<p>Or you could <a href="http://opencv.willowgarage.com/wiki/PythonInterface" rel="nofollow">OpenCV's new Python interface</a>, and their <a href="http://opencv.willowgarage.com/documentation/python/miscellaneous%5Ffunctions.html" rel="nofollow">kmeans</a> implementation.</p>
<br /><b>#3</b><br /><p>By the way, K-Means is a very simple algorithm to implement. It's an educational experience to implement it yourself.</p>
<br /><b>#4</b><br /><p>You can also use GDAL, which has many many functions to work with spatial data.</p>
<br /><b>#5</b><br /><p>For continuous data, k-means is very easy.</p>

<p>You need a list of your means, and for each data point, find the mean its closest to and average the new data point to it. your means will represent the recent salient clusters of points in the input data.</p>

<p>I do the averaging continuously, so there is no need to have the old data to obtain the new average. Given the old average <code>k</code>,the next data point <code>x</code>, and a constant <code>n</code> which is the number of past data points to keep the average of, the new average is</p>

<pre><code>k*(1-(1/n)) + n*(1/n)
</code></pre>

<p>Here is the full code in Python</p>

<pre><code>from __future__ import division
from random import random

# init means and data to random values
# use real data in your code
means = [random() for i in range(10)]
data = [random() for i in range(1000)]

param = 0.01 # bigger numbers make the means change faster
# must be between 0 and 1

for x in data:
    closest_k = 0;
    smallest_error = 9999; # this should really be positive infinity
    for k in enumerate(means):
        error = abs(x-k[1])
        if error &lt; smallest_error:
            smallest_error = error
            closest_k = k[0]
        means[closest_k] = means[closest_k]*(1-param) + x*(param)
</code></pre>

<p>you could just print the means when all the data has passed through, but its much more fun to watch it change in real time. I used this on frequency envelopes of 20ms bits of sound and after talking to it for a minute or two, it had consistent categories for the short 'a' vowel, the long 'o' vowel, and the 's' consonant. wierd!</p>
<br /><b>#6</b><br /><p>(Years later) this kmeans.py under <a href="http://stackoverflow.com/questions/5529625/is-it-possible-to-specify-your-own-distance-function-using-scikits-learn-k-means">is-it-possible-to-specify-your-own-distance-function-using-scikits-learn-k-means</a> is straightforward and reasonably fast; it uses any of the 20-odd metrics in scipy.spatial.distance.</p>
<br /><b>#7</b><br /><p>you can try the scikits-learn implementation:
<a href="http://scikit-learn.sourceforge.net/modules/generated/scikits.learn.cluster.KMeans.html" rel="nofollow">http://scikit-learn.sourceforge.net/modules/generated/scikits.learn.cluster.KMeans.html</a></p>
<br />