<h3>Question (ID-4083523):</h3><h2>Alternatives to ApacheBench for profiling my code speed</h2><p>I've done some experiments using Apache Bench to profile my code response times, and it doesn't quite generate the right kind of data for me. I hope the good people here have ideas.</p>

<p>Specifically, I need a tool that</p>

<ul>
<li>Does HTTP requests over the network (it doesn't need to do anything very fancy)</li>
<li>Records response times as accurately as possible (at least to a few milliseconds)</li>
<li>Writes the response time data to a file without further processing (or provides it to my code, if a library)</li>
</ul>

<p>I know about <code>ab -e</code>, which prints data to a file. The problem is that this prints only the quantile data, which is useful, but not what I need. The <code>ab -g</code> option would work, except that it doesn't print sub-second data, meaning I don't have the resolution I need.</p>

<p>I wrote a few lines of Python to do it, but the httplib is horribly inefficient and so the results were useless. In general, I need better precision than pure Python is likely to provide. If anyone has suggestions for a library usable from Python, I'm all ears.</p>

<p>I need something that is high performance, repeatable, and reliable.</p>

<p>I know that half my responses are going to be along the lines of "internet latency makes that kind of detailed measurements meaningless." In my particular use case, this is not true. I need high resolution timing details. Something that actually used my HPET hardware would be awesome.</p>

<p>Throwing a bounty on here because of the low number of answers and views.</p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>I have done this in two ways.</p>

<p>With "loadrunner" which is a wonderful but pretty expensive product (from I think HP these days).</p>

<p>With combination perl/php and the Curl package. I found the CURL api slightly easier to use from php. Its pretty easy to roll your own GET and PUT requests. I would also recommend manually running through some sample requests with Firefox and the LiveHttpHeaders add on to captute the exact format of the http requests you need.</p>
<br /><b>#1</b><br /><p><a href="http://jakarta.apache.org/jmeter/usermanual/get-started.html" rel="nofollow">JMeter</a> is pretty handy. It has a GUI from which you can set up your requests and threadpools and it also can be run from the command line.</p>
<br /><b>#2</b><br /><p>If you can code in Java, you can look at the combination of <a href="http://jwebunit.sourceforge.net/faq.html#junitperf-support" rel="nofollow">JUnitPerf + HttpUnit</a>.</p>

<p>The downside is that you will have to do more things yourself. But at the price of this you will get unlimited flexibility and arguably more preciseness than with GUI tools, not to mention HTML parsing, JavaScript execution, etc.</p>

<p>There's also another project called <a href="http://grinder.sourceforge.net/" rel="nofollow">Grinder</a> which seems to be purposed for a similar task but I don't have any experience with it.</p>
<br /><b>#3</b><br /><p>A good reference of opensource perfomance testing tools: <a href="http://www.opensourcetesting.org/performance.php" rel="nofollow">http://www.opensourcetesting.org/performance.php</a></p>

<p>You will find descriptions and a "most popular" list</p>
<br /><b>#4</b><br /><p>I've used a script to drive 10 boxes on the same switch to generate load by "replaying" requests to 1 server.  I had my web app logging response time (server only) to the granularity I needed, but I didn't care about the response time to the client.  I'm not sure you care to include the trip to and from the client in your calculations, but if you did it shouldn't be to difficult to code up. I then processed my log with a script which extracted the times per url and did scatter plot graphs, and trend graphs based on load. </p>

<p>This satisfied my requirements which were:</p>

<ul>
<li>Real world distribution of calls to different urls.</li>
<li>Trending performance based on load.</li>
<li>Not influencing the web app by running other intensive ops on the same box.</li>
</ul>

<p>I did controller as a shell script that foreach server started a process in the background to loop over all the urls in a file calling curl on each one.  I wrote the log processor in Perl since I was doing more Perl at that time.</p>
<br /><b>#5</b><br /><p><a href="http://code.google.com/p/httperf/" rel="nofollow">httperf</a> is very powerful.</p>
<br />