<h3>Question (ID-1041543):</h3><h2>Unable to see Python's approximations in mathematical calculations</h2><p><strong>Problem:</strong> to see when computer makes approximation in mathematical calculations when I use Python</p>

<p><strong>Example of the problem:</strong></p>

<p>My old teacher once said the following statement</p>

<pre><code>You cannot never calculate 200! with your computer.
</code></pre>

<p>I am not completely sure whether it is true or not nowadays.
It seems that it is, since I get a lot zeros for it from a <a href="http://stackoverflow.com/questions/1041348/unable-to-make-a-factorial-function-in-python">Python script</a>.</p>

<p><strong>How can you see when your Python code makes approximations?</strong></p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>Python use <a href="http://en.wikipedia.org/wiki/Arbitrary-precision%5Farithmetic" rel="nofollow">arbitrary-precision arithmetic</a> to calculate with integers, so it can exactly calculate 200!.  For real numbers (so-called <a href="http://en.wikipedia.org/wiki/Floating-point" rel="nofollow"><em>floating-point</em></a>), Python does not use an exact representation.  It uses a binary representation called <a href="http://en.wikipedia.org/wiki/IEEE%5F754" rel="nofollow">IEEE 754</a>, which is essentially scientific notation, except in base 2 instead of base 10.</p>

<p>Thus, any real number that cannot be exactly represented in base 2 with 53 bits of precision, Python cannot produce an exact result.  For example, 0.1 (in base 10) is an infinite decimal in base 2, 0.0001100110011..., so it cannot be exactly represented.  Hence, if you enter on a Python prompt:</p>

<pre><code>&gt;&gt;&gt; 0.1
0.10000000000000001
</code></pre>

<p>The result you get back is different, since has been converted from decimal to binary (with 53 bits of precision), back to decimal.  As a consequence, you get things like this:</p>

<pre><code>&gt;&gt;&gt; 0.1 + 0.2 == 0.3
False
</code></pre>

<p>For a good (but long) read, see <a href="http://docs.sun.com/source/806-3568/ncg%5Fgoldberg.html" rel="nofollow">What Every Programmer Should Know About Floating-Point Arithmetic</a>.</p>
<br /><b>#1</b><br /><p>Python has unbounded <strong>integer</strong> sizes in the form of a <strong>long</strong> type.  That is to say, if it is a whole number, the limit on the size of the number is restricted by the memory available to Python.</p>

<p>When you compute a large number such as 200! and you see an L on the end of it, that means Python has automatically cast the int to a <em>long</em>, because an int was not large enough to hold that number.</p>

<p>See section 6.4 of <a href="http://docs.python.org/library/stdtypes.html" rel="nofollow">this page</a> for more information.</p>
<br /><b>#2</b><br /><p>See <a href="http://stackoverflow.com/questions/538551/handling-very-large-numbers-in-python">Handling very large numbers in Python</a>. </p>

<p>Python has a BigNum class for holding 200! and will use it automatically. </p>

<p>Your teacher's statement, though not exactly true here is true in general. Computers have limitations, and it is good to know what they are. Remember that every time you add another integer of data storage, you can store a number that is 2^32 (4 billion +) times larger. It is hard to comprehend how many more numbers that is - but maths gets slower as you add more integers to store the exact value of a very large number.</p>

<p>As an example (what you can store with 1000 bits)</p>

<pre><code>&gt;&gt;&gt; 2 &lt;&lt; 1000
2143017214372534641896850098120003621122809623411067214887500776740702102249872244986396
7576313917162551893458351062936503742905713846280871969155149397149607869135549648461970
8421492101247422837559083643060929499671638825347975351183310878921541258291423929553730
84335320859663305248773674411336138752L
</code></pre>

<p>I tried to illustrate how big a number you can store with 10000 bits, or even 8,000,000 bits (a megabyte) but that number is many pages long.</p>
<br /><b>#3</b><br /><p>200! is a very large number indeed. </p>

<p>If the range of an IEEE 64-bit double is 1.7E +/- 308 (15 digits), you can see that the largest factorial you can get is around 170!.  </p>

<p>Python can handle arbitrary sized numbers, as can Java with its BigInteger.</p>
<br /><b>#4</b><br /><p>Without some sort of clarification to that statement, it's obviously false. Just from personal experience, early lessons in programming (in the late 1980s) included solving very similar, if not exactly the same, problems. In general, to know some device which does calculations isn't making approximations, you have to prove (in the math sense of a proof) that it isn't.</p>

<p>Python's integer types (named <code>int</code> and <code>long</code> in 2.x, both folded into just the <code>int</code> type in 3.x) are very good, and do not overflow like, for example, the <code>int</code> type in C. If you do the obvious of <code>print 200 * 199 * 198 * ...</code> it may be slow, but it will be exact. Similiarly, addition, subtraction, and modulus are exact. Division is a mixed bag, as there's two operators, <code>/</code> and <code>//</code>, and they underwent a change in 2.x&mdash;in general you can only treat it as inexact.</p>

<p>If you want more control yet don't want to limit yourself to integers, look at the <a href="http://docs.python.org/library/decimal.html" rel="nofollow"><code>decimal</code></a> module.</p>
<br /><b>#5</b><br /><p>Python handles large numbers automatically (unlike a language like C where you can overflow its datatypes and the values reset to zero, for example) - over a certain point (<code>sys.maxint</code> or 2147483647) it converts the integer to a "long" (denoted by the <code>L</code> after the number), which can be any length:</p>

<pre><code>&gt;&gt;&gt; def fact(x):
...     return reduce(lambda x, y: x * y, range(1, x+1))
... 
&gt;&gt;&gt; fact(10)
3628800
&gt;&gt;&gt; fact(200)
788657867364790503552363213932185062295135977687173263294742533244359449963403342920304284011984623904177212138919638830257642790242637105061926624952829931113462857270763317237396988943922445621451664240254033291864131227428294853277524242407573903240321257405579568660226031904170324062351700858796178922222789623703897374720000000000000000000000000000000000000000000000000L
</code></pre>

<p>Long numbers are "easy", floating point is more complicated, and almost any computer representation of a floating point number is an approximation, for example:</p>

<pre><code>&gt;&gt;&gt; float(1)/3
0.33333333333333331
</code></pre>

<p>Obviously you can't store an infinite number of 3's in memory, so it cheats and rounds it a bit..</p>

<p>You may want to look at the <a href="http://docs.python.org/library/decimal.html" rel="nofollow">decimal</a> module:</p>

<blockquote>
  <ul>
  <li>Decimal numbers can be represented exactly. In contrast, numbers like 1.1 do not have an exact representation in binary floating point. End users typically would not expect 1.1 to display as 1.1000000000000001 as it does with binary floating point.</li>
  <li>Unlike hardware based binary floating point, the decimal module has a user alterable precision (defaulting to 28 places) which can be as large as needed for a given problem</li>
  </ul>
</blockquote>
<br />