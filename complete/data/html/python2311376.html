<h3>Question (ID-2311376):</h3><h2>speed-up python function to process files with data segments separated by a blank space</h2><p>I need to process files with data segments separated by a blank space, for example:</p>

<pre><code>93.18 15.21 36.69 33.85 16.41 16.81 29.17 
21.69 23.71 26.38 63.70 66.69 0.89 39.91 
86.55 56.34 57.80 98.38 0.24 17.19 75.46 
[...]
1.30 73.02 56.79 39.28 96.39 18.77 55.03

99.95 28.88 90.90 26.70 62.37 86.58 65.05 
25.16 32.61 17.47 4.23 34.82 26.63 57.24 
36.72 83.30 97.29 73.31 31.79 80.03 25.71 
[...]
2.74 75.92 40.19 54.57 87.41 75.59 22.79

.
.
.
</code></pre>

<p>for this I am using the following function. 
In every call I get the necessary data, but I need to speed-up the code.</p>

<p>Is there a more efficient way?</p>

<p><strong>EDIT</strong>: <em>I will be updating the code with the changes that achieve improvements</em></p>

<p><strong>ORIGINAL:</strong></p>

<pre><code>def get_pos_nextvalues(pos_file, indices):
    result = []
    for line in pos_file:
        line = line.strip()
        if not line:
            break
        values = [float(value) for value in line.split()]
        result.append([float(values[i]) for i in indices])
    return np.array(result)
</code></pre>

<p><strong>NEW:</strong></p>

<pre><code>def get_pos_nextvalues(pos_file, indices):
    result = ''
    for line in pos_file:
        if len(line) &gt; 1:
            s = line.split()
            result += ' '.join([s [i] for i in indices])
        else:
            break
    else:
        return np.array([])
    result = np.fromstring(result, dtype=float, sep=' ')
    result = result.reshape(result.size/len(indices), len(indices))
    return result
</code></pre>

<p>.</p>

<pre><code>pos_file = open(filename, 'r', buffering=1024*10)

[...]

while(some_condition):
    vs = get_pos_nextvalues(pos_file, (4,5,6))
    [...]
</code></pre>

<p><strong>speedup</strong> = 2.36</p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>not to convert floats to floats would be the first step. I would suggest, however, to first <a href="http://docs.python.org/library/profile.html" rel="nofollow">profile your code</a> and then try to optimize the bottleneck parts.</p>

<p>I understand that you've changed your code from the original, but </p>

<pre><code>values = [value for value in line.split()]
</code></pre>

<p>is not a good thing either. just write <code>values = line.split()</code> if this is what you mean.</p>

<p>Seeing how you're using NumPy, I'd suggest some <a href="http://www.scipy.org/Cookbook/InputOutput#head-8b33b550a3efcb0981f6720ba42677720325588c" rel="nofollow">methods of file reading that are demonstrated in their docs</a>.</p>
<br /><b>#1</b><br /><p>You are only reading every character exactly once, so there isn't any real performance to gain.</p>

<p>You could combine strip and split if the empty lines contain a lot of whitespace.</p>

<p>You could also save some time initializing the numpy array from start, instead of first creating a python array and then converting.</p>
<br /><b>#2</b><br /><p>try increasing the read buffer, IO is probably the bottle neck of your code</p>

<pre><code>open('file.txt', 'r', 1024 * 10) 
</code></pre>

<p>also if the data is fully sequential you can try to skip the line by line code and convert a bunch of lines at once</p>
<br /><b>#3</b><br /><p>Instead of :</p>

<pre><code>if len(line) &lt;= 1: # only '\n' in «empty» lines
    break
values = line.split()
</code></pre>

<p>try this:</p>

<pre><code>values = line.split()
if not values: # line is wholly whitespace, end of segment
    break
</code></pre>
<br /><b>#4</b><br /><p><a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html" rel="nofollow">numpy.fromfile</a> doesn't work for you?</p>

<pre><code>arr = fromfile('tmp.txt', sep=' ', dtype=int)
</code></pre>
<br /><b>#5</b><br /><p>Here's a variant that <em>might</em> be faster for few indices.  It builds a string of only the desired values so that <code>np.fromstring</code> does less work.</p>

<pre><code>def get_pos_nextvalues_fewindices(pos_file, indices):
    result = ''
    for line in pos_file:
        if len(line) &gt; 1:
            s = line.split()
            for i in indices:
                result += s[i] + ' '
        else:
            return np.array([])
    result = np.fromstring(result, dtype=float, sep=' ')
    result = result.reshape(result.size/len(indeces), len(indeces))
    return result
</code></pre>

<p>This trades off the overhead of <code>split()</code> and an added loop for less parsing.  Or perhaps there's some clever regex trick you can do to extract the desired substrings directly?</p>

<p><strong>Old Answer</strong></p>

<p><code>np.mat('1.23 2.34 3.45 6\n1.32 2.43 7 3.54')</code> converts the string to a <code>numpy</code> matrix of floating point values.  This might be a faster kernel for you to use.  For instance:</p>

<pre><code>import numpy as np
def ReadFileChunk(pos_file):
    chunktxt = ""
    for line in pos_file:
        if len(line) &gt; 1:
            chunktxt = chunktxt + line
        else:
            break

    return np.mat(chunktxt).tolist()
    # or alternatively
    #return np.array(np.mat(s))
</code></pre>

<p>Then you can move your indexing stuff to another function.  Hopefully having <code>numpy</code> parse the string internally is faster than calling <code>float()</code> repetitively. </p>
<br />