<h3>Question (ID-160889):</h3><h2>Emulation of lex like functionality in Perl or Python</h2><p>Here's the deal. Is there a way to have strings tokenized in a line based on multiple regexes?</p>

<p>One example:</p>

<p>I have to get all href tags, their corresponding text and some other text based on a different regex.
So I have 3 expressions and would like to tokenize the line and extract tokens of text matching every expression.</p>

<p>I have actually done this using flex (not to be confused with Adobe), which is an implementation of the good old lex. lex provides
an elegant way to do this by executing "actions" based on expressions. One can control the way
lex reading a file too (block / line based read).</p>

<p>The problem is that flex actually produces C/ C++ code which actually does the tokenizing job. I have a 
make file which wraps all these things.
I was wondering if perl /python can in some way do the same thing. Its just that I would like to do everything
I like in a single programming language itself.</p>

<p>Tokenizing is just one of the things that I want to do as part of my application.</p>

<p>Apart from perl or python can any language (functional also) do this?</p>

<p>I did read about PLY and ANTLR here (<a href="http://stackoverflow.com/questions/34081/parsing-where-can-i-learn-about-it#34085">http://stackoverflow.com/questions/34081/parsing-where-can-i-learn-about-it#34085</a>).</p>

<p>But is there a way to do it naturally in python itself? pardon my ignorance, but are these tools used in any popular products / services?</p>

<p>Thank you.</p>
<br /><h3>Answers (Total-9):</h3><b>#0</b><br /><p>Look at documentation for following modules on <a href="http://www.cpan.org/" rel="nofollow">CPAN</a></p>

<p><a href="http://search.cpan.org/search?query=HTML%3A%3ATreeBuilder&amp;mode=all" rel="nofollow">HTML::TreeBuilder</a></p>

<p><a href="http://search.cpan.org/author/MSISK/HTML-TableExtract-2.10/lib/HTML/TableExtract.pm" rel="nofollow">HTML::TableExtract</a></p>

<p>and</p>

<p><a href="http://search.cpan.org/search?query=parse+recdescent&amp;mode=all" rel="nofollow">Parse::RecDescent</a></p>

<p>I've used these modules to process quite large and complex web-pages.</p>
<br /><b>#1</b><br /><p>If you're specifically after parsing links out of web-pages, then Perl's <a href="http://search.cpan.org/perldoc?WWW::Mechanize" rel="nofollow">WWW::Mechanize</a> module will figure things out for you in a very elegant fashion.  Here's a sample program that grabs the first page of Stack Overflow and parses out all the links, printing their text and corresponding URLs:</p>

<pre><code>#!/usr/bin/perl
use strict;
use warnings;
use WWW::Mechanize;

my $mech = WWW::Mechanize-&gt;new;

$mech-&gt;get("http://stackoverflow.com/");

$mech-&gt;success or die "Oh no! Couldn't fetch stackoverflow.com";

foreach my $link ($mech-&gt;links) {
    print "* [",$link-&gt;text, "] points to ", $link-&gt;url, "\n";
}
</code></pre>

<p>In the main loop, each <code>$link</code> is a <a href="http://search.cpan.org/perldoc?WWW::Mechanize::Link" rel="nofollow">WWW::Mechanize::Link</a> object, so you're not just constrained to getting the text and URL.</p>

<p>All the best,</p>

<p>Paul</p>
<br /><b>#2</b><br /><p>Sounds like you really just want to parse HTML, I recommend looking at any of the wonderful packages for doing so:</p>

<ul>
<li><a href="http://crummy.com/software/BeautifulSoup" rel="nofollow">BeautifulSoup</a></li>
<li><a href="http://codespeak.net/lxml/lxmlhtml.html" rel="nofollow">lxml.html</a></li>
<li><a href="http://code.google.com/p/html5lib/" rel="nofollow">html5lib</a></li>
</ul>

<p>Or! You can use a parser like one of the following:</p>

<ul>
<li><a href="http://pyparsing.wikispaces.com" rel="nofollow">PyParsing</a></li>
<li><a href="http://dparser.sourceforge.net/" rel="nofollow">DParser</a> - A GLR parser with good python bindings.</li>
<li><a href="http://www.antlr.org" rel="nofollow">ANTLR</a> - A recursive decent parser generator that can generate python code.</li>
</ul>

<p>This example is from the BeautifulSoup <a href="http://www.crummy.com/software/BeautifulSoup/documentation.html" rel="nofollow">Documentation</a>:</p>

<pre><code>from BeautifulSoup import BeautifulSoup, SoupStrainer
import re

links = SoupStrainer('a')
[tag for tag in BeautifulSoup(doc, parseOnlyThese=links)]
# [&lt;a href="http://www.bob.com/"&gt;success&lt;/a&gt;, 
#  &lt;a href="http://www.bob.com/plasma"&gt;experiments&lt;/a&gt;, 
#  &lt;a href="http://www.boogabooga.net/"&gt;BoogaBooga&lt;/a&gt;]

linksToBob = SoupStrainer('a', href=re.compile('bob.com/'))
[tag for tag in BeautifulSoup(doc, parseOnlyThese=linksToBob)]
# [&lt;a href="http://www.bob.com/"&gt;success&lt;/a&gt;, 
#  &lt;a href="http://www.bob.com/plasma"&gt;experiments&lt;/a&gt;]
</code></pre>
<br /><b>#3</b><br /><p>Have you looked at <a href="http://pyparsing.wikispaces.com/" rel="nofollow">PyParsing</a>?</p>

<p>From their homepage:</p>

<p>Here is a program to parse "Hello, World!" (or any greeting of the form ", !"):</p>

<pre><code>from pyparsing import Word, alphas
greet = Word( alphas ) + "," + Word( alphas ) + "!" # &lt;-- grammar defined here
hello = "Hello, World!"
print hello, "-&gt;", greet.parseString( hello )
</code></pre>

<p>The program outputs the following:</p>

<pre><code>Hello, World! -&gt; ['Hello', ',', 'World', '!']
</code></pre>
<br /><b>#4</b><br /><p>If your problem has anything at all to do with web scraping, I recommend looking at <a href="http://search.cpan.org/perldoc?Web::Scraper" rel="nofollow">Web::Scraper</a> , which provides easy element selection via XPath respectively CSS selectors. I have a (German) <a href="http://datenzoo.de/pub/gpw2008/web-scraper/web-scraper-talk.html" rel="nofollow">talk on Web::Scraper</a> , but if you run it through babelfish or just look at the code samples, that can help you to get a quick overview of the syntax.</p>

<p>Hand-parsing HTML is onerous and won't give you much over using one of the premade HTML parsers. If your HTML is of very limited variation, you can get by by using clever regular expressions, but if you're already breaking out hard-core parser tools, it sounds as if your HTML is far more regular than what is sane to parse with regular expressions.</p>
<br /><b>#5</b><br /><p>Also check out <a href="http://search.cpan.org/dist/pQuery/" rel="nofollow">pQuery</a> it as a really nice Perlish way of doing this kind of stuff....</p>

<pre><code>use pQuery;

pQuery( 'http://www.perl.com' )-&gt;find( 'a' )-&gt;each( 
    sub {
        my $pQ = pQuery( $_ ); 
        say $pQ-&gt;text, ' -&gt; ', $pQ-&gt;toHtml;
    }
);

# prints all HTML anchors on www.perl.com
# =&gt;  link text -&gt; anchor HTML
</code></pre>

<p>However if your requirement is beyond HTML/Web then here is the earlier "Hello World!" example in <a href="http://search.cpan.org/dist/Parse-RecDescent/lib/Parse/RecDescent.pm" rel="nofollow">Parse::RecDescent</a>...</p>

<pre><code>use strict;
use warnings;
use Parse::RecDescent;

my $grammar = q{
    alpha : /\w+/
    sep   : /,|\s/
    end   : '!'
    greet : alpha sep alpha end { shift @item; return \@item }
};

my $parse = Parse::RecDescent-&gt;new( $grammar );
my $hello = "Hello, World!";
print "$hello -&gt; @{ $parse-&gt;greet( $hello ) }";

# =&gt; Hello, World! -&gt; Hello , World !
</code></pre>

<p>Probably too much of a large hammer to crack this nut ;-)</p>

<p>/I3az/</p>
<br /><b>#6</b><br /><p>From  <a href="http://perldoc.perl.org/perlop.html#Regexp-Quote-Like-Operators" rel="nofollow">perlop</a>:</p>

<blockquote>
  <p>A useful idiom for lex -like scanners
  is <code>/\G.../gc</code> . You can combine
  several regexps like this to process a
  string part-by-part, doing different
  actions depending on which regexp
  matched. Each regexp tries to match
  where the previous one leaves off.</p>

<pre><code> LOOP:
    {
      print(" digits"),   	redo LOOP if /\G\d+\b[,.;]?\s*/gc;
      print(" lowercase"),    redo LOOP if /\G[a-z]+\b[,.;]?\s*/gc;
      print(" UPPERCASE"),    redo LOOP if /\G[A-Z]+\b[,.;]?\s*/gc;
      print(" Capitalized"),  redo LOOP if /\G[A-Z][a-z]+\b[,.;]?\s*/gc;
      print(" MiXeD"),    	redo LOOP if /\G[A-Za-z]+\b[,.;]?\s*/gc;
      print(" alphanumeric"), redo LOOP if /\G[A-Za-z0-9]+\b[,.;]?\s*/gc;
      print(" line-noise"),   redo LOOP if /\G[^A-Za-z0-9]+/gc;
      print ". That's all!\n";
    }
</code></pre>
</blockquote>
<br /><b>#7</b><br /><p>Modifying Bruno's example to include error checking:</p>

<pre><code>my $input = "...";
while (1) {
    if ($input =~ /\G(\w+)/gc) { print "word: '$1'\n"; next }
    if ($input =~ /\G(\s+)/gc) { print "whitespace: '$1'\n"; next }

    if ($input !~ /\G\z/gc)  { print "tokenizing error at character " . pos($input) . "\n" }
    print "done!\n"; last;
}
</code></pre>

<p>(Note that using scalar //g is unfortunately the one place where you really can't avoid using the $1, etc. variables.)</p>
<br /><b>#8</b><br /><p>Oh, yes. Look <a href="http://stackoverflow.com/questions/133886/simple-regex-based-lexer-in-python">here</a>.</p>
<br />