<h3>Question (ID-3803673):</h3><h2>Optimizing mean in python</h2><p>I have a function which updates the centroid (mean) in a K-means algoritm.
I ran a profiler and noticed that this function uses a lot of computing time.</p>

<p>It looks like:</p>

<pre><code>def updateCentroid(self, label):
    X=[]; Y=[]
    for point in self.clusters[label].points:
        X.append(point.x)
        Y.append(point.y)
    self.clusters[label].centroid.x = numpy.mean(X)
    self.clusters[label].centroid.y = numpy.mean(Y)
</code></pre>

<p>So I ponder, is there a more efficient way to calculate the mean of these points?
If not, is there a more elegant way to formulate it? ;)</p>

<p>EDIT:</p>

<p>Thanks for all great responses!
I was thinking that perhaps I can calculate the mean cumulativly, using something like:
<img src="http://i.stack.imgur.com/SolBc.png" alt="alt text"></p>

<p>where x_bar(t) is the new mean and x_bar(t-1) is the old mean.</p>

<p>Which would result in a function similar to this:</p>

<pre><code>def updateCentroid(self, label):
    cluster = self.clusters[label]
    n = len(cluster.points)
    cluster.centroid.x *= (n-1) / n
    cluster.centroid.x += cluster.points[n-1].x / n
    cluster.centroid.y *= (n-1) / n
    cluster.centroid.y += cluster.points[n-1].y / n
</code></pre>

<p>Its not really working but do you think this could work with some tweeking?</p>
<br /><h3>Answers (Total-9):</h3><b>#0</b><br /><p>A K-means algorithm is already implemented in <a href="http://docs.scipy.org/doc/scipy/reference/cluster.vq.html" rel="nofollow">scipy.cluster.vq</a>. If there is something about that implementation that you are trying to change, then I'd suggest start by studying the code there:</p>

<pre><code>In [62]: import scipy.cluster.vq as scv
In [64]: scv.__file__
Out[64]: '/usr/lib/python2.6/dist-packages/scipy/cluster/vq.pyc'
</code></pre>

<p>PS. Because the algorithm you posted holds the data behind a dict (<code>self.clusters</code>) and attribute lookup (<code>.points</code>) you are forced to use slow Python looping just to get at your data. A major speed gain could be achieved by sticking with numpy arrays. See the scipy implementation of k-means clustering for ideas on a better data structure.</p>
<br /><b>#1</b><br /><p>Why not avoid constructing the extra arrays?</p>

<pre><code>def updateCentroid(self, label):
  sumX=0; sumY=0
  N = len( self.clusters[label].points)
  for point in self.clusters[label].points:
    sumX += point.x
    sumY += point.y
  self.clusters[label].centroid.x = sumX/N
  self.clusters[label].centroid.y = sumY/N
</code></pre>
<br /><b>#2</b><br /><p>The costly part of your function is most certainly the iteration over the points. Avoid it altogether by making <code>self.clusters[label].points</code> a numpy array itself, and then compute the mean directly on it. For example if points contains X and Y coordinates concatenated in a 1D array:</p>

<pre><code>points = self.clusters[label].points
x_mean = numpy.mean(points[0::2])
y_mean = numpy.mean(points[1::2])
</code></pre>
<br /><b>#3</b><br /><p>Without extra lists:</p>

<pre><code>def updateCentroid(self, label):
    self.clusters[label].centroid.x = numpy.fromiter(point.x for point in self.clusters[label].points, dtype = np.float).mean()
    self.clusters[label].centroid.y = numpy.fromiter(point.y for point in self.clusters[label].points, dtype = np.float).mean()
</code></pre>
<br /><b>#4</b><br /><p>Perhaps the added features of numpy's <code>mean</code> are adding a bit of overhead.</p>

<pre><code>&gt;&gt;&gt; def myMean(itr):
...   c = t = 0
...   for item in itr:
...     c += 1
...     t += item
...   return t / c
...
&gt;&gt;&gt; import timeit
&gt;&gt;&gt; a = range(20)
&gt;&gt;&gt; t1 = timeit.Timer("myMean(a)","from __main__ import myMean, a")
&gt;&gt;&gt; t1.timeit()
6.8293311595916748
&gt;&gt;&gt; t2 = timeit.Timer("average(a)","from __main__ import a; from numpy import average")
&gt;&gt;&gt; t2.timeit()
69.697283029556274
&gt;&gt;&gt; t3 = timeit.Timer("average(array(a))","from __main__ import a; from numpy import average, array")
&gt;&gt;&gt; t3.timeit()
51.65147590637207
&gt;&gt;&gt; t4 = timeit.Timer("fromiter(a,npfloat).mean()","from __main__ import a; from numpy import average, fromiter,float as npfloat")
&gt;&gt;&gt; t4.timeit()
18.513712167739868
</code></pre>

<p>Looks like numpy's best performance came when using <code>fromiter</code>.</p>
<br /><b>#5</b><br /><p>Ok, I figured out a moving average solution which is fast without changing the data structures:</p>

<pre><code>def updateCentroid(self, label):
    cluster = self.clusters[label]
    n = len(cluster.points)
    cluster.centroid.x = ((n-1)*cluster.centroid.x + cluster.points[n-1].x)/n
    cluster.centroid.y = ((n-1)*cluster.centroid.y + cluster.points[n-1].y)/n
</code></pre>

<p>This lowered computation time (for the whole k means algorithm) to 13% of original. =)</p>

<p>Thank you all for some great insight!</p>
<br /><b>#6</b><br /><p>Try this:</p>

<pre><code>def updateCentroid(self, label):

    self.clusters[label].centroid.x = numpy.array([point.x for point in self.clusters[label].points]).mean()
    self.clusters[label].centroid.y = numpy.array([point.y for point in self.clusters[label].points]).mean()
</code></pre>
<br /><b>#7</b><br /><p>That's the problem with profilers that only tell you about functions. <a href="http://stackoverflow.com/questions/375913/what-can-i-use-to-profile-c-code-in-linux/378024#378024">This is the method I use</a>, and it pinpoints costly lines of code, including points where functions are called.</p>

<p>That said, there's a general idea that data structure is free. As @Michael-Anderson asked, why not avoid making an array? That's the first thing I saw in your code, that you're building arrays by appending. You don't need to.</p>
<br /><b>#8</b><br /><p>One way to go is add an x_sum and y_sum to your "clusters" object and sum the coordinates as points are added. If things are moving around, you can also update the sum as points move. Then getting the centroid is just a matter of dividing the x_sum and y_sum by the number of points. If your points are numpy vectors that can be added, then you don't even need to sum the components, just maintain a sum of all the vectors and multiply be 1/len at the end.</p>
<br />