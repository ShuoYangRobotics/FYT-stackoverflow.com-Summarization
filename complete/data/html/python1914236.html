<h3>Question (ID-1914236):</h3><h2>Python string pattern recognition/compression</h2><p>I can do basic regex alright, but this is slightly different, namely I don't know what the pattern is going to be.</p>

<p>For example, I have a list of similar strings:</p>

<pre><code>lst = ['asometxt0moretxt', 'bsometxt1moretxt', 'aasometxt10moretxt', 'zzsometxt999moretxt']
</code></pre>

<p>In this case the common pattern is two segments of common text: <code>'sometxt'</code> and <code>'moretxt'</code>, starting and separated by something else that is variable in length.</p>

<p>The common string and variable string can of course occur at any order and at any number of occasions.</p>

<p>What would be a good way to condense/compress the list of strings into their common parts and individual variations?</p>

<p>An example output might be:</p>

<pre><code>c = ['sometxt', 'moretxt']

v = [('a','0'), ('b','1'), ('aa','10'), ('zz','999')]
</code></pre>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>Here is a scary one to get the ball rolling.</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; makere = lambda n: ''.join(['(.*?)(.+)(.*?)(.+)(.*?)'] + ['(.*)(\\2)(.*)(\\4)(.*)'] * (n - 1))
&gt;&gt;&gt; inp = ['asometxt0moretxt', 'bsometxt1moretxt', 'aasometxt10moretxt', 'zzsometxt999moretxt']
&gt;&gt;&gt; re.match(makere(len(inp)), ''.join(inp)).groups()
('a', 'sometxt', '0', 'moretxt', '', 'b', 'sometxt', '1', 'moretxt', 'aa', '', 'sometxt', '10', 'moretxt', 'zz', '', 'sometxt', '999', 'moretxt', '')
</code></pre>

<p>I hope its sheer ugliness will inspire better solutions :)</p>
<br /><b>#1</b><br /><p>This solution finds the two longest common substrings and uses them to delimit the input strings:</p>

<pre><code>def an_answer_to_stackoverflow_question_1914394(lst):
    """
    &gt;&gt;&gt; lst = ['asometxt0moretxt', 'bsometxt1moretxt', 'aasometxt10moretxt', 'zzsometxt999moretxt']
    &gt;&gt;&gt; an_answer_to_stackoverflow_question_1914394(lst)
    (['sometxt', 'moretxt'], [('a', '0'), ('b', '1'), ('aa', '10'), ('zz', '999')])
    """
    delimiters = find_delimiters(lst)
    return delimiters, list(split_strings(lst, delimiters))
</code></pre>

<p><code>find_delimiters</code> and friends finds the delimiters:</p>

<pre><code>import itertools

def find_delimiters(lst):
    """
    &gt;&gt;&gt; lst = ['asometxt0moretxt', 'bsometxt1moretxt', 'aasometxt10moretxt', 'zzsometxt999moretxt']
    &gt;&gt;&gt; find_delimiters(lst)
    ['sometxt', 'moretxt']
    """
    candidates = list(itertools.islice(find_longest_common_substrings(lst), 3))
    if len(candidates) == 3 and len(candidates[1]) == len(candidates[2]):
        raise ValueError("Unable to find useful delimiters")
    if candidates[1] in candidates[0]:
        raise ValueError("Unable to find useful delimiters")
    return candidates[0:2]

def find_longest_common_substrings(lst):
    """
    &gt;&gt;&gt; lst = ['asometxt0moretxt', 'bsometxt1moretxt', 'aasometxt10moretxt', 'zzsometxt999moretxt']
    &gt;&gt;&gt; list(itertools.islice(find_longest_common_substrings(lst), 3))
    ['sometxt', 'moretxt', 'sometx']
    """
    for i in xrange(min_length(lst), 0, -1):
        for substring in common_substrings(lst, i):
            yield substring


def min_length(lst):
    return min(len(item) for item in lst)

def common_substrings(lst, length):
    """
    &gt;&gt;&gt; list(common_substrings(["hello", "world"], 2))
    []
    &gt;&gt;&gt; list(common_substrings(["aabbcc", "dbbrra"], 2))
    ['bb']
    """
    assert length &lt;= min_length(lst)
    returned = set()
    for i, item in enumerate(lst):
        for substring in all_substrings(item, length):
            in_all_others = True
            for j, other_item in enumerate(lst):
                if j == i:
                    continue
                if substring not in other_item:
                    in_all_others = False
            if in_all_others:
                if substring not in returned:
                    returned.add(substring)
                    yield substring

def all_substrings(item, length):
    """
    &gt;&gt;&gt; list(all_substrings("hello", 2))
    ['he', 'el', 'll', 'lo']
    """
    for i in range(len(item) - length + 1):
        yield item[i:i+length]
</code></pre>

<p><code>split_strings</code> splits the strings using the delimiters:</p>

<pre><code>import re

def split_strings(lst, delimiters):
    """
    &gt;&gt;&gt; lst = ['asometxt0moretxt', 'bsometxt1moretxt', 'aasometxt10moretxt', 'zzsometxt999moretxt']
    &gt;&gt;&gt; list(split_strings(lst, find_delimiters(lst)))
    [('a', '0'), ('b', '1'), ('aa', '10'), ('zz', '999')]
    """
    for item in lst:
        parts = re.split("|".join(delimiters), item)
        yield tuple(part for part in parts if part != '')
</code></pre>
<br /><b>#2</b><br /><p>This seems to be an example of the <a href="http://en.wikipedia.org/wiki/Longest%5Fcommon%5Fsubsequence%5Fproblem" rel="nofollow">longest common subsequence problem</a>. One way could be to look at how <a href="http://en.wikipedia.org/wiki/Diff" rel="nofollow">diffs</a> are generated. The <a href="http://en.wikipedia.org/wiki/Hunt-McIlroy%5Falgorithm" rel="nofollow">Hunt-McIlroy algorithm</a> seems to have been the first, and is such the simplest, especially since it apparently is non-heuristic.</p>

<p>The first link contains detailed discussion and (pseudo) code examples. Assuming, of course, Im not completely of the track here.</p>
<br /><b>#3</b><br /><p>I guess you should start by identifying substrings (patterns) that frequently occur in the strings. Since naively counting substrings in a set of strings is rather computationally expensive, you'll need to come up with something smart.</p>

<p>I've done substring counting on a large amount of data using <a href="http://en.wikipedia.org/wiki/Generalised%5Fsuffix%5Ftree" rel="nofollow">generalized suffix trees</a> <a href="http://cosmion.net/jeroen/software/gst/" rel="nofollow">(example here)</a>. Once you know the most frequent substrings/patterns in the data, you can take it from there.</p>
<br /><b>#4</b><br /><p>This look much like the <a href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch" rel="nofollow">LZW</a> algorithm for data (text) compression. There should be python implementations out there, which you may be able to adapt to your need.</p>

<p>I assume you have no a priori knowledge of these sub strings that repeat often.</p>
<br /><b>#5</b><br /><p>How about subbing out the known text, and then splitting?</p>

<pre><code>import re
[re.sub('(sometxt|moretxt)', ',', x).split(',') for x in lst]
# results in
[['a', '0', ''], ['b', '1', ''], ['aa', '10', ''], ['zz', '999', '']]
</code></pre>
<br />