<h3>Question (ID-1805148):</h3><h2>Why is (python|ruby) interpreted?</h2><p>What are the technical reasons why languages like Python and Ruby are interpreted (out of the box) instead of compiled? It seems to me like it should not be too hard for people knowledgeable in this domain to make these languages not be interpreted like they are today, and we would see significant performance gains. So certainly I am missing something. </p>
<br /><h3>Answers (Total-12):</h3><b>#0</b><br /><p>Several reasons:</p>

<ul>
<li>faster development loop, <em>write-test</em> vs <em>write-compile-link-test</em></li>
<li>easier to arrange for dynamic behavior (reflection, metaprogramming)</li>
<li>makes the whole system portable (just recompile the underlying C code and you are good to go on a new platform)</li>
</ul>

<p>Think of what would happen if the system was <em>not</em> interpreted. Say you used translation-to-C as the mechanism. The compiled code would periodically have to check if it had been superseded by metaprogramming. A similar situation arises with <code>eval()</code>-type functions.  In those cases, it would have to run the compiler again, an outrageously slow process, or it would have to <em>also</em> have the interpreter around at run-time anyway. </p>

<p>The only alternative here is a JIT compiler. These systems are highly complex and sophisticated and have even bigger run-time footprints than all the other alternatives. They start up very slowly, making them impractical for scripting. Ever seen a Java script? I haven't.</p>

<p>So, you have two choices:</p>

<ul>
<li>all the disadvantages of both a compiler and an interpreter</li>
<li>just the disadvantages of an interpreter</li>
</ul>

<p>It's not surprising that generally the primary implementation just goes with the second choice. It's quite possible that some day we may see secondary implementations like compilers appearing. Ruby 1.9 and Python have bytecode VM's; those are &frac12;-way there. A compiler might target just non-dynamic code, or it might have various levels of language support declarable as options. But since such a thing can't be the primary implementation, it represents a lot of work for a very marginal benefit. Ruby already has 200,000 lines of C in it...</p>

<p>I suppose I should add that one can always add a compiled C (or, with some effort, any other language) extension. So, say you have a slow numerical operation. If you add, say <code>Array#newOp</code> with a C implementation then you get the speedup, the program stays in Ruby (or whatever) and your environment gets a new instance method. Everybody wins! So this reduces the need for a problematic secondary implementation.</p>
<br /><b>#1</b><br /><p>Exactly like (in the typical implementation of) Java or C#, Python gets first compiled into some form of bytecode, depending on the implementation (CPython uses a specialized form of its own, Jython uses JVM just like a typical Java, IronPython uses CLR just like a typical C#, and so forth) -- that bytecode then gets further processed for execution by a virtual machine (AKA interpreter), which may also generate machine code "just in time" -- known as JIT -- if and when warranted (CLR and JVM implementations often do, CPython's own virtual machine typically doesn't but can be made to do so e.g. with psyco or Unladen Swallow).</p>

<p>JIT may pay for itself for sufficiently long-running programs (if memory's way cheaper than CPU cycles), but it may not (due to slower startup times and larger memory footprint), especially when the types also have to be inferred or specialized as part of the code generation. Generating machine code without type inference or specialization is easy if that's what you want, e.g. <a href="http://wiki.python.org/moin/Freeze" rel="nofollow">freeze</a> does it for you, but it really doesn't present the advantages that "machine code fetishists" attribute to it.  E.g., you get an executable binary of 1.5 to 2 MB in lieu of a tiny "hello world" <code>.pyc</code> -- not much point!-).  That executable is stand-alone and distributable as such, but it will only work on a very specific narrow range of operating systems and CPU architectures, so the tradeoffs are quite iffy in most cases. And, the time it takes to prepare the executable is quite long indeed, so it would be a crazy choice to make that mode of operation the default one.</p>
<br /><b>#2</b><br /><p>Merely replacing an interpreter with a compiler won't give you as big a performance boost as you might think for a language like Python. When most time is actually spend doing symbolic lookups of object members in dictionaries, it doesn't really matter if the call to the function performing such lookup is interpreted, or is native machine code - the difference, while not quite negligible, will be dwarfed by lookup overhead.</p>

<p>To really improve performance, you need optimizing compilers. And optimization techniques here are very different from what you have with C++, or even Java JIT - an optimizing compiler for a dynamically typed / duck typed language such as Python needs to do some very creative type inference (including probabilistic - i.e. "90% chance of it being T" and then generating efficient machine code for that case with a check/branch before it) and escape analysis. This is hard.</p>
<br /><b>#3</b><br /><p>I think the biggest reason for the languages being interpreted is portability. As a programmer you can write code that will run in an interpreter not a specific OS. So your programs behave more uniformly across platforms (more so than compiled languages). Another advantage I can think of is it's easier to have a dynamic type system in an interpreted language. I think the creators of the language were thinking having a language where programmers can be more productive due to automatic memory management, dynamic type system and meta programming wins over any performance loss due to the language being interpreted. If you are concerned about performance you can always compile the language to native machine code employing a technique like JIT compilation.</p>
<br /><b>#4</b><br /><p>Today, there is no longer a strong distinction between "compiled" and "interpreted" languages. Python is in fact compiled just as much as Java is, the only differences are:</p>

<ul>
<li>The Python compiler is much faster than the Java compiler</li>
<li>Python automatically compiles source code as it is executed, there is no separate "compile" step required</li>
<li>Python bytecode is different from JVM bytecode</li>
</ul>

<p>Python even has a function called <a href="http://docs.python.org/library/functions.html#compile" rel="nofollow"><code>compile()</code></a> which is an interface to the compiler.</p>

<p>It sounds like the distinction you are making is between "dynamically typed" and "statically typed" languages. In dynamic languages such as Python, you can write code like:</p>

<pre><code>def fn(x, y):
    return x.foo(y)
</code></pre>

<p>Notice that the types of <code>x</code> and <code>y</code> are not specified. At runtime, this function will look at <code>x</code> to see whether it has a member function named <code>foo</code>, and if so will call it with <code>y</code>. If not, it will throw a runtime error that indicates no such function was found. This sort of runtime lookup is much easier to represent using an intermediate representation like bytecode, where a runtime VM does the lookup instead of having to generate machine code to do the lookup itself (or, call a function to do the lookup which is what the bytecode will do anyway).</p>

<p>Python has projects such as <a href="http://psyco.sourceforge.net/" rel="nofollow">Psyco</a>, <a href="http://codespeak.net/pypy/dist/pypy/doc/" rel="nofollow">PyPy</a>, and <a href="http://code.google.com/p/unladen-swallow/" rel="nofollow">Unladen Swallow</a> that take various approaches to compiling Python object code into something closer to native code. There is active research in this area but there is not (as yet) a simple answer.</p>
<br /><b>#5</b><br /><p>The effort required to create a good compiler to generate native code for a new language is <em>staggering</em>.  Small research groups typically take 5 to 10 years (examples: <a href="http://www.smlnj.org/" rel="nofollow">SML/NJ</a>, Haskell, Clean, Cecil, <a href="http://www.cs.princeton.edu/software/lcc/" rel="nofollow">lcc</a>, <a href="http://caml.inria.fr/" rel="nofollow">Objective Caml</a>, <a href="http://mlton.org/" rel="nofollow">MLton</a>, and many others).  And when the language in question requires type checking and other decisions to be made at run time, a compiler writer has to work much harder to get good native-code performance (for an excellent example, see work by Craig Chambers and later Urs Hoelzle on Self).  <strong>The performance gains you might hope for are harder to realize than you might think.</strong> This phenomenon partly explains <a href="http://stackoverflow.com/questions/376611/why-interpreted-langs-are-mostly-ducktyped-while-compiled-have-strong-typing/376828#376828">why so many dynamically typed languages are interpreted</a>.</p>

<p>As noted, a decent interpreter is also instantly portable, while porting compilers to new machine architectures takes substantial effort (and is a problem I personally have been working on for over 20 years, with some time off for good behavior).  So an interpreter is a way to reach a wide audience quickly.</p>

<p>Finally, although fast compilers and slow interpreters exist, it's usually easer to make the edit-translate-go cycle faster by using an interpreter.  (For some nice examples of fast compilers see the aforementioned <a href="http://www.cs.princeton.edu/software/lcc/" rel="nofollow">lcc</a> as well as Ken Thompson's <a href="http://golang.org/" rel="nofollow">go</a> compiler.  For an example of a relatively slow interpreter see <a href="http://www.haskell.org/ghc/" rel="nofollow">GHCi</a>.</p>
<br /><b>#6</b><br /><p>Well, isn't one of the strengths of these languages that they are so easily scriptable? They wouldn't be if they were compiled. And on the other hand, dynamic languages are easier to intereprete than to compile.</p>
<br /><b>#7</b><br /><p>In a compiled language, the loop you get into when making software is</p>

<ol>
<li>Make a change</li>
<li>Compile changes</li>
<li>Test changes</li>
<li>goto 1</li>
</ol>

<p>Interpreted languages tend to be faster to make stuff in because you get to cut out step two of that process (and when you're dealing with a large system where compile times can be upwards of two minutes, step two can add a significant amount of time).</p>

<p>This isn't necessarily the reason python|ruby designers thought of, but keep in mind that "How efficiently does the machine run this?" is only half the software development problem.</p>

<p>It also seems like it would be easier to compile code in a language that's interpreted naturally than it would be to add an interpreter to a language that's compiled by default.</p>
<br /><b>#8</b><br /><p><a href="http://en.wikipedia.org/wiki/Read-eval-print%5Floop" rel="nofollow">REPL</a>.  Don't knock it 'till you've tried it. :)</p>
<br /><b>#9</b><br /><p>By design. </p>

<p>The authors wanted something where they can write scripts into.</p>

<p>Python gets compiled the first time it is executed though </p>
<br /><b>#10</b><br /><p>Compiling Ruby at least is notoriously hard. I'm working on one, and as part of that I wrote a blog post <a href="http://www.hokstad.com/the-problem-with-compiling-ruby.html" rel="nofollow">enumerating some of the issues here</a>. </p>

<p>Specifically, Ruby is suffering from a very unclear (i.e. non-existent) boundary between the "read" and "execute" phase of the program that makes it hard to compile efficiently. You could just emulate what the interpreter does, but then you're not going to see much speed up, so it wouldn't be worth the effort. If you want to compile it efficiently you then face a lot of additional complications to handle the extreme level of dynamism in Ruby.</p>

<p>The good news is that there <em>are</em> techniques for overcoming this. Self, Smalltalk and Lisp/Scheme's have dealt quite successfully with most of the same issues. But it takes time to sift through it and figure out how to make it work with Ruby. It also doesn't help that Ruby has a very convoluted grammar.</p>
<br /><b>#11</b><br /><p>Raw compute performance is probably not a goal of most interpreted languages. Interpreted languages are typically more concerned about programmer productivity than raw speed. In most cases these languages are plenty fast enough for the tasks the languages were designed to tackle. </p>

<p>Given that, and that just about the only advantages of a compiler are type checking (difficult to do in a dynamic language) and speed, there's not much incentive to write compilers for most interpreted languages.</p>
<br />