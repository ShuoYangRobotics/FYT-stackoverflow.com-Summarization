<h3>Question (ID-914382):</h3><h2>How can I count unique terms in a plaintext file case-insensitively?</h2><p>This can be in any high-level language that is likely to be available on a typical unix-like system (Python, Perl, awk, standard unix utils {sort, uniq}, etc). Hopefully it's fast enough to report the total number of unique terms for a 2MB text file.</p>

<p>I only need this for quick sanity-checking, so it doesn't need to be well-engineered.</p>

<p>Remember, case-insensitve.</p>

<p>Thank you guys very much.</p>

<p>Side note: If you use Python, please don't use version 3-only code. The system I'm running it on only has 2.4.4.</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><p>Using bash/UNIX commands:</p>

<pre><code>sed -e 's/[[:space:]]\+/\n/g' $FILE | sort -fu | wc -l
</code></pre>
<br /><b>#1</b><br /><p>In Perl:</p>

<pre><code>my %words; 
while (&lt;&gt;) { 
    map { $words{lc $_} = 1 } split /\s/); 
} 
print scalar keys %words, "\n";
</code></pre>
<br /><b>#2</b><br /><p>In Python 2.4 (possibly it works on earlier systems as well):</p>

<pre><code>#! /usr/bin/python2.4
import sys
h = set()
for line in sys.stdin.xreadlines():
  for term in line.split():
    h.add(term)
print len(h)
</code></pre>

<p>In Perl:</p>

<pre><code>$ perl -ne 'for (split(" ", $_)) { $H{$_} = 1 } END { print scalar(keys%H), "\n" }' &lt;file.txt
</code></pre>
<br /><b>#3</b><br /><p>Using just standard Unix utilities:</p>

<pre><code>&lt; somefile tr 'A-Z[:blank:][:punct:]' 'a-z\n' | sort | uniq -c
</code></pre>

<p>If you're on a system without Gnu <code>tr</code>, you'll need to replace "<code>[:blank:][:punct:]</code>" with a list of all the whitespace and punctuation characters you'd like to consider to be separators of words, rather than part of a word, e.g., "<code> \t.,;</code>".</p>

<p>If you want the output sorted in descending order of frequency, you can append "<code>| sort -r -n</code>" to the end of this.</p>

<p>Note that this will produce an irrelevant count of whitespace tokens as well; if you're concerned about this, after the <code>tr</code> you can use sed to filter out the empty lines.</p>
<br /><b>#4</b><br /><p>Here is a Perl one-liner:</p>

<pre><code>perl -lne '$h{lc $_}++ for split /[\s.,]+/; END{print scalar keys %h}' file.txt
</code></pre>

<p>Or to list the count for each item:</p>

<pre><code>perl -lne '$h{lc $_}++ for split /[\s.,]+/; END{printf "%-12s %d\n", $_, $h{$_} for sort keys %h}' file.txt
</code></pre>

<p>This makes an attempt to handle punctuation so that "foo." is counted with "foo" while "don't" is treated as a single word, but you can adjust the regex to suit your needs. </p>
<br /><b>#5</b><br /><p>Simply (52 strokes):</p>

<pre><code>perl -nE'@w{map lc,split/\W+/}=();END{say 0+keys%w}'
</code></pre>

<p>For older perl versions (55 strokes):</p>

<pre><code>perl -lne'@w{map lc,split/\W+/}=();END{print 0+keys%w}'
</code></pre>
<br /><b>#6</b><br /><p>A shorter version in Python:</p>

<pre><code>print len(set(w.lower() for w in open('filename.dat').read().split()))
</code></pre>

<p>Reads the entire file into memory, splits it into words using whitespace, converts each word to lower case, creates a (unique) set from the lowercase words, counts them and prints the output.</p>

<p>Also possible using a one liner:</p>

<pre><code>python -c "print len(set(w.lower() for w in open('filename.dat').read().split()))"
</code></pre>
<br /><b>#7</b><br /><p>Here is an awk oneliner.</p>

<pre><code>$ gawk -v RS='[[:space:]]' 'NF&amp;&amp;!a[toupper($0)]++{i++}END{print i}' somefile
</code></pre>

<ul>
<li>'NF' means 'if there is a charactor'.</li>
<li>'!a[topuuer[$0]++]' means 'show only 
uniq words'.</li>
</ul>
<br />