<h3>Question (ID-4948509):</h3><h2>Removing duplicated lines from a txt file</h2><p>I am processing large text files (~20MB) containing data delimited by line.
Most data entries are duplicated and I want to remove these duplications to only keep one copy.</p>

<p>Also, to make the problem slightly more complicated, some entries are repeated with an extra bit of info appended. In this case I need to keep the entry containing the extra info and delete the older versions.</p>

<p>e.g.
I need to go from this:
<PRE>
BOB 123 1DB
JIM 456 3DB AX
DAVE 789 1DB
BOB 123 1DB
JIM 456 3DB AX
DAVE 789 1DB
BOB 123 1DB EXTRA BITS
</PRE>
to this:
<PRE>
JIM 456 3DB AX
DAVE 789 1DB
BOB 123 1DB EXTRA BITS
</PRE>
NB. the final order doesn't matter.</p>

<p>What is an efficient way to do this?</p>

<p>I can use awk, python or any standard linux command line tool.</p>

<p>Thanks.</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><p>How about the following (in Python):</p>

<pre><code>prev = None
for line in sorted(open('file')):
  line = line.strip()
  if prev is not None and not line.startswith(prev):
    print prev
  prev = line
if prev is not None:
  print prev
</code></pre>

<p>If you find memory usage an issue, you can do the sort as a pre-processing step using Unix <code>sort</code> (which is <a href="http://vkundeti.blogspot.com/2008/03/tech-algorithmic-details-of-unix-sort.html" rel="nofollow">disk-based</a>) and change the script so that it doesn't read the entire file into memory.</p>
<br /><b>#1</b><br /><p><code>awk '{x[$1 " " $2 " " $3] = $0} END {for (y in x) print x[y]}'</code></p>

<p>If you need to specify the number of columns for different files:</p>

<pre><code>awk -v ncols=3 '
  {
    key = "";
    for (i=1; i&lt;=ncols; i++) {key = key FS $i}
    if (length($0) &gt; length(x[key])) {x[key] = $0}
  }
  END {for (y in x) print y "\t" x[y]}
'
</code></pre>
<br /><b>#2</b><br /><p>This or a slight variant should do:</p>

<pre><code>finalData = {}
for line in input:
    parts = line.split()
    key,extra = tuple(parts[0:3]),parts[3:]
    if key not in finalData or extra:
        finalData[key] = extra

pprint(finalData)
</code></pre>

<p>outputs:</p>

<pre><code>{('BOB', '123', '1DB'): ['EXTRA', 'BITS'],
 ('DAVE', '789', '1DB'): [],
 ('JIM', '456', '3DB'): ['AX']}
</code></pre>
<br /><b>#3</b><br /><p>This variation on glenn jackman's answer should work regardless of the position of lines with extra bits:</p>

<pre><code>awk '{idx = $1 " " $2 " " $3; if (length($0) &gt; length(x[idx])) x[idx] = $0} END {for (idx in x) print x[idx]}' inputfile
</code></pre>

<p>Or</p>

<pre><code>awk -v ncols=3 '
  {
    key = "";
    for (i=1; i&lt;=ncols; i++) {key = key FS $i}
    if (length($0) &gt; length(x[key])) x[key] = $0
  }
  END {for (y in x) print x[y]}
' inputfile
</code></pre>
<br /><b>#4</b><br /><p>You'll have to define a function to split your line into important bits and extra bits, then you can do:</p>

<pre><code>def split_extra(s):
    """Return a pair, the important bits and the extra bits."""
    return blah blah blah

data = {}
for line in open('file'):
    impt, extra = split_extra(line)
    existing = data.setdefault(impt, extra)
    if len(extra) &gt; len(existing):
        data[impt] = extra

out = open('newfile', 'w')
for impt, extra in data.iteritems():
    out.write(impt + extra)
</code></pre>
<br /><b>#5</b><br /><p>Since you need the extra bits the fastest way is to create a set of unique entries (sort -u will do) and then you must compare each entry against each other, e.g.
<pre><code>if x.startswith(y) and not y.startswith(x)</pre></code>
and just leave x and discard y.</p>
<br /><b>#6</b><br /><p>If you have perl and want only the last entry to be preserved :</p>

<pre><code>cat file.txt | perl -ne 'BEGIN{%k={}} @_ = split(/ /);$kw = shift(@_); $kws{$kw} = "@_"; END{ foreach(sort keys %kws){ print "$_ $kws{$_}";} }' &gt; file.new.txt
</code></pre>
<br /><b>#7</b><br /><p>The function <code>find_unique_lines</code> will work for a file object or a list of strings.</p>

<pre><code>import itertools

def split_line(s):
    parts = s.strip().split(' ')
    return " ".join(parts[:3]), parts[3:], s

def find_unique_lines(f):
    result = {}
    for key, data, line in itertools.imap(split_line, f):
        if data or key not in result:
            result[key] = line
    return result.itervalues()

test = """BOB 123 1DB
JIM 456 3DB AX
DAVE 789 1DB
BOB 123 1DB
JIM 456 3DB AX
DAVE 789 1DB
BOB 123 1DB EXTRA BITS""".split('\n')

for line in find_unique_lines(test):
        print line</code></pre>

<pre>BOB 123 1DB EXTRA BITS
JIM 456 3DB AX
DAVE 789 1DB</pre>
<br />