<h3>Question (ID-6728719):</h3><h2>How do I make this list function faster?</h2><pre><code>def removeDuplicatesFromList(seq): 
    # Not order preserving 
    keys = {}
    for e in seq:
        keys[e] = 1
    return keys.keys()

def countWordDistances(li):
    '''
    If li = ['that','sank','into','the','ocean']    
    This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }
    However, if there is a duplicate term, take the average of their positions
    '''
    wordmap = {}
    unique_words = removeDuplicatesFromList(li)
    for w in unique_words:
        distances = [i+1 for i,x in enumerate(li) if x == w]
        wordmap[w] = float(sum(distances)) / float(len(distances)) #take average
    return wordmap
</code></pre>

<p>How do I make this function faster?</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><pre><code>import collections
def countWordDistances(li):
    wordmap = collections.defaultdict(list)
    for i, w in enumerate(li, 1):
        wordmap[w].append(i)
    for k, v in wordmap.iteritems():
        wordmap[k] = sum(v)/float(len(v))

    return wordmap
</code></pre>

<p>This makes only one pass through the list, and keeps operations to a minimum.  I timed this on a word list with 1.1M entries, 29k unique words, and it was almost twice as fast as Patrick's answer.  On a list of 10k words, 2k unique, it was more than 300x faster than the OP's code.</p>

<p>To make Python code go faster, there are two rules to keep in mind: use the best algorithm, and avoid Python.  </p>

<p>On the algorithm front, iterating the list once instead of N+1 times (N= number of unique words) is the main thing that will speed this up.  </p>

<p>On the "avoid Python" front, I mean: you want your code to be executing in C as much as possible.  So using <code>defaultdict</code> is better than a dict where you explicitly check if the key is present.  <code>defaultdict</code> does that check for you, but does it in C, in the Python implementation.  <code>enumerate</code> is better than <code>for i in range(len(li))</code>, again because it's fewer Python steps.   And <code>enumerate(li, 1)</code> makes the counting start at 1 instead of having to have a Python +1 somewhere in the loop.</p>

<p>Edited: Third rule: use PyPy.  My code goes twice as fast on PyPy as on 2.7.</p>
<br /><b>#1</b><br /><p>Based off @Ned Batchelder's solution, but without creating dummy lists:</p>

<pre><code>import collections
def countWordDistances(li):
    wordmap = collections.defaultdict(lambda:[0.0, 0.0])
    for i, w in enumerate(li, 1):
        wordmap[w][0] += i
        wordmap[w][1] += 1.0
    for k, (t, n) in wordmap.iteritems():
        wordmap[k] = t / n
    return wordmap
</code></pre>
<br /><b>#2</b><br /><p>I'm not sure if this will be faster than using a set, but it requires only one pass through the list:</p>

<pre><code>def countWordDistances(li):
    wordmap = {}
    for i in range(len(li)):
        if li[i] in wordmap:
            avg, num = wordmap[li[i]]
            new_avg = avg*(num/(num+1.0)) + (1.0/(num+1.0))*i
            wordmap[li[i]] = new_avg, num+1
        else:
            wordmap[li[i]] = (i, 1)

    return wordmap
</code></pre>

<p>This returns a modified version of wordmap, with the values associated with each key being a tuple of the average position and the number of occurences. You could obviously easily transform this to the form of the original output, but this would take some time.</p>

<p>The code basically keeps a running average while iterating through the list, recalculating each time by taking a weighted average. </p>
<br /><b>#3</b><br /><p>Use a set:</p>

<pre><code>def countWordDistances(li):
    '''
    If li = ['that','sank','into','the','ocean']    
    This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }
    However, if there is a duplicate term, take the average of their positions
    '''
    wordmap = {}
    unique_words = set(li)
    for w in unique_words:
        distances = [i+1 for i,x in enumerate(li) if x == w]
        wordmap[w] = float(sum(distances)) / float(len(distances)) #take average
    return wordmap
</code></pre>
<br /><b>#4</b><br /><p>The first thing that springs to mind is to use a set to remove duplicate words:</p>

<pre><code>unique_words = set(li)
</code></pre>

<p>In general, though, if you're worried about speed you need to profile the function to see where the bottleneck is, and then try to reduce that bottleneck.</p>
<br /><b>#5</b><br /><p>Use a <a href="http://docs.python.org/library/stdtypes.html#set-types-set-frozenset" rel="nofollow"><code>frozenset</code></a> instead of a <code>dict</code>, since you're not doing anything with the values:</p>

<pre><code>def removeDuplicatesFromList(seq):
    return frozenset(seq)
</code></pre>
<br /><b>#6</b><br /><p>Using list comprehension:</p>

<pre><code>def countWordDistances(l):
    unique_words = set(l)
    idx = [[i for i,x in enumerate(l) if x==item]
            for item in unique_words]
    return {item:1.*sum(idx[i])/len(idx[i]) + 1.
            for i,item in enumerate(unique_words)}

li = ['that','sank','into','the','ocean']
countWordDistances(li)
# {'into': 3.0, 'ocean': 5.0, 'sank': 2.0, 'that': 1.0, 'the': 4.0}

li2 = ['that','sank','into','the','ocean', 'that']
countWordDistances(li2)
# {'into': 3.0, 'ocean': 5.0, 'sank': 2.0, 'that': 3.5, 'the': 4.0}
</code></pre>
<br /><b>#7</b><br /><p>Oneliner -</p>

<pre><code>from __future__ import division   # no need for this if using py3k

def countWordDistances(li):
    '''
    If li = ['that','sank','into','the','ocean']    
    This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }
    However, if there is a duplicate term, take the average of their positions
    '''
    return {w:sum(dist)/len(dist) for w,dist in zip(set(li), ([i+1 for i,x in enumerate(li) if x==w] for w in set(li))) }
</code></pre>

<p>What I am making in the last line is a dictionary comprehension, similar to a list comprehension.</p>
<br />