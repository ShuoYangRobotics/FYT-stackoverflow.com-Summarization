<h3>Question (ID-8700666):</h3><h2>Identifying common elements in multiple files</h2><p>I have 8 files of one column and non uniform number of rows in each column. I need to identify the elements which are common in all of these 8 files.</p>

<p>I can do this task for comparing two files, but I am unable to write workable one liner in shell to do the same.</p>

<p>Any ideas.....</p>

<p>Thank you in advance.</p>

<p>File 1 <br>
Paul <br>
pawan <br></p>

<p>File 2 <br>
Raman <br>
Paul<br>
sweet <br>
barua<br></p>

<p>File 3<br>
Sweet<br>
barua<br>
Paul<br></p>

<p>The answer of the comparison of these three files should be Paul.</p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>The following one-liner should do (change 3 to 8 to match your case)</p>

<pre><code>$ sort * | uniq -c | grep 3
      3 Paul
</code></pre>

<p>Probably better to do this in python though, using <code>sets</code>...</p>
<br /><b>#1</b><br /><pre><code>python -c 'import sys;print "".join(sorted(set.intersection(*[set(open(a).readlines()) for a in sys.argv[1:]])))' File1 File2 File3
</code></pre>

<p>prints <code>Paul</code> for your files <code>File1</code>, <code>File2</code> and <code>File3</code>.</p>
<br /><b>#2</b><br /><h2>Perl</h2>

<pre><code>$ perl -lnE '$c{$_}{$ARGV}++ }{ print for grep { keys %{$c{$_}} == 8 } keys %c;' file[1-8]
</code></pre>

<p>It should be possible to get rid of the hard-coded <code>8</code> as well with <code>@{[ glob "@ARGV" ]}</code> but I don't have time to test it now.</p>

<p>This solution will correctly handle the existence of duplicate lines across files as well.</p>
<br /><b>#3</b><br /><p>Here I've been trying to find a concise way to make sure each match comes from a different file. If there are no duplicates within the files, it's fairly simple in perl:</p>

<pre><code>perl -lnwE '$a{$_}++; END { for (keys %a) { print if $a{$_} == 3 } }' files*
</code></pre>

<p>The <code>-l</code> option will auto-chomp your input (remove newline), and add a newline to the print. This is important in case of missing eof newlines.</p>

<p>The <code>-n</code> option will read input from file name arguments (or stdin).</p>

<p>The hash assignment will count duplicates, and the END block will print out what duplicates appeared 3 times. Change 3 to however many files you have.</p>

<p>If you want a slightly more flexible version, you can count the arguments in a BEGIN block.</p>

<pre><code>perl -lnwE 'BEGIN { $n = scalar @ARGV } 
    $a{$_}++; END { for (keys %a) { print if $a{$_} == $n } }' files*
</code></pre>
<br /><b>#4</b><br /><pre><code>$ awk '++a[$0]==3' file{1..3}.txt
Paul
</code></pre>

<hr>

<h2>update</h2>

<pre><code>$ awk '(FILENAME SEP $0) in b{next}; b[FILENAME,$0]=1 &amp;&amp; ++a[$0]==3' file{1..3}.txt
Paul
</code></pre>
<br /><b>#5</b><br /><p>This might work for you:</p>

<pre><code>ls file{1..3} | 
xargs -n1 sort -u | 
sort | 
uniq -c | 
sed 's/^\s*'"$(ls file{1..3} | wc -l)"'\s*//p;d'
</code></pre>
<br />