<h3>Question (ID-556730):</h3><h2>Python list serialization - fastest method</h2><p>I need to load (de-serialize) a pre-computed list of integers from a file in a Python script (into a Python list). The list is large (upto millions of items), and I can choose the format I store it in, as long as loading is fastest.</p>

<p>Which is the fastest method, and why?</p>

<ol>
<li>Using <code>import</code> on a .py file that just contains the list assigned to a variable</li>
<li>Using <code>cPickle</code>'s <code>load</code></li>
<li>Some other method (perhaps <code>numpy</code>?)</li>
</ol>

<p>Also, how can one benchmark such things reliably?</p>

<p><strong>Addendum:</strong> measuring this reliably is difficult, because <code>import</code> is cached so it can't be executed multiple times in a test. The loading with pickle also gets faster after the first time probably because page-precaching by the OS. Loading 1 million numbers with <code>cPickle</code> takes 1.1 sec the first time run, and 0.2 sec on subsequent executions of the script.</p>

<p>Intuitively I feel <code>cPickle</code> should be faster, but I'd appreciate numbers (this is quite a challenge to measure, I think). </p>

<p>And yes, it's important for me that this performs quickly.</p>

<p>Thanks</p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>I would guess cPickle will be fastest if you really need the thing in a list.</p>

<p>If you can use an array, which is a built-in sequence type, I timed this at a quarter of a second for 1 million integers:</p>

<pre><code>from array import array
from datetime import datetime

def WriteInts(theArray,filename):
    f = file(filename,"wb")
    theArray.tofile(f)
    f.close()

def ReadInts(filename):
    d = datetime.utcnow()
    theArray = array('i')
    f = file(filename,"rb")
    try:
        theArray.fromfile(f,1000000000)
    except EOFError:
        pass
    print "Read %d ints in %s" % (len(theArray),datetime.utcnow() - d)
    return theArray

if __name__ == "__main__":
    a = array('i')
    a.extend(range(0,1000000))
    filename = "a_million_ints.dat"
    WriteInts(a,filename)
    r = ReadInts(filename)
    print "The 5th element is %d" % (r[4])
</code></pre>
<br /><b>#1</b><br /><p>For benchmarking, see the timeit module in the Python standard library. To see what is the fastest way, implement all the ways you can think of and measure them with timeit.</p>

<p>Random thought: depending on what you're doing exactly, you may find it fastest to store "sets of integers" in the style used in <strong>.newsrc</strong> files:</p>

<pre><code>1, 3-1024, 11000-1200000
</code></pre>

<p>If you need to check whether something is in that set, then loading and matching with such a representation should be among the fastest ways. This assumes your sets of integers are reasonably dense, with long consecutive sequences of adjacent values.</p>
<br /><b>#2</b><br /><p>"how can one benchmark such things reliably?"  </p>

<p>I don't get the question.</p>

<p>You write a bunch of little functions to create and save your list in various forms.</p>

<p>You write a bunch of little functions to load your lists in their various forms.</p>

<p>You write a little timer function to get start time, execute the load procedure a few dozen times (to get a solid average that's long enough that OS scheduling noise doesn't dominate your measurements).</p>

<p>You summarize your data in a little report.  </p>

<p>What's unreliable about this?</p>

<p>Here are some unrelated questions that shows how to measure and compare performance.  </p>

<p><a href="http://stackoverflow.com/questions/489999/python-convert-list-of-ints-to-one-number">http://stackoverflow.com/questions/489999/python-convert-list-of-ints-to-one-number</a></p>

<p><a href="http://stackoverflow.com/questions/376461/string-concatenation-vs-string-substitution-in-python">http://stackoverflow.com/questions/376461/string-concatenation-vs-string-substitution-in-python</a></p>
<br /><b>#3</b><br /><p>Do you need to always load the whole file? If not, <a href="http://docs.python.org/library/struct.html" rel="nofollow">upack_from()</a> might be the best solution. Suppose, that you have 1000000 integers, but you'd like to load just the ones from 50000 to 50099, you'd do:</p>

<pre><code>import struct
intSize = struct.calcsize('i') #this value would be constant for a given arch
intFile = open('/your/file.of.integers')
intTuple5K100 = struct.unpack_from('i'*100,intFile,50000*intSize)
</code></pre>
<br /><b>#4</b><br /><p>To help you with timing, the Python Library provides the <a href="http://docs.python.org/library/timeit.html#module-timeit" rel="nofollow"><code>timeit</code></a> module:</p>

<blockquote>
  <p>This module provides a simple way to time small bits of Python code. It has both command line as well as callable interfaces. It avoids a number of common traps for measuring execution times.</p>
</blockquote>

<p>An example (from the manual) that compares the cost of using <code>hasattr()</code> vs. <code>try/except</code> to test for missing and present object attributes:</p>

<pre><code>% timeit.py 'try:' '  str.__nonzero__' 'except AttributeError:' '  pass'
100000 loops, best of 3: 15.7 usec per loop
% timeit.py 'if hasattr(str, "__nonzero__"): pass'
100000 loops, best of 3: 4.26 usec per loop
% timeit.py 'try:' '  int.__nonzero__' 'except AttributeError:' '  pass'
1000000 loops, best of 3: 1.43 usec per loop
% timeit.py 'if hasattr(int, "__nonzero__"): pass'
100000 loops, best of 3: 2.23 usec per loop
</code></pre>
<br /><b>#5</b><br /><p>cPickle will be the fastest since it is saved in binary and no real python code has to be parsed.</p>

<p>Other advantates are that it is more secure (since it does not execute commands) and you have no problems with setting <code>$PYTHONPATH</code> correctly.</p>
<br />