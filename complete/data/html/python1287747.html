<h3>Question (ID-1287747):</h3><h2>Any efficient way to read datas from large binary file?</h2><p>I need to handle tens of Gigabytes data in one binary file. Each record in the data file is variable length.</p>

<p>So the file is like:</p>

<pre><code>&lt;len1&gt;&lt;data1&gt;&lt;len2&gt;&lt;data2&gt;..........&lt;lenN&gt;&lt;dataN&gt;
</code></pre>

<p>The data contains integer, pointer, double value and so on. </p>

<p>I found python can not even handle this situation. There is no problem if I read the whole file in memory. It's fast. But it seems the <code>struct</code> package is not good at performance. It almost stuck on unpack the bytes.</p>

<p>Any help is appreciated.</p>

<p>Thanks.</p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p><code>struct</code> and <code>array</code>, which other answers recommend, are fine for the details of the implementation, and might be all you need if your needs are always to sequentially read all of the file or a prefix of it. Other options include <a href="http://docs.python.org/library/functions.html?highlight=buffer#buffer" rel="nofollow">buffer</a>, <a href="http://docs.python.org/library/mmap.html" rel="nofollow">mmap</a>, even <a href="http://docs.python.org/library/ctypes.html" rel="nofollow">ctypes</a>, depending on many details you don't mention regarding your exact needs. Maybe a little specialized Cython-coded helper can offer all the extra performance you need, if no suitable and accessible library (in C, C++, Fortran, ...) already exists that can be interfaced for the purpose of handling this humongous file as you need to.</p>

<p>But clearly there are peculiar issues here -- how can a data file contain <strong>pointers</strong>, for example, which are intrinsically a concept related to addressing <em>memory</em>? Are they maybe "offsets" instead, and, if so, how exactly are they based and coded? Are your needs at all more advanced than simply sequential reading (e.g., random access), and if so, can you do a first "indexing" pass to get all the offsets from start of file to start of record into a more usable, compact, handily-formatted auxiliary file? (<em>That</em> binary file of offsets would be a natural for <code>array</code> -- unless the offsets need to be longer than <code>array</code> supports on your machine!). What is the distribution of record lengths and compositions and number of records to make up the "tens of gigabytes"?  Etc, etc.</p>

<p>You have a very large scale problem (and no doubt very large scale hardware to support it, since you mention that you can easily read all of the file into memory that means a 64bit box with many tens of GB of RAM -- wow!), so it's well worth the detailed care to optimize the handling thereof -- but we can't help much with such detailed care unless we know enough detail to do so!-).</p>
<br /><b>#1</b><br /><p>have a look at <code>array</code> module, specifically at <a href="http://docs.python.org/library/array.html#array.array.fromfile" rel="nofollow"><code>array.fromfile</code></a> method. This bit:</p>

<blockquote>
  <p>Each record in the data file is variable length.</p>
</blockquote>

<p>is rather unfortunate. but you could handle it with a try-except clause.</p>
<br /><b>#2</b><br /><p>For a similar task, I defined a class like this:</p>

<pre><code>class foo(Structure):
        _fields_ = [("myint", c_uint32)]
</code></pre>

<p>created an instance</p>

<pre><code>bar = foo()
</code></pre>

<p>and did,</p>

<pre><code>block = file.read(sizeof(bar))
memmove(addressof(bar), block, sizeof(bar))
</code></pre>

<p>In the event of variable-size records, you can use a similar method for retrieving <em>lenN</em>, and then read the corresponding data entries. Seems trivial to implement. However, I have no idea of how fast this method is compared to using <em>pack()</em> and <em>unpack()</em>, perhaps someone else has profiled both methods.</p>
<br /><b>#3</b><br /><p>For help with parsing the file without reading it into memory you can use the <a href="http://python-bitstring.googlecode.com" rel="nofollow">bitstring</a> module.</p>

<p>Internally this is using the struct module and a bytearray, but an immutable Bits object can be initialised with a filename so it won't read it all into memory.</p>

<p>For example:</p>

<pre><code>from bitstring import Bits

s = Bits(filename='your_file')
while s.bytepos != s.length:
    # Read a byte and interpret as an unsigned integer
    length = s.read('uint:8')
    # Read 'length' bytes and convert to a Python string
    data = s.read(length*8).bytes
    # Now do whatever you want with the data
</code></pre>

<p>Of course you can parse the data however you want.</p>

<p>You can also use slice notation to read the file contents, although note that the indices will be in bits rather than bytes so for example <code>s[-800:]</code> would be the final 100 bytes.</p>
<br /><b>#4</b><br /><p>What if you use dump the data file into sqlite3 in memory.</p>

<pre><code>import sqlite3
sqlite3.Connection(":memory:")
</code></pre>

<p>You can then use sql to process the data.</p>

<p>Besides, you might want to look at <a href="http://docs.python.org/tutorial/classes.html#generators" rel="nofollow">generators</a> (or <a href="http://diveintopython3.org/generators.html" rel="nofollow">here</a>) and <a href="http://docs.python.org/tutorial/classes.html#iterators" rel="nofollow">iterators</a> (or <a href="http://diveintopython3.org/advanced-iterators.html" rel="nofollow">here</a> and <a href="http://diveintopython3.org/iterators.html" rel="nofollow">here</a>).</p>
<br /><b>#5</b><br /><p>PyTables is a very good library to handle HDF5, a binary format used in astronomy and meteorology to handle very big datasets:</p>

<ul>
<li><a href="http://www.pytables.org/moin" rel="nofollow">PyTables</a></li>
</ul>

<p>It works more or less like an hierarchical database, where you can store multiple tables, inside columns. Have a look at it.</p>
<br />