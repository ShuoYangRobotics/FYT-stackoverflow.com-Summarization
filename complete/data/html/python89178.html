<h3>Question (ID-89178):</h3><h2>In Python, what is the fastest algorithm for removing duplicates from a list so that all elements are unique *while preserving order*?</h2><p>For example:</p>

<pre><code>&gt;&gt;&gt; x = [1, 1, 2, 'a', 'a', 3]
&gt;&gt;&gt; unique(x)
[1, 2, 'a', 3]
</code></pre>

<p>Assume list elements are hashable.</p>

<p><strong>Clarification:</strong> The result should keep the first duplicate in the list. For example, [1, 2, 3, 2, 3, 1] becomes [1, 2, 3].</p>
<br /><h3>Answers (Total-26):</h3><b>#0</b><br /><pre><code>def unique(items):
    found = set([])
    keep = []

    for item in items:
        if item not in found:
            found.add(item)
            keep.append(item)

    return keep

print unique([1, 1, 2, 'a', 'a', 3])
</code></pre>
<br /><b>#1</b><br /><p>Using:</p>

<pre><code>lst = [8, 8, 9, 9, 7, 15, 15, 2, 20, 13, 2, 24, 6, 11, 7, 12, 4, 10, 18, 13, 23, 11, 3, 11, 12, 10, 4, 5, 4, 22, 6, 3, 19, 14, 21, 11, 1, 5, 14, 8, 0, 1, 16, 5, 10, 13, 17, 1, 16, 17, 12, 6, 10, 0, 3, 9, 9, 3, 7, 7, 6, 6, 7, 5, 14, 18, 12, 19, 2, 8, 9, 0, 8, 4, 5]
</code></pre>

<p>And using the timeit module:</p>

<pre><code>$ python -m timeit -s 'import uniquetest' 'uniquetest.etchasketch(uniquetest.lst)'
</code></pre>

<p>(and so on for the various other functions -- which I named after their posters), I have the following results (on my first generation Intel MacBook Pro):</p>

<ul>
<li>Allen: 14.6 usec per loop [1]</li>
<li>Terhorst: 26.6 usec per loop</li>
<li>Tarle: 44.7 usec per loop</li>
<li>ctcherry: 44.8 usec per loop</li>
<li>Etchasketch 1 (the short one): 64.6 usec per loop</li>
<li>Schinckel: 65 usec per loop</li>
<li>Etchasketch 2: 71.6 usec per loop</li>
<li>Little: 89.4 usec per loop</li>
<li>Tyler: 179 usec per loop</li>
</ul>

<p>[1] Note that Allen modifies the list in place â€“ I believe this has skewed the time, in that the timeit module runs the code 100000 times and 99999 of them are with the dupe-less list.</p>

<p>Summary: Straight-forward implementation with sets wins over confusing one-liners :-)</p>
<br /><b>#2</b><br /><p>What's going to be fastest depends on what percentage of your list is duplicates. If it's nearly all duplicates, with few unique items, creating a new list will probably be faster. If it's mostly unique items, removing them from the original list (or a copy) will be faster.</p>

<p>Here's one for modifying the list in place:</p>

<pre><code>def unique(items):
  seen = set()
  for i in xrange(len(items)-1, -1, -1):
    it = items[i]
    if it in seen:
      del items[i]
    else:
      seen.add(it)
</code></pre>

<p>Iterating backwards over the indices ensures that removing items doesn't affect the iteration.</p>
<br /><b>#3</b><br /><p>This is the fastest in-place method I've found (assuming a large proportion of duplicates):</p>

<pre><code>def unique(l):
    s = set(); n = 0
    for x in l:
        if x not in s: s.add(x); l[n] = x; n += 1
    del l[n:]
</code></pre>

<p>This is 10% faster than Allen's implementation, on which it is based (timed with timeit.repeat, JIT compiled by psyco). It keeps the first instance of any duplicate.</p>

<p>repton-infinity: I'd be interested if you could confirm my timings.</p>
<br /><b>#4</b><br /><p>Here is the fastest solution so far (for the following input):</p>

<pre><code>def del_dups(seq):
    seen = {}
    pos = 0
    for item in seq:
        if item not in seen:
            seen[item] = True
            seq[pos] = item
            pos += 1
    del seq[pos:]

lst = [8, 8, 9, 9, 7, 15, 15, 2, 20, 13, 2, 24, 6, 11, 7, 12, 4, 10, 18, 
       13, 23, 11, 3, 11, 12, 10, 4, 5, 4, 22, 6, 3, 19, 14, 21, 11, 1, 
       5, 14, 8, 0, 1, 16, 5, 10, 13, 17, 1, 16, 17, 12, 6, 10, 0, 3, 9, 
       9, 3, 7, 7, 6, 6, 7, 5, 14, 18, 12, 19, 2, 8, 9, 0, 8, 4, 5]
del_dups(lst)
print(lst)
# -&gt; [8, 9, 7, 15, 2, 20, 13, 24, 6, 11, 12, 4, 10, 18, 23, 3, 5, 22, 19, 14, 
#     21, 1, 0, 16, 17]
</code></pre>

<p>Dictionary lookup is slightly faster then the set's one in Python 3.</p>
<br /><b>#5</b><br /><p>Obligatory generator-based variation:</p>

<pre><code>def unique(seq):
  seen = set()
  for x in seq:
    if x not in seen:
      seen.add(x)
      yield x
</code></pre>
<br /><b>#6</b><br /><p>Taken from <a href="http://www.peterbe.com/plog/uniqifiers-benchmark" rel="nofollow">http://www.peterbe.com/plog/uniqifiers-benchmark</a></p>

<pre><code>def f5(seq, idfun=None):  
    # order preserving 
    if idfun is None: 
        def idfun(x): return x 
    seen = {} 
    result = [] 
    for item in seq: 
        marker = idfun(item) 
        # in old Python versions: 
        # if seen.has_key(marker) 
        # but in new ones: 
        if marker in seen: continue 
        seen[marker] = 1 
        result.append(item) 
    return result
</code></pre>
<br /><b>#7</b><br /><p>You can actually do something really cool in Python to solve this.  You can create a list comprehension that would reference itself as it is being built.  As follows:</p>

<pre><code>   # remove duplicates...
   def unique(my_list):
       return [x for x in my_list if x not in locals()['_[1]'].__self__]
</code></pre>

<p>Edit: <strong>I removed the "self", and it works on Mac OS X, Python 2.5.1.</strong></p>

<p>The _[1] is Python's "secret" reference to the new list.  The above, of course, is a little messy, but you could adapt it fit your needs as necessary.  For example, you can actually write a function that returns a reference to the comprehension; it would look more like:</p>

<pre><code>return [x for x in my_list if x not in this_list()]
</code></pre>

<p><hr /></p>
<br /><b>#8</b><br /><p>One-liner:</p>

<pre><code>new_list = reduce(lambda x,y: x+[y][:1-int(y in x)], my_list, [])
</code></pre>
<br /><b>#9</b><br /><p>Do the duplicates necessarily need to be in the list in the first place?  There's no overhead as far as looking the elements up, but there is a little bit more overhead in adding elements (though the overhead should be O(1) ).</p>

<pre><code>&gt;&gt;&gt; x  = []
&gt;&gt;&gt; y = set()
&gt;&gt;&gt; def add_to_x(val):
...     if val not in y:
...             x.append(val)
...             y.add(val)
...     print x
...     print y
... 
&gt;&gt;&gt; add_to_x(1)
[1]
set([1])
&gt;&gt;&gt; add_to_x(1)
[1]
set([1])
&gt;&gt;&gt; add_to_x(1)
[1]
set([1])
&gt;&gt;&gt;
</code></pre>
<br /><b>#10</b><br /><p>Benchmark and a clear answer <a href="http://www.peterbe.com/plog/uniqifiers-benchmark" rel="nofollow">here</a>.</p>
<br /><b>#11</b><br /><p>has_key in python is O(1). Insertion and retrieval from a hash is also O(1). Loops through n items twice, so O(n).</p>

<pre><code>def unique(list):
  s = {}
  output = []
  for x in list:
    count = 1
    if(s.has_key(x)):
      count = s[x] + 1

    s[x] = count
  for x in list:
    count = s[x]
    if(count &gt; 0):
      s[x] = 0
      output.append(x)
  return output
</code></pre>
<br /><b>#12</b><br /><p>There are some great, efficient solutions here.  However, for anyone not concerned with the absolute most efficient <code>O(n)</code> solution, I'd go with the simple one-liner <code>O(n^2*log(n))</code> solution:</p>

<pre><code>def unique(xs):
    return sorted(set(xs), key=lambda x: xs.index(x))
</code></pre>

<p>or the more efficient two-liner <code>O(n*log(n))</code> solution:</p>

<pre><code>def unique(xs):
    positions = dict((e,pos) for pos,e in reversed(list(enumerate(xs))))
    return sorted(set(xs), key=lambda x: positions[x])
</code></pre>
<br /><b>#13</b><br /><p>An in-place one-liner for this:</p>

<pre><code>&gt;&gt;&gt; x = [1, 1, 2, 'a', 'a', 3]
&gt;&gt;&gt; [ item for pos,item in enumerate(x) if x.index(item)==pos ]
[1, 2, 'a', 3]
</code></pre>
<br /><b>#14</b><br /><p>Here are two recipes from the <em>itertools</em> documentation:</p>

<pre><code>def unique_everseen(iterable, key=None):
    "List unique elements, preserving order. Remember all elements ever seen."
    # unique_everseen('AAAABBBCCDAABBB') --&gt; A B C D
    # unique_everseen('ABBCcAD', str.lower) --&gt; A B C D
    seen = set()
    seen_add = seen.add
    if key is None:
        for element in ifilterfalse(seen.__contains__, iterable):
            seen_add(element)
            yield element
    else:
        for element in iterable:
            k = key(element)
            if k not in seen:
                seen_add(k)
                yield element

def unique_justseen(iterable, key=None):
    "List unique elements, preserving order. Remember only the element just seen."
    # unique_justseen('AAAABBBCCDAABBB') --&gt; A B C D A B
    # unique_justseen('ABBCcAD', str.lower) --&gt; A B C A D
    return imap(next, imap(itemgetter(1), groupby(iterable, key)))
</code></pre>
<br /><b>#15</b><br /><p>This may be the simplest way (not the fastest):</p>

<pre><code>list(OrderedDict.fromkeys(iterable))
</code></pre>
<br /><b>#16</b><br /><p>I have no experience with python, but an algorithm would be to sort the list, then remove duplicates (by comparing to previous items in the list), and finally find the position in the new list by comparing with the old list.</p>

<p>Longer answer: <a href="http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/52560" rel="nofollow">http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/52560</a></p>
<br /><b>#17</b><br /><p>I haven't done any tests, but one possible algorithm might be to create a second list, and iterate through the first list.  If an item is not in the second list, add it to the second list.</p>

<pre><code>x = [1, 1, 2, 'a', 'a', 3]
y = []
for each in x:
    if each not in y:
        y.append(each)
</code></pre>
<br /><b>#18</b><br /><pre><code>&gt;&gt;&gt; def unique(list):
...   y = []
...   for x in list:
...     if x not in y:
...       y.append(x)
...   return y
</code></pre>
<br /><b>#19</b><br /><p>O(n) if dict is hash, O(nlogn) if dict is tree, and simple, fixed.  Thanks to Matthew for the suggestion.  Sorry I don't know the underlying types.</p>

<pre><code>def unique(x):    
  output = []
  y = {}
  for item in x:
    y[item] = ""

  for item in x:
    if item in y:
      output.append(item)

  return output
</code></pre>
<br /><b>#20</b><br /><p>If you take out the empty list from the call to set() in Terhost's answer, you get a little speed boost.</p>

<p>Change:
    found = set([])<br />
to:
    found = set()</p>

<p>However, you don't need the set at all.</p>

<pre><code>def unique(items):
    keep = []

    for item in items:
        if item not in keep:
            keep.append(item)

    return keep
</code></pre>

<p>Using timeit I got these results:</p>

<p>with set([]) -- 4.97210427363<br />
with set() -- 4.65712377445<br />
with no set -- 3.44865284975</p>
<br /><b>#21</b><br /><p>Remove duplicates and preserve order:</p>

<p>This is a fast 2-liner that leverages built-in functionality of list comprehensions and dicts.</p>

<pre><code>x = [1, 1, 2, 'a', 'a', 3]

tmpUniq = {} # temp variable used below 
results = [tmpUniq.setdefault(i,i) for i in x if i not in tmpUniq]

print results
[1, 2, 'a', 3]
</code></pre>

<p>The dict.setdefaults() function returns the value as well as adding it to the temp dict directly in the list comprehension.  Using the built-in functions and the hashes of the dict will work to maximize efficiency for the process.</p>
<br /><b>#22</b><br /><pre><code>&gt;&gt;&gt; x=[1,1,2,'a','a',3]
&gt;&gt;&gt; y = [ _x for _x in x if not _x in locals()['_[1]'] ]
&gt;&gt;&gt; y
[1, 2, 'a', 3]
</code></pre>

<p><br>
"locals()['_[1]']" is the "secret name" of the list being created.</p>
<br /><b>#23</b><br /><p>I don't know if this one is fast or not, but at least it is simple.</p>

<p>Simply, convert it first to a set and then again to a list</p>

<pre><code>def unique(container):
  return list(set(container))
</code></pre>
<br /><b>#24</b><br /><p>One pass.</p>

<pre><code>a = [1,1,'a','b','c','c']

new_list = []
prev = None

while 1:
    try:
        i = a.pop(0)
        if i != prev:
            new_list.append(i)
        prev = i
    except IndexError:
        break
</code></pre>
<br /><b>#25</b><br /><p>a=[1,2,3,4,5,7,7,8,8,9,9,3,45]</p>

<p>def unique(l):</p>

<pre><code>ids={}
for item in l:
	if not ids.has_key(item):
		ids[item]=item
return  ids.keys()
</code></pre>

<p>print a</p>

<p>print unique(a)</p>

<h1>----------------------------</h1>

<p>Inserting elements will take theta(n)
retrieving if element is exiting or not will take constant time
testing all the items will take also theta(n)
so we can see that this solution will take theta(n)
Bear in Mind that dictionary in python implemented by hash table</p>
<br />