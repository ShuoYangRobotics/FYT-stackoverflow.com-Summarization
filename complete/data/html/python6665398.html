<h3>Question (ID-6665398):</h3><h2>ALGORITHM - String similarity score/hash</h2><p>Is there a method to calculate something like general "similarity score" of a string? In a way that I am not comparing two strings together but rather I get some number/scores (hash) for each string that can later tell me that two strings are or are not similar. Two similar strings should have similar (close) scores/hashes. </p>

<p>Let's consider these strings and scores as an example:</p>

<p>Hello world                1000</p>

<p>Hello world!               1010</p>

<p>Hello earth                1125</p>

<p>Foo bar                    3250</p>

<p>FooBarbar                  3750</p>

<p>Foo Bar!                   3300</p>

<p>Foo world!                 2350</p>

<p>You can see that Hello world! and Hello world are similar and their scores are close to each other.</p>

<p>This way, finding the most similar strings to a given string would be done by subtracting given strings score from other scores and then sorting their absolute value.</p>

<p>My end aim is : there would be streaming log messages(only pure messages) and i wanna find the pattern of those messages(some sort of regular expression type).But that gets started only when i can bucket similar strings. I again focus that <strong>I should get some number/scores (hash) for each string AND THAT CAN LATER tell me that two strings are or are not similar</strong></p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>There are several such "scores", but they all depend on how you define similarity.</p>

<ul>
<li>I think the python library already has a <a href="http://en.wikipedia.org/wiki/Soundex" rel="nofollow"><code>soundex</code></a> implementation.</li>
<li>you can also compute the <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance of two strings</a></li>
<li>NYSIIS?</li>
</ul>
<br /><b>#1</b><br /><p>Have a look at <a href="http://en.wikipedia.org/wiki/Locality_sensitive_hashing" rel="nofollow">locality-sensitive hashing</a>.</p>

<blockquote>
  <p>The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items).</p>
</blockquote>

<p>There's a very good explanation available <a href="http://d3s.mff.cuni.cz/~holub/sw/shash/" rel="nofollow">here</a> together with some sample code.</p>
<br /><b>#2</b><br /><p>TL;DR: <a href="http://code.activestate.com/recipes/572156-bk-tree/" rel="nofollow">Python BK-tree</a></p>

<p>Interesting question. I have limited experience within this field, but since the Levenshtein distance fulfills the triangle inequality, I figured that there must be a way of computing some sort of absolute distance to an origin in order to find strings in the vicinity of each other without performing direct comparisons against all entries in the entire database.</p>

<p>While googling on some terms related to this, I found one particularly interesting thesis: <a href="http://uwspace.uwaterloo.ca/bitstream/10012/3788/1/thesis.pdf" rel="nofollow">Aspects of Metric Spaces in Computation</a> by Matthew Adam Skala.</p>

<p>At page 26 he discusses similarity measures based on kd-trees and others, but concludes:</p>

<blockquote>
  <p>However, general metric spaces do not provide the geometry required by
  those techniques. For a general metric space with no other
  assumptions, it is necessary distance-based to use a distance-based
  approach that indexes points solely on the basis of their distance
  from each other. Burkhard and Keller [35] offered one of the first
  such index structures, now known as a BK-tree for their initials, in
  1973. In a BK-tree, the metric is assumed to have a few discrete return values, each internal node contains a vantage point, and the
  subtrees correspond to the different values of the metric.</p>
</blockquote>

<p>A blog entry about how BK-trees work can be found <a href="http://blog.notdot.net/2007/4/Damn-Cool-Algorithms-Part-1-BK-Trees" rel="nofollow">here</a>.</p>

<p>In the thesis, Skala  goes on describing other solutions to this problem, including <a href="http://en.wikipedia.org/wiki/Vp-tree" rel="nofollow">VP-trees</a> and GH-trees. Chapter 6 analyses distances based on the Levenshtein edit distance. He also presents some other interesting distance metrics for strings.</p>

<p>I also found <a href="http://rads.stackoverflow.com/amzn/click/0123694469" rel="nofollow">Foundations of Multidimensional and Metric Data Structures</a>, which seems relevant to your question.</p>
<br /><b>#3</b><br /><p>You may be interested in <a href="http://en.wikipedia.org/wiki/Hamming_distance" rel="nofollow">Hamming Distance</a>. The Python function hamming_distance() computes the Hamming distance between two strings.</p>

<pre><code>def hamming_distance(s1, s2):
    assert len(s1) == len(s2)
    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))
</code></pre>
<br /><b>#4</b><br /><p>You can always use Levenshtein distance, also, there is a written implementation for that:
<a href="http://code.google.com/p/pylevenshtein/" rel="nofollow">http://code.google.com/p/pylevenshtein/</a></p>

<p>But, for simplicity, you can use builtin difflib module:</p>

<pre><code>&gt;&gt;&gt; import difflib
&gt;&gt;&gt; l
{'Hello Earth', 'Hello World!', 'Foo Bar!', 'Foo world!', 'Foo bar', 'Hello World', 'FooBarbar'}
&gt;&gt;&gt; difflib.get_close_matches("Foo World", l)
['Foo world!', 'Hello World', 'Hello World!']
</code></pre>

<p><a href="http://docs.python.org/library/difflib.html#difflib.get_close_matches" rel="nofollow">http://docs.python.org/library/difflib.html#difflib.get_close_matches</a></p>
<br /><b>#5</b><br /><p>For a fast way of determining string similarity, you might want to use <a href="http://thedigitalstandard.blogspot.com/2009/11/why-fuzzy-hashing-is-really-cool.html" rel="nofollow">fuzzy hashing</a>.</p>
<br />