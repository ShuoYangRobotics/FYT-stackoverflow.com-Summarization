<h3>Question (ID-465795):</h3><h2>What is a simple way to generate keywords from a text?</h2><p>I suppose I could take a text and remove high frequency English words from it. By keywords, I mean that I want to extract words that are most the characterizing of the content of the text (tags ) . It doesn't have to be perfect, a good approximation is perfect for my needs.</p>

<p>Has anyone done anything like that?  Do you known a Perl or Python library that does that? </p>

<p>Lingua::EN::Tagger is exactly what I asked however I needed a library that could work for french text too.</p>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><p>The name for the "high frequency English words" is <a href="http://en.wikipedia.org/wiki/Stop_words" rel="nofollow">stop words</a> and there are many lists available.  I'm not aware of any python or perl libraries, but you could encode your stop word list in a binary tree or hash (or you could use python's frozenset), then as you read each word from the input text, check if it is in your 'stop list' and filter it out.</p>

<p>Note that after you remove the stop words, you'll need to do some <a href="http://en.wikipedia.org/wiki/Stemming" rel="nofollow">stemming</a> to normalize the resulting text (remove plurals, -ings, -eds), then remove all the duplicate "keywords".</p>
<br /><b>#1</b><br /><p>You could try using the perl module <a href="http://search.cpan.org/~acoburn/Lingua-EN-Tagger-0.15/Tagger.pm" rel="nofollow">Lingua::EN::Tagger</a> for a quick and easy solution. </p>

<p>A more complicated module <a href="http://code.google.com/p/lingua-en-semtags-engine/" rel="nofollow">Lingua::EN::Semtags::Engine</a> uses Lingua::EN::Tagger with a WordNet database to get a more structured output. Both are pretty easy to use, just check out the documentation on CPAN or use perldoc after you install the module.</p>
<br /><b>#2</b><br /><p>In Perl there's <a href="http://search.cpan.org/perldoc?Lingua::EN::Keywords" rel="nofollow">Lingua::EN::Keywords</a>.</p>
<br /><b>#3</b><br /><p>To find the most frequently-used words in a text, do something like this:</p>

<pre><code>#!/usr/bin/perl -w

use strict;
use warnings 'all';

# Read the text:
open my $ifh, '&lt;', 'text.txt'
  or die "Cannot open file: $!";
local $/;
my $text = &lt;$ifh&gt;;

# Find all the words, and count how many times they appear:
my %words = ( );
map { $words{$_}++ }
  grep { length &gt; 1 &amp;&amp; $_ =~ m/^[\@a-z-']+$/i }
    map { s/[",\.]//g; $_ }
      split /\s/, $text;

print "Words, sorted by frequency:\n";
my (@data_line);
format FMT = 
@&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;...     @########
@data_line
.
local $~ = 'FMT';

# Sort them by frequency:
map { @data_line = ($_, $words{$_}); write(); }
  sort { $words{$b} &lt;=&gt; $words{$a} }
    grep { $words{$_} &gt; 2 }
      keys(%words);
</code></pre>

<p>Example output looks like this:</p>

<pre><code>john@ubuntu-pc1:~/Desktop$ perl frequency.pl 
Words, sorted by frequency:
for                                   32
Jan                                   27
am                                    26
of                                    21
your                                  21
to                                    18
in                                    17
the                                   17
Get                                   13
you                                   13
OTRS                                  11
today                                 11
PSM                                   10
Card                                  10
me                                     9
on                                     9
and                                    9
Offline                                9
with                                   9
Invited                                9
Black                                  8
get                                    8
Web                                    7
Starred                                7
All                                    7
View                                   7
Obama                                  7
</code></pre>
<br /><b>#4</b><br /><p>The simplest way to do what you want is this...</p>

<pre>
>>> text = "this is some of the sample text"
>>> words = [word for word in set(text.split(" ")) if len(word) > 3]
>>> words
['this', 'some', 'sample', 'text']
</pre>

<p>I don't know of any standard module that does this, but it wouldn't be hard to replace the limit on three letter words with a lookup into a set of common English words.</p>
<br /><b>#5</b><br /><p>One liner solution (words longer than two chars which occurred more than two times):</p>

<pre><code>perl -ne'$h{$1}++while m/\b(\w{3,})\b/g}{printf"%-20s %5d\n",$_,$h{$_}for sort{$h{$b}&lt;=&gt;$h{$a}}grep{$h{$_}&gt;2}keys%h'
</code></pre>

<p><strong>EDIT:</strong> If one wants to sort alphabetically words with same frequency can use this enhanced one:</p>

<pre><code>perl -ne'$h{$1}++while m/\b(\w{3,})\b/g}{printf"%-20s %5d\n",$_,$h{$_}for sort{$h{$b}&lt;=&gt;$h{$a}or$a cmp$b}grep{$h{$_}&gt;2}keys%h'
</code></pre>
<br /><b>#6</b><br /><p>I think the most accurate way that still maintains a semblance of simplicity would be to count the word frequencies in your source, then weight them according to their frequencies in common English (or whatever other language) usage.  </p>

<p>Words that appear less frequently in common use, like "coffeehouse" are more likely to be a keyword than words that appear more often, like "dog."  Still, if your source mentions "dog" 500 times and "coffeehouse" twice it's more likely that "dog" is a keyword even though it's a common word.</p>

<p>Deciding on the weighting scheme would be the difficult part.</p>
<br />