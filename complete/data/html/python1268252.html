<h3>Question (ID-1268252):</h3><h2>Python: Possible to share in-memory data between 2 separate processes</h2><p>I have an xmlrpc server using Twisted.  The server has a huge amount of data stored in-memory.  Is it possible to have a secondary, separate xmlrpc server running which can access the object in-memory in the first server?</p>

<p>So, serverA starts up and creates an object.  serverB starts up and can read from the object in serverA.</p>

<p><strong>* EDIT *</strong></p>

<p>The data to be shared is a list of 1 million tuples.</p>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><p>Without some deep and dark rewriting of the Python core runtime (to allow forcing of an allocator that uses a given segment of shared memory and ensures compatible addresses between disparate processes) there is no way to "share objects in memory" in any general sense. That list will hold a million addresses of tuples, each tuple made up of addresses of all of its items, and each of these addresses will have be assigned by pymalloc in a way that inevitably varies among processes and spreads all over the heap.</p>

<p>On just about every system except Windows, it's possible to spawn a subprocess that has essentially read-only access to objects in the parent process's space... as long as the parent process doesn't alter those objects, either. That's obtained with a call to <code>os.fork()</code>, that in practice "snapshots" all of the memory space of the current process and starts another simultaneous process on the copy/snapshot. On all modern operating systems, this is actually very fast thanks to a "copy on write" approach: the pages of virtual memory that are not altered by either process after the fork are not really copied (access to the same pages is instead shared); as soon as either process modifies any bit in a previously shared page, poof, that page is copied, and the page table modified, so the modifying process now has its own copy while the other process still sees the original one.</p>

<p>This extremely limited form of sharing can still be a lifesaver in some cases (although it's extremely limited: remember for example that adding a reference to a shared object counts as "altering" that object, due to reference counts, and so will force a page copy!)... except on Windows, of course, where it's not available. With this single exception (which I don't think will cover your use case), sharing of object graphs that include references/pointers to other objects is basically unfeasible -- and just about any objects set of interest in modern languages (including Python) falls under this classification.</p>

<p>In extreme (but sufficiently simple) cases one can obtain sharing by renouncing the native memory representation of such object graphs. For example, a list of a million tuples each with sixteen floats could actually be represented as a single block of 128 MB of shared memory -- all the 16M floats in double-precision IEEE representation laid end to end -- with a little shim on top to "make it look like" you're addressing things in the normal way (and, of course, the not-so-little-after-all shim would also have to take care of the extremely hairy inter-process synchronization problems that are certain to arise;-). It only gets hairier and more complicated from there.</p>

<p>Modern approaches to concurrency are more and more disdaining shared-anything approaches in favor of shared-nothing ones, where tasks communicate by message passing (even in multi-core systems using threading and shared address spaces, the synchronization issues and the performance hits the HW incurs in terms of caching, pipeline stalls, etc, when large areas of memory are actively modified by multiple cores at once, are pushing people away).</p>

<p>For example, the multiprocessing module in Python's standard library relies mostly on pickling and sending objects back and forth, not on sharing memory (surely not in a R/W way!-).</p>

<p>I realize this is not welcome news to the OP, but if he does need to put multiple processors to work, he'd better think in terms of having anything they must share reside in places where they can be accessed and modified by message passing -- a database, a memcache cluster, a dedicated process that does nothing but keep those data in memory and send and receive them on request, and other such message-passing-centric architectures.</p>
<br /><b>#1</b><br /><pre><code>mmap.mmap(0, 65536, 'GlobalSharedMemory')
</code></pre>

<p>I think the tag ("GlobalSharedMemory") must be the same for all processes wishing to share the same memory.</p>

<p><a href="http://docs.python.org/library/mmap.html" rel="nofollow">http://docs.python.org/library/mmap.html</a></p>
<br /><b>#2</b><br /><p>You can use the Python Multiprocessing module.</p>

<p><a href="http://docs.python.org/library/multiprocessing.html#sharing-state-between-processes" rel="nofollow">http://docs.python.org/library/multiprocessing.html#sharing-state-between-processes</a></p>
<br /><b>#3</b><br /><p>Does all that data need to be in memory at once? Can it perhaps be stored in a database, and the seperate processes access the ones they need individually or in groups?</p>
<br /><b>#4</b><br /><p>Why not just use a database for the shared data?  You have a multitude of lightweight options where you don't need to worry about the concurrency issues: sqlite, any of the nosql/key-value breed of databases, etc.  </p>
<br /><b>#5</b><br /><p>Why not stick the shared data into memcache server? then both servers can access it quite easily.</p>
<br /><b>#6</b><br /><p>You could write a C library to create and manipulate shared-memory arrays for your specific purpose, and then use ctypes to access them from Python.</p>

<p>Or, put them on the filesystem in /dev/shm (which is tmpfs).  You'd save a lot of development effort for very little performance overhead: reads/writes from a tmpfs filesystem are little more than a memcpy.</p>
<br />