<h3>Question (ID-3109773):</h3><h2>Extracting semantic/stylistic features from text</h2><p>I would like to know of open source tools (for java/python) which could help me extract semantic &amp; stylistic features from text. Examples of semantic features would be adjective-noun ratio, a particular sequence of part-of-speech tags (adjective followed by a noun: adj|nn) etc. Examples of stylistic features would be number of unique words, number of pronouns etc. Currently, I know only of <a href="http://sourceforge.net/projects/wvtool/files/" rel="nofollow">Word to Web Tools </a> which converts a  block of text into the rudimentary vector space model. </p>

<p>I am aware of few text-mining packages like <a href="http://gate.ac.uk/" rel="nofollow">GATE</a>, <a href="http://www.nltk.org/" rel="nofollow">NLTK</a> , <a href="http://rapid-i.com/content/view/181/190/" rel="nofollow">Rapid Miner</a>, <a href="http://mallet.cs.umass.edu/" rel="nofollow"> Mallet </a> and <a href="http://sourceforge.net/apps/trac/minorthird/wiki" rel="nofollow"> MinorThird </a>. However, I couldn't find any mechanism to suit my task. </p>

<p>Regards, <br>--Denzil</p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>I think that the Stanford Parser is one of the best and comprehensive NLP tools available for free: not only will it allow you to parse the <strong>structural dependencies</strong> (to count nouns/adjectives) but it will also give you the <strong>grammatical dependencies</strong> in the sentence (so you can extract the subject, object, etc).  The latter component is something that Python libraries simply cannot do yet (see <a href="http://stackoverflow.com/questions/3125926/does-nltk-have-a-tool-for-dependency-parsing">http://stackoverflow.com/questions/3125926/does-nltk-have-a-tool-for-dependency-parsing</a>) and is probably going to be the most important feature in regards to your software's ability to work with semantics.</p>

<p>If you're interested in Java and Python tools, then Jython is probably the most fun to use for you.  I was in the exact same boat, so I wrote this post about using Jython to run the example code provided in the Stanford Parser - I would give it a glance and see what you think: <a href="http://blog.gnucom.cc/2010/using-the-stanford-parser-with-jython/" rel="nofollow">http://blog.gnucom.cc/2010/using-the-stanford-parser-with-jython/</a></p>

<p><strong>Edit:</strong> After reading one of your comments I learned you need to parse 29 Million sentences.  I think you could benefit greatly by using pure Java to combine two really powerful technologies: Stanford Parser + Hadoop.  Both are written purely in Java and have an extremely rich API that you can use to parse vasts amount of data in a fraction of the time on a cluster of machines.  If you don't have the machines, you can use Amazon's EC2 cluster.  If you need an example of using Stanford Parser + Hadoop leave a comment for me, and I'll update the post with a URL to my example.</p>
<br /><b>#1</b><br /><p>If your text is mostly natural language (in English), you try to extract phrases using a part-of-speech (POS) tagger.  Monty tagger is a pure python POS tagger.
I've got very satisfactory performance out of a C++ POS tagger, such as the CRFTagger <a href="http://sourceforge.net/projects/crftagger/" rel="nofollow">http://sourceforge.net/projects/crftagger/</a>.  I tied it to Python using <code>subprocess.Popen</code>.  The POS tags allow you to keep only the important pieces of a sentence: nouns and verbs, for example, which can then be indexed using any indexing tools such as Lucene or Xapian (my favourite).</p>
<br /><b>#2</b><br /><p>I use <a href="http://lucene.apache.org/java/docs/index.html" rel="nofollow">Lucene</a>'s analyzers and indexing mechanism to build vector spaces for documents and then navigate in this space. You can construct term frequency vectors for documents, use an existing document to search other <a href="http://lucene.apache.org/java/3_0_2/api/all/org/apache/lucene/search/similar/MoreLikeThis.html" rel="nofollow">similar</a> documents in the vector space. If your data is big (millions of documents, tens of thousand of features) then you could like Lucene. You can also do stemming, pos tagging and other stuff. This <a href="http://sujitpal.blogspot.com/2009/12/lucene-pos-tagging-tokenfilter.html" rel="nofollow">blog post</a> might be a good starting point for POS tagging. In short, Lucene provides you all the necessary mechanism to implement the tasks you mentioned.</p>

<p>One library that I hear frequently is <a href="http://code.google.com/p/semanticvectors/" rel="nofollow">Semantic Vectors</a>. It's again built on Lucene but I don't have a direct experience with that one. Other than this, I suggest to look at Wikipedia's Vector Space Model <a href="http://en.wikipedia.org/wiki/Vector_space_model#Software_that_implements_the_vector_space_model" rel="nofollow">article</a>. </p>
<br /><b>#3</b><br /><p>I used NLTK for some NLP (Natural Language Processing) tasks and it worked really well (albeit kind of slowly).  Why exactly do you want such a structured representation of your text? (true question, as depending on the application sometimes much simpler representations can be better)</p>
<br /><b>#4</b><br /><p>Here's a compilation of Java NLP tools that's reasonably up-to-date:
<a href="http://www.searchenginecaffe.com/2007/03/java-open-source-text-mining-and.html" rel="nofollow">http://www.searchenginecaffe.com/2007/03/java-open-source-text-mining-and.html</a></p>

<p>LingPipe (http://alias-i.com/lingpipe/) hasn't been mentioned in the answers yet, and is an excellent &amp; actively developed toolkit.</p>
<br /><b>#5</b><br /><p>One of the brilliant libraries I got hold off: <a href="http://code.google.com/p/textmatrix/" rel="nofollow">http://code.google.com/p/textmatrix/</a></p>
<br />