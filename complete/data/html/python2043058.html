<h3>Question (ID-2043058):</h3><h2>Scripting HTTP more effeciently</h2><p>Often times I want to automate http queries. I currently use Java(and commons http client), but would probably prefer a scripting based approach. Something really quick and simple. Where I can set a header, go to a page and not worry about setting up the entire OO lifecycle, setting each header, calling up an html parser... I am looking for a solution in ANY language, preferable scripting</p>
<br /><h3>Answers (Total-12):</h3><b>#0</b><br /><p>Mechanize for Python seems easy to use: <a href="http://wwwsearch.sourceforge.net/mechanize/" rel="nofollow">http://wwwsearch.sourceforge.net/mechanize/</a></p>
<br /><b>#1</b><br /><p>Have a look at <a href="http://seleniumhq.org/" rel="nofollow">Selenium</a>. It generates code for C#, Java, Perl, PHP, Python, and Ruby if you need to customize the script.</p>
<br /><b>#2</b><br /><p>Watir sounds close to what you want although it (like Selenium linked to in another answer) actually opens up a browser to do stuff. You can see some examples <a href="http://watir.com/examples/" rel="nofollow">here</a>. Another browser based record + playback approach system is <a href="http://sahi.co.in" rel="nofollow">sahi</a>.</p>

<p>If your application is uses <a href="http://www.wsgi.org/wsgi/" rel="nofollow">WSGI</a>, then <a href="http://pythonpaste.org/testing-applications.html" rel="nofollow">paste</a> is a nice option. </p>

<p>Mechanize linked to in another answer is a "browser in a library" and there are clones in <a href="http://search.cpan.org/dist/WWW-Mechanize/" rel="nofollow">perl</a>, <a href="http://mechanize.rubyforge.org/mechanize/" rel="nofollow">Ruby</a> and <a href="http://wwwsearch.sourceforge.net/mechanize/" rel="nofollow">Python</a>. The Perl one is the original one and this seems to be the way to go if you <em>don't</em> want a browser. The problem with this approach is that all the front end code (which might rely on JavaScript), won't be exercised. </p>
<br /><b>#3</b><br /><p>My turn : wget or perl with <a href="http://search.cpan.org/~gaas/libwww-perl-5.834/lib/LWP.pm" rel="nofollow">lwp</a>. You'll find example on the linked page.</p>
<br /><b>#4</b><br /><p>If you have simple needs (fetch a page and then parse it), it is hard to beat <a href="http://search.cpan.org/perldoc?LWP%3A%3ASimple" rel="nofollow">LWP::Simple</a> and <a href="http://search.cpan.org/perldoc?HTML%3A%3ATreeBuilder" rel="nofollow">HTML::TreeBuilder</a>.</p>

<pre><code>use strict;
use warnings;

use LWP::Simple;
use HTML::TreeBuilder;

my $url = 'http://www.example.com';
my $content = get( $url) or die "Couldn't get $url";

my $t = HTML::TreeBuilder-&gt;new_from_content( $content );
$t-&gt;eof;
$t-&gt;elementify;

# Get first match:
my $thing = $t-&gt;look_down( _tag =&gt; 'p', id =&gt; qr/match_this_regex/ );

print $thing ? $thing-&gt;as_text : "No match found\n";

# Get all matches:
my @things = $t-&gt;look_down( _tag =&gt; 'p', id =&gt; qr/match_this_regex/ );

print $_ ? $_-&gt;as_text : "No match found" for @things;
</code></pre>
<br /><b>#5</b><br /><p>I'm testing ReST APIs at the moment and found the <a href="http://code.google.com/p/rest-client/" rel="nofollow">ReST Client</a> very nice. It's a GUI program, but nonetheless you can save and restore queries as XML files (or let them be generated), embed, write test scripts, and so on. And it's Java based (which is not an ad-hoc advantage, but you mentioned it).</p>

<p>Minus points for recording sessions. The ReST Client is good for stateless "one-shots".</p>

<p>If it doesn't suit your needs, I'd go for the already mentioned Mechanize (or <a href="http://code.google.com/p/www-mechanize/" rel="nofollow">WWW-Mechanize</a>, as it is called at CPAN).</p>
<br /><b>#6</b><br /><p>Depending on exactly what you're doing the easiest solution looks to be bash + curl.</p>

<p>The man page for the latter is available here:</p>

<p><a href="http://curl.haxx.se/docs/manpage.html" rel="nofollow">http://curl.haxx.se/docs/manpage.html</a></p>

<p>You can do posts as well as gets, HTTPS, show headers, work with cookies, basic and digest HTTP authentication, tunnel through all sorts of proxies, including NTLM on *nix amongst other things.</p>

<p>curl is also available as shared library with C and PHP support.</p>

<p>HTH</p>

<p>C.</p>
<br /><b>#7</b><br /><p>Python <a href="http://docs.python.org/library/urllib.html" rel="nofollow">urllib</a> may be what you're looking for.</p>

<p>Alternatively powershell exposes the full .NET http library in a scripting environment.</p>
<br /><b>#8</b><br /><p><a href="http://twill.idyll.org/" rel="nofollow">Twill</a> is pretty good and made for testing.  It can be used as script, in an interactive session or within a Python program.</p>
<br /><b>#9</b><br /><p>Perl and <a href="http://search.cpan.org/dist/WWW-Mechanize" rel="nofollow">WWW::Mechanize</a> can make web scraping etc simple and easy, including easy handling of forms (let's say you want to go to a login page, fill in a username and password and submit the form, handling cookies / hidden session identifiers just as a browser would...)</p>

<p>Similarly, finding or extracting links from the fetched page is trivial.</p>

<p>If you need to parse stuff out of the resulting pages that WWW::Mechanize can't easily help with, then feed the result to <a href="http://search.cpan.org/dist/HTML-Tree/lib/HTML/TreeBuilder.pm" rel="nofollow">HTML::TreeBuilder</a> to make parsing easy.</p>
<br /><b>#10</b><br /><p>What about using PHP+Curl, or just bash?</p>
<br /><b>#11</b><br /><p>Some <strong>ruby</strong> libraries:</p>

<ul>
<li><a href="http://railstips.org/blog/archives/2008/07/29/its-an-httparty-and-everyone-is-invited/" rel="nofollow">httparty:</a> really interesting, the philosophy is interesting.</li>
<li><a href="http://mechanize.rubyforge.org/mechanize/" rel="nofollow">mechanize:</a> classic good-quality web automatization library.</li>
<li><a href="http://scrubyt.org/" rel="nofollow">scrubYt:</a> puzzling at first glance but fun to use.</li>
</ul>
<br />