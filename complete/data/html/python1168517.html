<h3>Question (ID-1168517):</h3><h2>Using map() to get number of times list elements exist in a string in Python</h2><p>I'm trying to get the number of times each item in a list is in a string in Python:</p>

<pre><code>paragraph = "I eat bananas and a banana"

def tester(x): return len(re.findall(x,paragraph))

map(tester, ['banana', 'loganberry', 'passion fruit'])
</code></pre>

<p>Returns [2, 0, 0] </p>

<p>What I'd like to do however is extend this so I can feed the paragraph value into the map() function.  Right now, the tester() function has paragraph hardcoded.  Does anybody have a way to do this (perhaps make an n-length list of paragraph values)?  Any other ideas here?</p>

<p>Keep in mind that each of the array values will have a weight at some point in the future - hence the need to keep the values in a list rather than crunching them all together.</p>

<p>UPDATE:  The paragraph will often be 20K and the list will often have 200+ members. My thinking is that map operates in parallel - so it will be much more efficient than any serial methods.</p>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><p>A closure would be a quick solution:</p>

<pre><code>paragraph = "I eat bananas and a banana"

def tester(s): 
    def f(x):
        return len(re.findall(x,s))
    return f

print map(tester(paragraph), ['banana', 'loganberry', 'passion fruit'])
</code></pre>
<br /><b>#1</b><br /><pre><code>targets = ['banana', 'loganberry', 'passion fruit']
paragraph = "I eat bananas and a banana"

print [paragraph.count(target) for target in targets]
</code></pre>

<p>No idea why you would use map() here.</p>
<br /><b>#2</b><br /><p>I know you didn't ask for list comprehension, but here it is anyway:</p>

<pre><code>paragraph = "I eat bananas and a banana"
words = ['banana', 'loganberry', 'passion fruit']
[len(re.findall(word, paragraph)) for word in words]
</code></pre>

<p>This returns
    [2, 0, 0]
as well.</p>
<br /><b>#3</b><br /><p>This is basically just going out of your way to avoid a list comprehension, but if you like functional style programming, then you'll like <a href="http://docs.python.org/library/functools.html#functools.partial" rel="nofollow">functools.partial</a>.</p>

<pre><code>&gt;&gt;&gt; from functools import partial
&gt;&gt;&gt; def counter(text, paragraph):
    return len(re.findall(text, paragraph))

&gt;&gt;&gt; tester = partial(counter, paragraph="I eat bananas and a banana")
&gt;&gt;&gt; map(tester, ['banana', 'loganberry', 'passion fruit'])
[2, 0, 0]
</code></pre>
<br /><b>#4</b><br /><p>For Q query words of average length L bytes on large texts of size T bytes, you need something that's NOT O(QLT). You need a DFA-style approach which can give you O(T) ... after setup costs. If your query set is rather static, then the setup cost can be ignored.</p>

<p>E.g. <code>http://en.wikipedia.org/wiki/Aho-Corasick_algorithm</code><br />
which points to a C-extension for Python:<br />
<code>http://hkn.eecs.berkeley.edu/~dyoo/python/ahocorasick/</code></p>
<br /><b>#5</b><br /><p>Here's a response to the movement of the goalposts ("I probably need the regex because I'll need word delimiters in the near future"):</p>

<p>This method parses the text once to obtain a list of all the "words". Each word is looked up in a dictionary of the target words, and if it is a target word it is counted. The time taken is O(P) + O(T) where P is the size of the paragraph and T is the number of target words. All other solutions to date (including the currently accepted solution) except my Aho-Corasick solution are O(PT).</p>

<pre><code>def counts_all(targets, paragraph, word_regex=r"\w+"):
    tally = dict((target, 0) for target in targets)
    for word in re.findall(word_regex, paragraph):
        if word in tally:
            tally[word] += 1
    return [tally[target] for target in targets]

def counts_iter(targets, paragraph, word_regex=r"\w+"):
    tally = dict((target, 0) for target in targets)
    for matchobj in re.finditer(word_regex, paragraph):
        word = matchobj.group()
        if word in tally:
            tally[word] += 1
    return [tally[target] for target in targets]
</code></pre>

<p>The finditer version is a strawman -- it's much slower than the findall version.</p>

<p>Here's the currently accepted solution expressed in a standardised form and augmented with word delimiters:</p>

<pre><code>def currently_accepted_solution_augmented(targets, paragraph):
    def tester(s): 
        def f(x):
            return len(re.findall(r"\b" + x + r"\b", s))
        return f
    return map(tester(paragraph), targets)
</code></pre>

<p>which goes overboard on closures and could be reduced to:</p>

<pre><code># acknowledgement:
# this is structurally the same as one of hughdbrown's benchmark functions
def currently_accepted_solution_augmented_without_extra_closure(targets, paragraph):
    def tester(x):
        return len(re.findall(r"\b" + x + r"\b", paragraph))
    return map(tester, targets)
</code></pre>

<p>All variations on the currently accepted solution are O(PT). Unlike the currently accepted solution, the regex search with word delimiters is not equivalent to a simple <code>paragraph.find(target)</code>. Because the re engine doesn't use the "fast search" in this case, adding the word delimiters changes it fron slow to <strong>very</strong> slow.</p>
<br /><b>#6</b><br /><p>Here's my version.  </p>

<pre><code>paragraph = "I eat bananas and a banana"

def tester(paragraph, x): return len(re.findall(x,paragraph))

print lambda paragraph: map(
    lambda x: tester(paragraph, x) , ['banana', 'loganberry', 'passion fruit']
        )(paragraph)
</code></pre>
<br />