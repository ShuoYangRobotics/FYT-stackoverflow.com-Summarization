<h3>Question (ID-769775):</h3><h2>The lines that stand out in a file, but aren't exact duplicates</h2><p>I'm combing a webapp's log file for statements that stand out.</p>

<p>Most of the lines are similar and uninteresting. I'd pass them through Unix <code>uniq</code>, however that filters nothing, as all the lines are slightly different: they all have a different timestamp, similar statements might print a different user ID, etc.</p>

<p>What's a way and/or tool to get just the lines that are notably different from any other? (But, again, not precise duplicates)</p>

<p>I was thinking about playing with Python's <a href="http://docs.python.org/library/difflib.html" rel="nofollow">difflib</a> but that seems geared toward diffing two files, rather than all pairs of lines in the same file.</p>

<p>[EDIT]</p>

<p>I assumed the solution would give a uniqueness score for each line. So by "notably different" I meant, I choose a threshold that the uniqueness score must exceed for any line to be included in the output.</p>

<p>Based on that, if there are other viable ways to define it, please discuss. Also, the method doesn't have to have 100% accuracy and recall.</p>

<p>[/EDIT]</p>

<p>Examples:</p>

<p>I'd prefer answers that are as general purpose as possible. I know I can strip away the timestamp at the beginning. Stripping the end is more challenging, as its language may be absolutely unlike anything else in the file. These sorts of details are why I shied from concrete examples before, but because some people asked...</p>

<p>Similar 1:</p>

<pre><code>2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234
2009-04-20 00:04:02 INFO  com.foo.Bar - URL:/graph?id=asdfghjk
</code></pre>

<p>Similar 2:</p>

<pre><code>2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses
2009-04-20 00:06:00 INFO  com.baz.abc.Accessor - Cache /path/to/some/different/dir hits: 4352685 / 271315, 0.004423% misses
</code></pre>

<p>Different 1:</p>

<pre><code>2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234
2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses
</code></pre>

<p>In the Different 1 case, I'd like both lines returned but not other lines like them. In other words, those 2 lines are distinct types (then I can later ask for only statistically rare line types). The edit distance is much bigger between those two, for one thing.</p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>Define "notably different".  Then have a look at <a href="http://en.wikipedia.org/wiki/Edit%5Fdistance" rel="nofollow">"edit distance" measures</a>.</p>
<br /><b>#1</b><br /><p>You could try a bit of code that counts words, and then sorts lines by those having the least common words. </p>

<p>If that doesn't do the trick, you can add in some smarts to filter out time stamps and numbers. </p>

<p>Your problem is similar to an earlier question on <a href="http://stackoverflow.com/questions/742711/shorten-a-text-and-only-keep-important-sentences">generating summaries of news stories</a>.</p>
<br /><b>#2</b><br /><p>I don't know a tool for you but if <em>I</em> were going to roll my own, I'd approach it like this:</p>

<p>Presumably the log lines have a well defined structure, no? So</p>

<ul>
<li>parse the lines on that structure</li>
<li>write a number of very basic relevance filters (functions that just return a simple number from the parsed structure)</li>
<li>run the parsed lines through a set of filters, and cut on the basis of the total score</li>
<li>possibly sort the remaining lines into various bins by the results of more filters</li>
<li>generate reports, dump bins to files, or other output</li>
</ul>

<p>If you are familiar with the unix tool <code>procmail</code>, I'm suggesting a similar treatment customized for your data.</p>

<p><hr /></p>

<p>As zacherates notes in the comments, your filters will typically ignore time stamps (and possibly IP address), and just concentrate on the content: for example <em>really</em> long http requests might represent an attack...or whatever applies to your domain.</p>

<p>Your binning filters might be as simple as a hash on a few selected fields, or you might try to do something with <a href="http://stackoverflow.com/questions/769775/the-lines-that-stand-out-in-a-file-but-arent-exact-duplicates/769790#769790">Charlie Martin's suggestion</a> and used edit distance measures.</p>
<br /><b>#3</b><br /><p>Perhaps you could do a basic calculation of "words the same"/"all words"?</p>

<p>e.g. (including an offset to allow you to ignore the timestamp and the word 'INFO', if that's always the same):</p>

<pre><code>def score(s1, s2, offset=26):
    words1 = re.findall('\w+', s1[offset:])
    words2 = re.findall('\w+', s2[offset:])
    return float(len(set(words1) &amp; set(words2)))/max(len(set(words1)), len(set(words2)))
</code></pre>

<p>Given:</p>

<pre><code>&gt;&gt;&gt; s1
'2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234'
&gt;&gt;&gt; s2
'2009-04-20 00:04:02 INFO  com.foo.Bar - URL:/graph?id=asdfghjk'
&gt;&gt;&gt; s3
'2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses'
&gt;&gt;&gt; s4
'2009-04-20 00:06:00 INFO  com.baz.abc.Accessor - Cache /path/to/some/different/dir hits: 4352685 / 271315, 0.004423% misses'
</code></pre>

<p>This yields:</p>

<pre><code>&gt;&gt;&gt; score(s1,s2)
0.8571428571428571
&gt;&gt;&gt; score(s3,s4)
0.75
&gt;&gt;&gt; score(s1,s3)
0.066666666666666666
</code></pre>

<p>You've still got to decide which lines to compare.  Also the use of set() may distort the scores slightly â€“ the price of a simple algorithm :-)</p>
<br /><b>#4</b><br /><p>I wonder if you could just focus on the part that defines uniqueness for you.  In this case, it seems that the part defining uniqueness is just the middle part:</p>

<pre>
2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234
                    ^---------------------^ 

2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses
                    ^--------------------------------^
</pre>

<p>I would then compare exactly this part, perhaps using a regular expression (just the parenthesized group; how to access sub-matches like this is language dependent):</p>

<pre><code>/^.{20}(\w+\s+[\w\.-]+\s+-\s+\w+)/
</code></pre>
<br /><b>#5</b><br /><p>I think you want to break this into fields, sort by the "severity level" field and the next field (looks like "class").  I'd use Haskell:</p>

<pre>
module Main where 
import Data.List (nubBy, sortBy)

sortAndNub s = nubBy fields2and3 
     $ sortBy fields2and3comp
     $ map words $ lines s

fields2and3 a b =    fieldEq 2 a b 
                  && fieldEq 3 a b
fieldEq f a b = a!!f == (b!!f)
fields2and3comp a b = case compare (a!!2) (b!!2) of
   LT -> LT
   GT -> GT
   EQ -> compare (a!!3) (b!!3)
main = interact $ unlines.(map unwords).sortAndNub
</pre>
<br />