<h3>Question (ID-226693):</h3><h2>Python Disk-Based Dictionary</h2><p>I was running some dynamic programming code (trying to brute-force disprove the Collatz conjecture =P) and I was using a dict to store the lengths of the chains I had already computed. Obviously, it ran out of memory at some point. Is there any easy way to use some variant of a <code>dict</code> which will page parts of itself out to disk when it runs out of room? Obviously it will be slower than an in-memory dict, and it will probably end up eating my hard drive space, but this could apply to other problems that are not so futile.</p>

<p>I realized that a disk-based dictionary is pretty much a database, so I manually implemented one using sqlite3, but I didn't do it in any smart way and had it look up every element in the DB one at a time... it was about 300x slower.</p>

<p>Is the smartest way to just create my own set of dicts, keeping only one in memory at a time, and paging them out in some efficient manner?</p>
<br /><h3>Answers (Total-9):</h3><b>#0</b><br /><p>The 3rd party <a href="http://pypi.python.org/pypi/shove" rel="nofollow">shove</a> module is also worth taking a look at. It's very similar to shelve in that it is a simple dict-like object, however it can store to various backends (such as file, SVN, and S3), provides optional compression, and is even threadsafe. It's a very handy module</p>

<pre><code>from shove import Shove

mem_store = Shove()
file_store = Shove('file://mystore')

file_store['key'] = value
</code></pre>
<br /><b>#1</b><br /><p>Hash-on-disk is generally addressed with Berkeley DB or something similar - several options are listed in the <a href="http://docs.python.org/library/persistence.html" rel="nofollow">Python Data Persistence documentation</a>. You can front it with an in-memory cache, but I'd test against native performance first; with operating system caching in place it might come out about the same.</p>
<br /><b>#2</b><br /><p>Last time I was facing a problem like this, I rewrote to use SQLite rather than a dict, and had a massive performance increase. That performance increase was at least partially on account of the database's indexing capabilities; depending on your algorithms, YMMV.</p>

<p>A thin wrapper that does SQLite queries in <code>__getitem__</code> and <code>__setitem__</code> isn't much code to write.</p>
<br /><b>#3</b><br /><p>With a little bit of thought it seems like you could get the <a href="http://blog.doughellmann.com/2007/08/pymotw-shelve.html" rel="nofollow">shelve module</a> to do what you want.</p>
<br /><b>#4</b><br /><p>The <a href="http://docs.python.org/library/shelve.html" rel="nofollow">shelve</a> module may do it; at any rate, it should be simple to test.  Instead of:</p>

<pre><code>self.lengths = {}
</code></pre>

<p>do:</p>

<pre><code>import shelve
self.lengths = shelve.open('lengths.shelf')
</code></pre>

<p>The only catch is that keys to shelves must be strings, so you'll have to replace</p>

<pre><code>self.lengths[indx]
</code></pre>

<p>with</p>

<pre><code>self.lengths[str(indx)]
</code></pre>

<p>(I'm assuming your keys are just integers, as per your comment to Charles Duffy's post)</p>

<p>There's no built-in caching in memory, but your operating system may do that for you anyway.</p>

<p>[actually, that's not quite true: you can pass the argument 'writeback=True' on creation.  The intent of this is to make sure storing lists and other mutable things in the shelf works correctly.  But a side-effect is that the whole dictionary is cached in memory.  Since this caused problems for you, it's probably not a good idea :-) ]</p>
<br /><b>#5</b><br /><p>You should bring more than one item at a time if there's some heuristic to know which are the most likely items to be retrieved next, and don't forget the indexes like Charles mentions.</p>
<br /><b>#6</b><br /><p>I did not try it yet but <a href="http://hamsterdb.com/" rel="nofollow">Hamster DB</a> is promising and has a Python interface.</p>
<br /><b>#7</b><br /><p>read answer for this question from GvR ;)
<a href="http://neopythonic.blogspot.com/2008/10/sorting-million-32-bit-integers-in-2mb.html" rel="nofollow">Sorting a million 32-bit integers in 2MB of RAM using Python</a></p>
<br /><b>#8</b><br /><p>I've read you think shelve is too slow and you tried to hack your own dict using sqlite.</p>

<p>Another did this too :</p>

<p><a href="http://sebsauvage.net/python/snyppets/index.html#dbdict" rel="nofollow">http://sebsauvage.net/python/snyppets/index.html#dbdict</a></p>

<p>It seems pretty efficient (and sebsauvage is a pretty good coder). Maybe you could give it a try ?</p>
<br />