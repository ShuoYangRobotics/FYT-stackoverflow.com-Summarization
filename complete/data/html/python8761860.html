<h3>Question (ID-8761860):</h3><h2>Design an algorithm, find the most frequently used word in a book</h2><p>An interview question:</p>

<p>Find the most frequently used word in a book.</p>

<p>My idea: </p>

<p>Use a hash table, traverse and mark the hash table. </p>

<p>If the book's size is known, if  any word is found to be used > 50%, then skip any new words in the following traversal and only count old words.  What if the book size is unknown?</p>

<p>It is O(n) and O(n) time and space. </p>

<p>Any better ideas? </p>

<p>Thanks</p>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><p>This is actually a classic example of <a href="http://en.wikipedia.org/wiki/MapReduce" rel="nofollow">map reduce</a>.</p>

<p>The example in the wikipedia page will give you the word count of each unique word, but you can easily add a step in the reduce step that keeps track of the current most common word(with some kind of mutex to deal with concurrency issues).</p>

<p>If you have a distributed cluster of machines or a highly parallelized computer this will run much faster than using the hash table.</p>
<br /><b>#1</b><br /><p>Usually <a href="http://en.wikipedia.org/wiki/Heap_%28data_structure%29" rel="nofollow">Heap</a> is the data-structure which suits well when we have to determine something like most/least used. </p>

<p>Even <a href="http://docs.python.org/release/3.1.3/library/collections.html#collections.Counter" rel="nofollow">Python;s Counter.nlargest</a> which is used for these purposes is implemented through the Heap Data-structure. </p>

<p>A Binary Heap Data-structure has the following Complexity</p>

<pre><code>CreateHeap - O(1)
FindMin - O(1)
deleteMin - O(logn)
Insert - O(logn)
</code></pre>

<p>I ran a comparition on Hash (using default dictionary in Python) and Heap (using Collections.Counter.nlargest in python) and the Hash is fairing slightly better than Heap.</p>

<pre><code>&gt;&gt;&gt; stmt1="""
import collections, random
somedata=[random.randint(1,1000) for i in xrange(1,10000)]
somehash=collections.defaultdict(int)
for d in somedata:
    somehash[d]+=1
maxkey=0
for k,v in somehash.items():
    if somehash[maxkey] &gt; v:
        maxkey=k
"""
&gt;&gt;&gt; stmt2="""
import collections,random
somedata=[random.randint(1,1000) for i in xrange(1,10000)]
collections.Counter(somedata).most_common(1)
"""
&gt;&gt;&gt; t1=timeit.Timer(stmt=stmt1)
&gt;&gt;&gt; t2=timeit.Timer(stmt=stmt2)
&gt;&gt;&gt; print "%.2f usec/pass" % (1000000 * t2.timeit(number=10)/10)
38168.96 usec/pass
&gt;&gt;&gt; print "%.2f usec/pass" % (1000000 * t1.timeit(number=10)/10)
33600.80 usec/pass
</code></pre>
<br /><b>#2</b><br /><p>There is a generalization of your optimization- if the book size is known and any word you have seen has a count > the remaining number of words + the next-highest count, your current highest-counted word is the answer.</p>
<br /><b>#3</b><br /><p>To determine complexity I think you need to consider two variables, n = total number of words, m = number of unique words.  I imagine the best case complexity will come out close to O(n log(m)) for speed, and O(m) for storage, assuming each time you iterate over each of n words, and build and search based on a hash table or other such structure which eventually contains m elements.</p>
<br /><b>#4</b><br /><p>Your solution is correct, fast, and probably the best/easiest from a practical standpoint. </p>

<p>The other poster's solutions have worse time complexities than your solution. For a hash, as you are using, the time complexity is indeed O(n). Each insertion is O(1) and there are n words, so the insertion phase costs O(n). Iterating through and finding the max is then O(n). The space is also O(n) as you mentioned.</p>

<p>Note that you will not be able to terminate your algorithm early using Chris's solution because searching your hash table is costly and there is no way for you to perform this in O(1) time after each insertion.</p>

<p>A heap will cost more in time because you need to maintain the heap during each insertion. A heap insertion is O(log(n)) and thus the total cost for insertion will be O(nlog(n)).</p>
<br /><b>#5</b><br /><p>If you are dealing with a book, then you know the vocabulary and the approximate word frequencies. Even if you are not given this information up front, you can get a good estimate by scanning a random sample.</p>

<p>For the exact answer, I would use a perfect hash function of the k most common words. A perfect hash function requires O(k) memory and guarantees fast worst-case O(1) lookup.</p>

<p>For the uncommon words, I would use a priority queue implemented as a heap or a self-balancing tree. A regular hash table might also be a good choice.</p>
<br /><b>#6</b><br /><p>I think you have the right solution, but you are wrong about the time complexity. The complexity of a hash based system is O(log(n)) and therefore your algorithm is of time complexity O(nlog(n)).</p>

<p>Edit: The best worst case complexity for hash table search is O(log(n)). The worst case complexity for a regular hash table implementation is O(n). In any case, it is definitely not O(1) as assumed by the author of this question)</p>
<br />