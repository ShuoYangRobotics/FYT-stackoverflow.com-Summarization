<h3>Question (ID-850117):</h3><h2>What's the most efficient way to insert thousands of records into a table (MySQL, Python, Django)</h2><p>I have a database table with a unique string field and a couple of integer fields. The string field is usually 10-100 characters long.</p>

<p>Once every minute or so I have the following scenario: I receive a list of 2-10 thousand tuples corresponding to the table's record structure, e.g. </p>

<pre><code>[("hello", 3, 4), ("cat", 5, 3), ...]
</code></pre>

<p>I need to insert all these tuples to the table (assume I verified neither of these strings appear in the database). For clarification, I'm using InnoDB, and I have an auto-incremental primary key for this table, the string is not the PK. </p>

<p>My code currently iterates through this list, for each tuple creates a Python module object with the appropriate values, and calls ".save()", something like so:</p>

<pre><code>@transaction.commit_on_success
def save_data_elements(input_list):
    for (s, i1, i2) in input_list:
        entry = DataElement(string=s, number1=i1, number2=i2)
        entry.save()
</code></pre>

<p>This code is currently one of the performance bottlenecks in my system, so I'm looking for ways to optimize it. </p>

<p>For example, I could generate SQL codes each containing an INSERT command for 100 tuples ("hard-coded" into the SQL) and execute it, but I don't know if it will improve anything.</p>

<p>Do you have any suggestion to optimize such a process?</p>

<p>Thanks</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><p>For MySQL specifically, the fastest way to load data is using <a href="http://dev.mysql.com/doc/refman/5.1/en/load-data.html" rel="nofollow">LOAD DATA INFILE</a>, so if you could convert the data into the format that expects, it'll probably be the fastest way to get it into the table.</p>
<br /><b>#1</b><br /><p>You can write the rows to a file in the format
"field1", "field2", .. and then use LOAD DATA to load them</p>

<pre><code>data = '\n'.join(','.join('"%s"' % field for field in row) for row in data)
f= open('data.txt', 'w')
f.write(data)
f.close()
</code></pre>

<p>Then execute this:</p>

<pre><code>LOAD DATA INFILE 'data.txt' INTO TABLE db2.my_table;
</code></pre>

<p><a href="http://dev.mysql.com/doc/refman/5.1/en/load-data.html" rel="nofollow">Reference</a></p>
<br /><b>#2</b><br /><p>If you don't <code>LOAD DATA INFILE</code> as some of the other suggestions mention, two things you can do to speed up your inserts are :</p>

<ol>
<li>Use prepared statements - this cuts out the overhead of parsing the SQL for every insert</li>
<li>Do all of your inserts in a single transaction - this would require using a DB engine that supports transactions (like InnoDB)</li>
</ol>
<br /><b>#3</b><br /><p>If you can do a hand-rolled <code>INSERT</code> statement, then that's the way I'd go. A single <code>INSERT</code> statement with multiple value clauses is much much faster than lots of individual <code>INSERT</code> statements.</p>
<br /><b>#4</b><br /><p>what format do you receive? if it is a file, you can do some sort of bulk load: <a href="http://www.classes.cs.uchicago.edu/archive/2005/fall/23500-1/mysql-load.html" rel="nofollow">http://www.classes.cs.uchicago.edu/archive/2005/fall/23500-1/mysql-load.html</a></p>
<br /><b>#5</b><br /><p>Regardless of the insert method, you will want to use the InnoDB engine for maximum read/write concurrency.  MyISAM will lock the entire table for the duration of the insert whereas InnoDB (under most circumstances) will only lock the affected rows, allowing SELECT statements to proceed.</p>
<br /><b>#6</b><br /><p>I donot know the exact details, but u can use json style data representation and use it as fixtures or something. I saw something similar on Django Video Workshop by Douglas Napoleone. See the videos at <a href="http://www.linux-magazine.com/online/news/django%5Fvideo%5Fworkshop" rel="nofollow">http://www.linux-magazine.com/online/news/django_video_workshop</a>. and <a href="http://www.linux-magazine.com/online/features/django%5Freloaded%5Fworkshop%5Fpart%5F1" rel="nofollow">http://www.linux-magazine.com/online/features/django_reloaded_workshop_part_1</a>. Hope this one helps.</p>

<p>Hope you can work it out. I just started learning django, so I can just point you to resources.</p>
<br /><b>#7</b><br /><p>This is unrelated to the actual load of data into the DB, but...</p>

<p>If providing a "The data is loading... The load will be done shortly" type of message to the user is an option, then you can run the INSERTs or LOAD DATA asynchronously in a different thread.</p>

<p>Just something else to consider.</p>
<br />