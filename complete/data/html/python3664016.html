<h3>Question (ID-3664016):</h3><h2>What languages are good for writing a web crawler?</h2><p>I have substantial PHP experience, although I realize that PHP probably isn't the best language for a large-scale web crawler because a process can't run indefinitely. What languages do people suggest?</p>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><p>Most languages would probably be a reasonable fit, the critical components are </p>

<ol>
<li>Libraries to deal with the Internet Protcols</li>
<li>Libraries to deal with regular expressions</li>
<li>Libraries to parse HTML content</li>
</ol>

<p>Today most languages have libraries with good support for the above, of course you will need some way to persist the results that might be a database of some sorts. </p>

<p>The more important thing rather than the language is understanding all concepts you need to deal with. Here are some Python examples that might help get you started.</p>

<p><a href="http://www.example-code.com/python/pythonspider.asp" rel="nofollow">http://www.example-code.com/python/pythonspider.asp</a></p>
<br /><b>#1</b><br /><p>Any language you can easily use with a good network library and support for parsing the formats you want to crawl. Those are really the only qualifications.</p>
<br /><b>#2</b><br /><p>You could consider using a combination of python and PyGtkMozEmbed or PyWebKitGtk plus javascript to create your spider.</p>

<p>The spidering could be done in javascript after the page and all other scripts have loaded.</p>

<p>You'd have one of the few web spiders that supports javascript, and might pick up some hidden stuff the others don't see :)</p>
<br /><b>#3</b><br /><p>C++ - if you know what you're doing. You will not need a web server and a web application, because a web crawler is just a client, after all. </p>
<br /><b>#4</b><br /><p>why write your own when you can copy <a href="http://code.activestate.com/recipes/576551-simple-web-crawler/" rel="nofollow">http://code.activestate.com/recipes/576551-simple-web-crawler/</a></p>

<p>you might need to fix a few things here and there, like use htmlentities instead of replacing &amp; with &amp;</p>
<br /><b>#5</b><br /><p>C is GoD of all when it comes to writing multicore/threaded crawlers but then it has its own complication. After C, some go for JAVA (due to wide exploration and usage) while other go to Python. If you have nice architecture, I can assure you these three language would really not limit your efficiency. </p>

<p>This python code is a C Curl implementation and can crawl around 10,000 pages in 300 secs on a nice server</p>

<pre><code>#! /usr/bin/env python
# -*- coding: iso-8859-1 -*-
# vi:ts=4:et
# $Id: retriever-multi.py,v 1.29 2005/07/28 11:04:13 mfx Exp $

#
# Usage: python retriever-multi.py &lt;file with URLs to fetch&gt; [&lt;# of
#          concurrent connections&gt;]
#

import sys
import pycurl

# We should ignore SIGPIPE when using pycurl.NOSIGNAL - see
# the libcurl tutorial for more info.
try:
    import signal
    from signal import SIGPIPE, SIG_IGN
    signal.signal(signal.SIGPIPE, signal.SIG_IGN)
except ImportError:
    pass


# Get args
num_conn = 10
try:
    if sys.argv[1] == "-":
        urls = sys.stdin.readlines()
    else:
        urls = open(sys.argv[1]).readlines()
    if len(sys.argv) &gt;= 3:
        num_conn = int(sys.argv[2])
except:
    print "Usage: %s &lt;file with URLs to fetch&gt; [&lt;# of concurrent connections&gt;]" % sys.argv[0]
    raise SystemExit


# Make a queue with (url, filename) tuples
queue = []
for url in urls:
    url = url.strip()
    if not url or url[0] == "#":
        continue
    filename = "doc_%03d.dat" % (len(queue) + 1)
    queue.append((url, filename))


# Check args
assert queue, "no URLs given"
num_urls = len(queue)
num_conn = min(num_conn, num_urls)
assert 1 &lt;= num_conn &lt;= 10000, "invalid number of concurrent connections"
print "PycURL %s (compiled against 0x%x)" % (pycurl.version, pycurl.COMPILE_LIBCURL_VERSION_NUM)
print "----- Getting", num_urls, "URLs using", num_conn, "connections -----"


# Pre-allocate a list of curl objects
m = pycurl.CurlMulti()
m.handles = []
for i in range(num_conn):
    c = pycurl.Curl()
    c.fp = None
    c.setopt(pycurl.FOLLOWLOCATION, 1)
    c.setopt(pycurl.MAXREDIRS, 5)
    c.setopt(pycurl.CONNECTTIMEOUT, 30)
    c.setopt(pycurl.TIMEOUT, 300)
    c.setopt(pycurl.NOSIGNAL, 1)
    m.handles.append(c)


# Main loop
freelist = m.handles[:]
num_processed = 0
while num_processed &lt; num_urls:
    # If there is an url to process and a free curl object, add to multi stack
    while queue and freelist:
        url, filename = queue.pop(0)
        c = freelist.pop()
        c.fp = open(filename, "wb")
        c.setopt(pycurl.URL, url)
        c.setopt(pycurl.WRITEDATA, c.fp)
        m.add_handle(c)
        # store some info
        c.filename = filename
        c.url = url
    # Run the internal curl state machine for the multi stack
    while 1:
        ret, num_handles = m.perform()
        if ret != pycurl.E_CALL_MULTI_PERFORM:
            break
    # Check for curl objects which have terminated, and add them to the freelist
    while 1:
        num_q, ok_list, err_list = m.info_read()
        for c in ok_list:
            c.fp.close()
            c.fp = None
            m.remove_handle(c)
            print "Success:", c.filename, c.url, c.getinfo(pycurl.EFFECTIVE_URL)
            freelist.append(c)
        for c, errno, errmsg in err_list:
            c.fp.close()
            c.fp = None
            m.remove_handle(c)
            print "Failed: ", c.filename, c.url, errno, errmsg
            freelist.append(c)
        num_processed = num_processed + len(ok_list) + len(err_list)
        if num_q == 0:
            break
    # Currently no more I/O is pending, could do something in the meantime
    # (display a progress bar, etc.).
    # We just call select() to sleep until some more data is available.
    m.select(1.0)


# Cleanup
for c in m.handles:
    if c.fp is not None:
        c.fp.close()
        c.fp = None
    c.close()
m.close()
</code></pre>
<br /><b>#6</b><br /><p>C# and C++ are probably the best two languages for this, it's just a matter of which you know better and which is faster (C# is probably easier).</p>

<p>I wouldn't recommend Python, Javascript, or PHP. They will usually be slower in text processing compared to a C-family language. If you're looking to crawl any significant chunk of the web, you'll need all the speed you can get.</p>

<p>I've used C# and the HtmlAgilityPack to do so before, it works relatively well and is pretty easy to pick up. The ability to use a lot of the same commands to work with HTML as you would XML makes it nice (I had experience working with XML in C#).</p>

<p>You might want to test the speed of available C# HTML parsing libraries vs C++ parsing libraries. I know in my app, I was running through 60-70 fairly messy pages a second and pulling a good bit of data out of each (but that was a site with a pretty constant layout).</p>

<p>Edit: I notice you mentioned accessing a database. Both C++ and C# have libraries to work with most common database systems, from SQLite (which would be great for a quick crawler on a few sites) to midrange engines like MySQL and MSSQL up to the bigger DB engines (I've never used Oracle or DB2 from either language, but it's possible).</p>
<br />