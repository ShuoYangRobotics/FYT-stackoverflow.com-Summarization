<h3>Question (ID-956867):</h3><h2>How to get string Objects instead Unicode ones from JSON in Python?</h2><p>I'm using Python (Python 2.5.2 on Ubuntu 8.10) to parse JSON from (ASCII encoded) text files. When loading these files with <code>json</code> (<code>simplejson</code>), all my string values are cast to Unicode objects instead of string objects.</p>

<p>The problem is, I have to use the data with some libraries that only accept string objects.</p>

<p>Is it possible to get string objects instead unicode ones from <code>simplejson</code>?<br />
<em>Any hints on how I can achieve this automatically?</em></p>

<p><strong>Edit:</strong> I can't change the libraries nor update them. One - the <code>csv</code> module - is even in the Python standard library (the documentation says it will support Unicode in the future). I could write wrappers of course, but maybe there is a more convenient way?</p>

<p>The actual data I parse from the JSON files is rather nested and complex, so it would be a pain to look for every Unicode object therein and cast it manually... </p>

<p>Here's a small example:</p>

<pre><code>>>> import simplejson as json
>>> l = ['a', 'b']
>>> l
['a', 'b']
>>> js = json.dumps(l)
>>> js
'["a", "b"]'
>>> nl = json.loads(js)
>>> nl
[u'a', u'b']</code></pre>

<p><strong>Update</strong>: I completely agree with Jarret Hardie and nosklo: Since the <a href="http://www.json.org/" rel="nofollow">JSON specs</a> specifically state strings as Unicode <code>simplejson</code> should return Unicode objects.</p>

<p><em>But</em> while searching the net, I came across some post, where people complain about <code>simplejson</code> actually returning string objects... I couldn't reproduce this behavior but it seems it is possible. <em>Any hints?</em></p>

<h1>Workaround</h1>

<p>Right now I use <strong>PyYAML</strong> to parse the files, it gives me string objects.<br />
Since JSON is a subset of YAML it works nicely.</p>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><p>That's because json has no difference between string objects and unicode objects. They're all strings in javascript.</p>

<p>I think <strong>JSON is right to return unicode objects</strong>. In fact, I wouldn't accept anything less, since javascript strings <strong>are in fact <code>unicode</code> objects</strong> (i.e. JSON (javascript) strings can store <em>any kind</em> of unicode character) so it makes sense to create <code>unicode</code> objects when translating strings from JSON. Plain strings just wouldn't fit since the library would have to guess the encoding you want.</p>

<p>It's better to use <code>unicode</code> string objects everywhere. So your best option is to update your libraries so they can deal with unicode objects.</p>

<p>But if you really want bytestrings, just encode the results to the encoding of your choice:</p>

<pre><code>&gt;&gt;&gt; nl = json.loads(js)
&gt;&gt;&gt; nl
[u'a', u'b']
&gt;&gt;&gt; nl = [s.encode('utf-8') for s in nl]
&gt;&gt;&gt; nl
['a', 'b']
</code></pre>
<br /><b>#1</b><br /><p>You can use the object_hook parameter for <a href="http://docs.python.org/library/json.html#json.loads" rel="nofollow">json.loads</a> to pass in a converter. You don't have to do the conversion after the fact. The <a href="http://docs.python.org/library/json.html" rel="nofollow">json</a> module will always pass the object_hook dicts only, and it will recursively pass in nested dicts, so you don't have to recurse into nested dicts yourself. I don't think I would convert unicode strings to numbers like Wells shows. If it's a unicode string, it was quoted as a string in the JSON file, so it is supposed to be a string (or the file is bad).</p>

<p>Also, I'd try to avoid doing something like str(val) on an unicode object. You should use value.encode(encoding) with a valid encoding, depending on what your external lib expects.</p>

<p>So, for example:</p>

<pre><code>def _decode_list(data):
    rv = []
    for item in data:
        if isinstance(item, unicode):
            item = item.encode('utf-8')
        elif isinstance(item, list):
            item = _decode_list(item)
        elif isinstance(item, dict):
            item = _decode_dict(item)
        rv.append(item)
    return rv

def _decode_dict(data):
    rv = {}
    for key, value in data.iteritems():
        if isinstance(key, unicode):
           key = key.encode('utf-8')
        if isinstance(value, unicode):
           value = value.encode('utf-8')
        elif isinstance(value, list):
           value = _decode_list(value)
        elif isinstance(value, dict):
           value = _decode_dict(value)
        rv[key] = value
    return rv

obj = json.loads(s, object_hook=_decode_dict)
</code></pre>
<br /><b>#2</b><br /><p>I'm afraid there's no way to achieve this automatically within the simplejson library.</p>

<p>The scanner and decoder in simplejson are designed to produce unicode text. To do this, the library uses a function called <code>c_scanstring</code> (if it's available, for speed), or <code>py_scanstring</code> if the C version is not available. The <code>scanstring</code> function is called several times by nearly every routine that simplejson has for decoding a structure that might contain text. You'd have to either monkeypatch the <code>scanstring</code> value in simplejson.decoder, or subclass <code>JSONDecoder</code> and provide pretty much your own entire implementation of anything that might contain text.</p>

<p>The reason that simplejson outputs unicode, however, is that the <a href="http://www.json.org/" rel="nofollow">json spec</a> specifically mentions that "A string is a collection of zero or more Unicode characters"... support for unicode is assumed as part of the format itself. Simplejson's <code>scanstring</code> implementation goes so far as to scan and interpret unicode escapes (even error-checking for malformed multi-byte charset representations), so the only way it can reliably return the value to you is as unicode.</p>

<p>If you have an aged library that needs an <code>str</code>, I recommend you either laboriously search the nested data structure after parsing (which I acknowledge is what you explicitly said you wanted to avoid... sorry), or perhaps wrap your libraries in some sort of facade where you can massage the input parameters at a more granular level. The second approach might be more manageable than the first if your data structures are indeed deeply nested.</p>
<br /><b>#3</b><br /><p>This is late to the game, but I built this recursive caster. It works for my needs and I think it's relatively complete. It may help you.</p>

<pre><code>def _parseJSON(self, obj):
	newobj = {}

	for key, value in obj.iteritems():
		key = str(key)

		if isinstance(value, dict):
			newobj[key] = self._parseJSON(value)
		elif isinstance(value, list):
			if key not in newobj:
				newobj[key] = []
				for i in value:
					newobj[key].append(self._parseJSON(i))
		elif isinstance(value, unicode):
			val = str(value)
			if val.isdigit():
				val = int(val)
			else:
				try:
					val = float(val)
				except ValueError:
					val = str(val)
			newobj[key] = val

	return newobj
</code></pre>

<p>Just pass it a JSON object like so:</p>

<pre><code>obj = json.loads(content, parse_float=float, parse_int=int)
obj = _parseJSON(obj)
</code></pre>

<p>I have it as a private member of a class, but you can repurpose the method as you see fit.</p>
<br /><b>#4</b><br /><p>So, I've run into the same problem. Guess what was the first Google result.</p>

<p>Because I need to pass all data to PyGTK, unicode strings aren't very useful to me either. So I have another recursive conversion method. It's actually also needed for typesafe JSON conversion - json.dump() would bail on any non-literals, like Python objects. Doesn't convert dict indexes though.</p>

<pre><code># removes any objects, turns unicode back into str
def filter_data(obj):
        if type(obj) in (int, float, str, bool):
                return obj
        elif type(obj) == unicode:
                return str(obj)
        elif type(obj) in (list, tuple, set):
                obj = list(obj)
                for i,v in enumerate(obj):
                        obj[i] = filter_data(v)
        elif type(obj) == dict:
                for i,v in obj.iteritems():
                        obj[i] = filter_data(v)
        else:
                print "invalid object in data, converting to string"
                obj = str(obj) 
        return obj
</code></pre>
<br /><b>#5</b><br /><p>The gotcha is that <code>simplejson</code> and <code>json</code> are two different modules, at least in the manner they deal with unicode. You have <code>json</code> in py 2.6+, and this gives you unicode values, whereas <code>simplejson</code> returns string objects. Just try easy_install-ing simplejson in your environment and see if that works. It did for me.</p>
<br /><b>#6</b><br /><p>I ran into this problem too, and having to deal with JSON, I came up with a small loop that converts the unicode keys to strings.  (Simplejson on GAE does not return string keys)</p>

<p>obj is the object decoded from json</p>

<pre><code>            if NAME_CLASS_MAP.has_key(cls):
                kwargs = {}
                for i in obj.keys():
                    kwargs[str(i)] = obj[i]
                o = NAME_CLASS_MAP[cls](**kwargs)
                o.save()
</code></pre>

<p>kwargs is what I pass to the constructor of the GAE application (which does not like unicode keys in **kwargs)</p>

<p>Not as robust as the solution from Wells, but much smaller.</p>
<br />