<h3>Question (ID-3372332):</h3><h2>Python: best/efficient way of finding a list of words in a text?</h2><p>I have a list of approximately 300 words and a huge amount of text that I want to scan to know how many times each word appears.</p>

<p>I am using the <a href="http://docs.python.org/library/re.html" rel="nofollow">re</a> module from python:</p>

<pre><code>for word in list_word:
    search = re.compile(r"""(\s|,)(%s).?(\s|,|\.|\))""" % word)
    occurrences = search.subn("", text)[1]
</code></pre>

<p>but I want to know if there is a more efficient or more elegant way of doing this?</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><p>If you have a huge amount of text, I wouldn't use regexps in this case but simply split text:</p>

<pre><code>words = {"this": 0, "that": 0}
for w in text.split():
  if w in words:
    words[w] += 1
</code></pre>

<p>words will give you the frequency for each word</p>
<br /><b>#1</b><br /><p>Try stripping all the punctuation from your text and then splitting on whitespace. Then simply do </p>

<pre><code>for word in list_word:
    occurence = strippedText.count(word)
</code></pre>

<p>Or if you're using python 3.0 I think you could do:</p>

<pre><code>occurences = {word: strippedText.count(word) for word in list_word}
</code></pre>
<br /><b>#2</b><br /><p>Googling: python frequency
gives me this page as the first result: <a href="http://www.daniweb.com/code/snippet216747.html" rel="nofollow">http://www.daniweb.com/code/snippet216747.html</a></p>

<p>Which seems to be what you're looking for.</p>
<br /><b>#3</b><br /><p>You can also split the text into words and search the resulting list.</p>
<br /><b>#4</b><br /><p>Regular expressions may not be what you want. Python has a number of built-in string operations that are <strong>much</strong> faster, and I believe .count() has what you need.</p>

<p><a href="http://docs.python.org/library/stdtypes.html#string-methods" rel="nofollow">http://docs.python.org/library/stdtypes.html#string-methods</a></p>
<br /><b>#5</b><br /><p>If Python is not a must, you can use awk</p>

<pre><code>$ cat file
word1
word2
word3
word4

$ cat file1
blah1 blah2 word1 word4 blah3 word2
junk1 junk2 word2 word1 junk3
blah4 blah5 word3 word6 end

$ awk 'FNR==NR{w[$1];next} {for(i=1;i&lt;=NF;i++) a[$i]++}END{for(i in w){ if(i in a) print i,a[i] } } ' file file1
word1 2
word2 2
word3 1
word4 1
</code></pre>
<br /><b>#6</b><br /><p>It sounds to me like the Natural Language Toolkit might have what you need.</p>

<p><a href="http://www.nltk.org/" rel="nofollow">http://www.nltk.org/</a></p>
<br /><b>#7</b><br /><p>Maybe you could adapt this my multisearch generator function.</p>

<pre><code>    from itertools import islice
testline = "Sentence 1.  Sentence 2?  Sentence 3!  Sentence 4.  Sentence 5."
def multis(search_sequence,text,start=0):
    """ multisearch by given search sequence values from text, starting from position start
        yielding tuples of text before sequence item and found sequence item"""
    x=''
    for ch in text[start:]:
        if ch in search_sequence:
            if x: yield (x,ch)
            else: yield ch
            x=''
        else:
            x+=ch
    else:
        if x: yield x

# split the first two sentences by the dot/question/exclamation.
two_sentences = list(islice(multis('.?!',testline),2)) ## must save the result of generation
print "result of split: ", two_sentences

print '\n'.join(sentence.strip()+sep for sentence,sep in two_sentences)
</code></pre>
<br />