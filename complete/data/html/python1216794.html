<h3>Question (ID-1216794):</h3><h2>Python subprocess.Popen erroring with OSError: [Errno 12] Cannot allocate memory after period of time</h2><p><strong>Note</strong>: This question has been re-asked with a summary of all debugging attempts <a href="http://stackoverflow.com/questions/1367373/python-subprocess-popen-oserror-errno-12-cannot-allocate-memory">here</a>.</p>

<p><hr /></p>

<p>I have a Python script that is running as a background process executing every 60 seconds. Part of that is a call to <a href="http://docs.python.org/library/subprocess.html" rel="nofollow">subprocess.Popen</a> to get the output of <a href="http://linuxcommand.org/man%5Fpages/ps1.html" rel="nofollow">ps</a>. </p>

<pre><code>ps = subprocess.Popen(['ps', 'aux'], stdout=subprocess.PIPE).communicate()[0]
</code></pre>

<p>After running for a few days, the call is erroring with:</p>

<pre>
File "/home/admin/sd-agent/checks.py", line 436, in getProcesses
File "/usr/lib/python2.4/subprocess.py", line 533, in __init__
File "/usr/lib/python2.4/subprocess.py", line 835, in _get_handles
OSError: [Errno 12] Cannot allocate memory
</pre>

<p>However the output of <a href="http://www.linuxcommand.org/man%5Fpages/free1.html" rel="nofollow">free</a> on the server is:</p>

<pre>
$ free -m
                  total       used       free     shared     buffers    cached
Mem:                894        345        549          0          0          0
-/+ buffers/cache:  345        549
Swap:                 0          0          0
</pre>

<p>I have searched around for the problem and found <a href="http://www.zenoss.com/community/wiki/common-error-messages-and-solutions/oserror-errno-12-cannot-allocate-memory-in-popen2.py/" rel="nofollow">this article</a> which says:</p>

<p>Solution is to add more swap space to your server. When the kernel is forking to start the modeler or discovery process, it first ensures there's enough space available on the swap store the new process if needed.</p>

<p>I note that there is no available swap from the free output above. Is this likely to be the problem and/or what other solutions might there be?</p>

<p><strong>Update 13th Aug 09</strong> The code above is called every 60 seconds as part of a series of monitoring functions. The process is daemonized and the check is scheduled using <a href="http://docs.python.org/library/sched.html" rel="nofollow">sched</a>. The specific code for the above function is:</p>

<pre><code>def getProcesses(self):
    self.checksLogger.debug('getProcesses: start')

    # Memory logging (case 27152)
    if self.agentConfig['debugMode'] and sys.platform == 'linux2':
    	mem = subprocess.Popen(['free', '-m'], stdout=subprocess.PIPE).communicate()[0]
    	self.checksLogger.debug('getProcesses: memory before Popen - ' + str(mem))

    # Get output from ps
    try:
    	self.checksLogger.debug('getProcesses: attempting Popen')

    	ps = subprocess.Popen(['ps', 'aux'], stdout=subprocess.PIPE).communicate()[0]

    except Exception, e:
    	import traceback
    	self.checksLogger.error('getProcesses: exception = ' + traceback.format_exc())
    	return False

    self.checksLogger.debug('getProcesses: Popen success, parsing')

    # Memory logging (case 27152)
    if self.agentConfig['debugMode'] and sys.platform == 'linux2':
    	mem = subprocess.Popen(['free', '-m'], stdout=subprocess.PIPE).communicate()[0]
    	self.checksLogger.debug('getProcesses: memory after Popen - ' + str(mem))

    # Split out each process
    processLines = ps.split('\n')

    del processLines[0] # Removes the headers
    processLines.pop() # Removes a trailing empty line

    processes = []

    self.checksLogger.debug('getProcesses: Popen success, parsing, looping')

    for line in processLines:
    	line = line.split(None, 10)
    	processes.append(line)

    self.checksLogger.debug('getProcesses: completed, returning')

    return processes
</code></pre>

<p>This is part of a bigger class called checks which is initialised once when the daemon is started.</p>

<p>The entire checks class can be found at <a href="http://github.com/dmytton/sd-agent/blob/82f5ff9203e54d2adeee8cfed704d09e3f00e8eb/checks.py" rel="nofollow">http://github.com/dmytton/sd-agent/blob/82f5ff9203e54d2adeee8cfed704d09e3f00e8eb/checks.py</a> with the getProcesses function defined from line 442. This is called by doChecks() starting at line 520.</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><p>You've perhaps got a memory leak bounded by some <a href="http://www.opengroup.org/onlinepubs/9699919799/functions/getrlimit.html" rel="nofollow">resource limit</a> (<code>RLIMIT_DATA</code>, <code>RLIMIT_AS</code>?) inherited by your python script.  Check your <em>ulimit(1)</em>s before you run your script, and profile the script's memory usage, as others have suggested.</p>

<p><strong>What do you do with the variable <code>ps</code> after the code snippet you show us?</strong>  Do you keep a reference to it, never to be freed?  Quoting the <a href="http://docs.python.org/library/subprocess.html#subprocess.Popen.communicate" rel="nofollow"><code>subprocess</code> module docs</a>:</p>

<blockquote>
  <p><strong>Note:</strong> The data read is buffered in memory, so do not use this
  method if the data size is large or unlimited.</p>
</blockquote>

<p>... and <em>ps aux</em> can be verbose on a busy system...</p>

<p><strong>Update</strong></p>

<p>You can check rlimits from with your python script using the <a href="http://docs.python.org/library/resource.html#resource.getrlimit" rel="nofollow">resource</a> module:</p>

<pre><code>import resource
print resource.getrlimit(resource.RLIMIT_DATA) # =&gt; (soft_lim, hard_lim)
print resource.getrlimit(resource.RLIMIT_AS)
</code></pre>

<p>If these return "unlimited" -- <code>(-1, -1)</code> -- then my hypothesis is incorrect and you may move on!</p>

<p>See also <a href="http://docs.python.org/library/resource.html#resource.getrusage" rel="nofollow"><code>resource.getrusage</code></a>, esp. the <code>ru_??rss</code> fields, which can help you to instrument for memory consumption from with the python script, without shelling out to an external program.</p>
<br /><b>#1</b><br /><p>That swap space answer is bogus. Historically Unix systems wanted swap space available like that, but they don't work that way anymore (and Linux never worked that way). You're not even close to running out of memory, so that's not likely the actual problem - you're running out of some other limited resource.</p>

<p>Given where the error is occuring (_get_handles calls os.pipe() to create pipes to the child), the only real problem you could be running into is not enough free file descriptors. I would instead look for unclosed files (lsof -p on the PID of the process doing the popen). If your program really needs to keep a lot of files open at one time, then increase the user limit and/or the system limit for open file descriptors.</p>
<br /><b>#2</b><br /><p>You might want to actually wait for all of those PS processes to finish before adding swap space.</p>

<p>It's not at all clear what "running as a background process executing every 60 seconds" means.</p>

<p>But your call to subprocess.Popen is forking a new process each time. </p>

<p><strong>Update</strong>.</p>

<p>I'd guess that you're somehow leaving all those processes running or hung in a zombie state.  However, the <code>communicate</code> method <em>should</em> clean up the spawned subprocesses.</p>
<br /><b>#3</b><br /><p>If you're running a background process, chances are that you've redirected your processes stdin/stdout/stderr.</p>

<p>In that case, append the option "close_fds=True" to your Popen call, which will prevent the child process from inheriting your redirected output.  This may be the limit you're bumping into.</p>
<br /><b>#4</b><br /><p>I don't think that the circumstances given in the Zenoss article you linked to is the only cause of this message, so it's not clear yet that swap space is definitely the problem. I would advise logging some more information even around successful calls, so that you can see the state of free memory every time just before you do the <code>ps</code> call.</p>

<p>One more thing - if you specify <code>shell=True</code> in the Popen call, do you see different behaviour?</p>

<p><strong>Update:</strong> If not memory, the next possible culprit is indeed file handles. I would advise running the failing command under <a href="http://en.wikipedia.org/wiki/Strace" rel="nofollow"><code>strace</code></a> to see exactly which system calls are failing.</p>
<br /><b>#5</b><br /><p>Have you watched your process over time? </p>

<ul>
<li>lsof </li>
<li>ps -aux | grep -i pname </li>
<li>top</li>
</ul>

<p>All should give interesting information. I am thinking that the process is tying up resources that should be freed up. Is there a chance that it is tying up resource handles (memory blocks, streams, file handles, thread or process handles)? stdin, stdout, stderr from the spawned "ps". Memory handles, ... from many small incremental allocations. I would be very interested in seeing what the above commands display for your process when it has just finished launching and running for the first time and after 24 hours of "sitting" there launching the sub-process regularly.</p>

<p>Since it dies after a few days, you could have it run for only a few loops, and then restart it once a day as a workaround. That would help you in the meantime.</p>

<p>Jacob</p>
<br /><b>#6</b><br /><p>You need to</p>

<pre><code>ps = subprocess.Popen(["sleep", "1000"])
os.waitpid(ps.pid, 0)
</code></pre>

<p>to free resources.</p>

<p>Note: this does not work on Windows.</p>
<br /><b>#7</b><br /><p>when you use popen you need to hand in close_fds=True if you want it to close extra file descriptors.  </p>

<p>creating a new pipe, which occurs in the _get_handles function from the back trace, creates 2 file descriptors, but your current code never closes them and your eventually hitting your systems max fd limit.  </p>

<p>Not sure why the error your getting is indicating an out of memory condition, it should be a file descriptor error as the return value of pipe() has an error code for this problem.</p>
<br />