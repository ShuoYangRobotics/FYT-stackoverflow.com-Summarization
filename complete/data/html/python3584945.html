<h3>Question (ID-3584945):</h3><h2>non-technical benefits of having string-type immutable</h2><p>I am wondering about the benefits of having the string-type immutable from the programmers point-of-view.</p>

<p>Technical benefits (on the compiler/language side) can be summarized mostly that it is easier to do optimisations if the type is immutable. Read <a href="http://stackoverflow.com/questions/2916358/immutable-strings-vs-stdstring">here</a> for a related question.</p>

<p>Also, in a mutable string type, either you have thread-safety already built-in (then again, optimisations are harder to do) or you have to do it yourself. You will have in any case the choice to use a mutable string type with built-in thread safety, so that is not really an advantage of immutable string-types. (Again, it will be easier to do the handling and optimisations to ensure thread-safety on the immutable type but that is not the point here.)</p>

<p>But what are the benefits of immutable string-types in the usage? What is the point of having some types immutable and others not? That seems very inconsistent to me.</p>

<p>In C++, if I want to have some string to be immutable, I am passing it as const reference to a function (<code>const std::string&amp;</code>). If I want to have a changeable copy of the original string, I am passing it as <code>std::string</code>. Only if I want to have it mutable, I am passing it as reference (<code>std::string&amp;</code>). So I just have the choice about what I want to do. I can just do this with every possible type.</p>

<p>In Python or in Java, some types are immutable (mostly all primitive types and strings), others are not.</p>

<p>In pure functional languages like Haskell, everything is immutable.</p>

<p>Is there a good reason why it make sense to have this inconsistency? Or is it just purely for technical lower level reasons?</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><blockquote>
  <p>What is the point of having some
  types immutable and others not?</p>
</blockquote>

<p>Without <em>some</em> mutable types, you'd have to go the whole hog to pure functional programming -- a completely different paradigm than the OOP and procedural approaches which are currently most popular, and, while extremely powerful, apparently very challenging to a lot of programmers (what happens when you <em>do</em> need side effects in a language where nothing is mutable, and in real-world programming of course you inevitably do, is part of the challenge -- Haskell's <a href="http://en.wikipedia.org/wiki/Monad_%28functional_programming%29" rel="nofollow">Monads</a> are a very elegant approach, for example, but how many programmers do you know that fully and confidently understand them and can use them as well as typical OOP constructs?-).</p>

<p>If you don't understand the enormous value of having multiple paradigms available (both FP one <em>and</em> ones crucially relying on mutable data), I recommend studying Haridi's and Van Roy's masterpiece, <a href="http://www.info.ucl.ac.be/~pvr/book.html" rel="nofollow">Concepts, Techniques, and Models of Computer Programming</a> -- "a <a href="http://mitpress.mit.edu/sicp/" rel="nofollow">SICP</a> for the 21st Century", as I once described it;-).</p>

<p>Most programmers, whether familiar with Haridi and Van Roy or not, will readily admit that having at least <strong>some</strong> mutable data types is important to them.  Despite the sentence I've quoted above from your Q, which takes a completely different viewpoint, I believe that may also be the root of your perplexity: <strong>not</strong> "why some of each", but rather "why some <em>immutables</em> at all".</p>

<p>The "thoroughly mutable" approach was once (accidentally) obtained in a Fortran implementation.  If you had, say,</p>

<pre><code>  SUBROUTINE ZAP(I)
  I = 0
  RETURN
</code></pre>

<p>then a program snippet doing, e.g.,</p>

<pre><code>  PRINT 23
  ZAP(23)
  PRINT 23
</code></pre>

<p>would print 23, then 0 -- the <strong>number 23</strong> had been mutated, so all references to 23 in the rest of the program would in fact refer to 0.  Not a bug in the compiler, technically: Fortran had subtle rules about what your program is and is not allowed to do in passing constants vs variables to procedures that assign to their arguments, and this snippet violates those little-known, non-compiler-enforceable rules, so it's a but in the program, not in the compiler.  In practice, of course, the number of bugs caused this way was unacceptably high, so typical compilers soon switched to less destructive behavior in such situations (putting constants in read-only segments to get a runtime error, if the OS supported that; or, passing a fresh <em>copy</em> of the constant rather than the constant itself, despite the overhead; and so forth) even though technically they were program bugs allowing the compiler to display undefined behavior quite "correctly";-).</p>

<p>The alternative enforced in some other languages is to add the complication of multiple ways of parameter passing -- most notably perhaps in C++, what with by-value, by-reference, by constant reference, by pointer, by constant pointer, ... and then of course you see programmers baffled by declarations such as <code>const foo* const bar</code> (where the rightmost <code>const</code> is basically irrelevant if <code>bar</code> is an argument to some function... but crucial instead if <code>bar</code> is a <em>local variable</em>...!-).</p>

<p>Actually Algol-68 probably went farther along this direction (if you can have a value and a reference, why not a reference to a reference? or reference to reference to reference? &amp;c -- Algol 68 put no limitations on this, and the rules to define what was going on are perhaps the subtlest, hardest mix ever found in an "intended for real use" programming language).  Early C (which only had by-value and by-explicit-pointer -- no <code>const</code>, no references, no complications) was no doubt in part a reaction to it, as was the original Pascal.  But <code>const</code> soon crept in, and complications started mounting again.</p>

<p>Java and Python (among other languages) cut through this thicket with a powerful machete of simplicity: all argument passing, <em>and</em> all assignment, is "by object reference" (never reference to a variable or other reference, never semantically implicit copies, &amp;c). Defining (at least) numbers as semantically immutable preserves programmers' sanity (as well as this precious aspect of language simplicity) by avoiding "oopses" such as that exhibited by the Fortran code above.</p>

<p>Treating strings as primitives just like numbers is quite consistent with the languages' intended high semantic level, because in real life we <em>do</em> need strings that are just as simple to use as numbers; alternatives such as defining strings as lists of characters (Haskell) or as arrays of characters (C) poses challenges to both the compiler (keeping efficient performance under such semantics) and the programmer (effectively ignoring this arbitrary structuring to enable use of strings as simple primitives, as real life programming often requires).</p>

<p>Python went a bit further by adding a simple immutable container (<code>tuple</code>) and tying <em>hashing</em> to "effective immutability" (which avoids certain surprises to the programmer that are found, e.g., in Perl, with its hashes allowing mutable strings as keys) -- and why not? Once you have immutability (a precious concept that saves the programmer from having to learn about N different semantics for assignment and argument passing, with N tending to increase with time;-), you might as well get full mileage out of it;-).</p>
<br /><b>#1</b><br /><p>I am not sure if this qualifies as non-technical, nevertheless: if strings are mutable, then most(*) collections need to make private copies of their string keys.</p>

<p>Otherwise a "foo" key changed externally to "bar" would result in "bar" sitting in the internal structures of the collection where "foo" is expected. This way "foo" lookup would find "bar", which is less of a problem (return nothing, reindex the offending key) but "bar" lookup would find nothing, which is a bigger problem.</p>

<p>(*) A dumb collection that does a linear scan of all keys on each lookup would not have to do that, since it would naturally accomodate key changes.</p>
<br /><b>#2</b><br /><p>There is no <em>overarching, fundamental</em> reason not to have strings mutable. The best explanation I have found for their immutability is that it promotes a more functional, less side-effectsy way of programming. This ends up being cleaner, more elegant, and more Pythonic.</p>

<p>Semantically, they should be immutable, no? The string <code>"hello"</code> should always represent <code>"hello"</code>. You can't change it any more than you can change the number three!</p>
<br /><b>#3</b><br /><p>Not sure if you would count this as a 'technical low level' benefit, but the fact that immutable string is implicitly threadsafe saves you a lot of effort of coding for thread safety.  </p>

<p>Slightly toy example... </p>

<p>Thread A - Check user with login name FOO has permission to do something, return true</p>

<p>Thread B - Modify user string to login name BAR</p>

<p>Thread A - Perform some operation with login name BAR due to previous permission check passing against FOO.</p>

<p>The fact that the String can't change saves you the effort of guarding against this.  </p>
<br /><b>#4</b><br /><p>If you want full consistency you can only make <strong>everything immutable</strong>, because mutable Bools or Ints would simply make no sense at all. Some functional languages do that in fact.</p>

<p>Python's philosophy is "Simple is better than complex." In C you need to be aware that strings can change and think about how that can affect you. Python assumes that the default use case for strings is "put text together" - there is absolutely nothing you need to know about strings to do that. But if you <em>want</em> your strings to change, you just have to use a more appropriate type (ie lists, StringIO, templates, etc).</p>
<br /><b>#5</b><br /><p>In a language with reference semantics for user-defined types, having mutable strings would be a desaster, because every time you assign a string variable, you would alias a mutable string object, and you would have to do defensive copies all over the place. That's why strings are immutable in Java and C# -- if the string object is immutable, it does not matter how many variables point to it.</p>

<p>Note that in C++, two string variables never share state (at least conceptionally -- technically, there might be <em>copy-on-write</em> going on, but that is getting out of fashion due to inefficiencies in multi-threading scenarios).</p>
<br /><b>#6</b><br /><p>If strings are mutable, then many consumers of a string will have to to make copies of it.  If strings are immutable, this is far less important (unless immutability is enforced by hardware interlocks, it might not be a bad idea for some security-conscious consumers of a string to make their own copies in case the strings they're given aren't as immutable as they should be).</p>

<p>The StringBuilder class is pretty good, though I think it would be nicer if it had a "Value" property (read would be equivalent to ToString, but it would show up in object inspectors; write would allow direct setting of the whole content) and a default widening conversion to a string.  It would have been nice in theory to have MutableString type descended from a common ancestor with String, so a mutable string could be passed to a function which didn't care whether a string was mutable, though I suspect that optimizations which rely on the fact that Strings have a certain fixed implementation would have been less effective.</p>
<br /><b>#7</b><br /><p>The main advantage for the programmer is that with mutable strings, you never need to worry about who might alter your string. Therefore, you never have to consciously decide "Should I copy this string here?".</p>
<br />