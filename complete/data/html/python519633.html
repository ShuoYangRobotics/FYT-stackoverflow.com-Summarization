<h3>Question (ID-519633):</h3><h2>Lazy Method for Reading Big File in Python?</h2><p>I have a very big file 4GB and when I try to read it my computer hangs.
So I want to read it piece by piece and after processing each piece store the processed piece into another file and read next piece.</p>

<p>Is there any method to <code>yield</code> these pieces ?</p>

<p>I would love to have a <strong>lazy method</strong>.</p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>To write a lazy function, just use <a href="http://docs.python.org/tutorial/classes.html#generators" rel="nofollow"><code>yield</code></a>:</p>

<pre><code>def read_in_chunks(file_object, chunk_size=1024):
    """Lazy function (generator) to read a file piece by piece.
    Default chunk size: 1k."""
    while True:
        data = file_object.read(chunk_size)
        if not data:
            break
        yield data


f = open('really_big_file.dat')
for piece in read_in_chunks(f):
    process_data(piece)
</code></pre>

<p><hr /></p>

<p>Another option would be to use <a href="http://docs.python.org/library/functions.html#iter" rel="nofollow"><code>iter</code></a> and a helper function:</p>

<pre><code>f = open('really_big_file.dat')
def read1k():
    return f.read(1024)

for piece in iter(read1k, ''):
    process_data(piece)
</code></pre>

<p><hr /></p>

<p>If the file is line-based, the file object is already a lazy generator of lines:</p>

<pre><code>for line in open('really_big_file.dat'):
    process_data(line)
</code></pre>
<br /><b>#1</b><br /><p>You can use the <a href="http://docs.python.org/library/mmap.html" rel="nofollow">mmap module</a> to map the contents of the file into memory and access it with indices and slices. Here an example from the documentation:</p>

<pre><code>import mmap
with open("hello.txt", "r+") as f:
    # memory-map the file, size 0 means whole file
    map = mmap.mmap(f.fileno(), 0)
    # read content via standard file methods
    print map.readline()  # prints "Hello Python!"
    # read content via slice notation
    print map[:5]  # prints "Hello"
    # update content using slice notation;
    # note that new content must have same size
    map[6:] = " world!\n"
    # ... and read again using standard file methods
    map.seek(0)
    print map.readline()  # prints "Hello  world!"
    # close the map
    map.close()
</code></pre>
<br /><b>#2</b><br /><p>file.readlines() takes in an optional size argument which approximates the number of lines read in the lines returned.</p>

<pre><code>bigfile = open('bigfilename','r')
tmp_lines = bigfile.readlines(BUF_SIZE)
while tmp_lines:
    process([line for line in tmp_lines])
    tmp_lines = bigfile.readlines(BUF_SIZE)
</code></pre>
<br /><b>#3</b><br /><p>Take a look at <a href="http://neopythonic.blogspot.com/2008/10/sorting-million-32-bit-integers-in-2mb.html">this post on Neopythonic</a>: "Sorting a million 32-bit integers in 2MB of RAM using Python"</p>
<br /><b>#4</b><br /><p>i am not allowed to comment due to my low reputation, but SilentGhosts solution should be much easier with file.readlines([sizehint])</p>

<p><a href="http://docs.python.org/library/stdtypes.html#file-objects" rel="nofollow">python file methods</a></p>

<p>edit: SilentGhost is right, but this should be better than:</p>

<pre><code>s = "" 
for i in xrange(100): 
   s += file.next()
</code></pre>
<br /><b>#5</b><br /><p>I'm in a somewhat similar situation. It's not clear whether you know chunk size in bytes; I usually don't, but the number of records (lines) that is required is known:</p>

<pre><code>def get_line():
     with open('4gb_file') as file:
         for i in file:
             yield i

lines_required = 100
gen = get_line()
chunk = [i for i, j in zip(gen, range(lines_required))]
</code></pre>

<p><strong>Update</strong>: Thanks nosklo. Here's what I meant. It almost works, except that it loses a line 'between' chunks.</p>

<pre><code>chunk = [next(gen) for i in range(lines_required)]
</code></pre>

<p>Does the trick w/o losing any lines, but it doesn't look very nice.</p>
<br />