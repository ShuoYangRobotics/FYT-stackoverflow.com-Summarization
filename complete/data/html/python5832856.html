<h3>Question (ID-5832856):</h3><h2>How to read file N lines at a time in Python?</h2><p>I need to read a big file by reading at most N lines at a time, until EOF. What is the most effective way of doing it in Python? Something like:</p>

<pre><code>with open(filename, 'r') as infile:
    while not EOF:
        lines = [get next N lines]
        process(lines)
</code></pre>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><p>One solution would be a list comprehension and the slice operator:</p>

<pre><code>with open(filename, 'r') as infile:
    lines = [line for line in infile][:N]
</code></pre>

<p>After this <code>lines</code> is tuple of lines. However, this would load the complete file into memory. If you don't want this (i.e. if the file could be really large) there is another solution using a generator expression and <a href="http://docs.python.org/library/itertools.html#itertools.islice" rel="nofollow"><code>islice</code></a> from the itertools package:</p>

<pre><code>from itertools import islice
with open(filename, 'r') as infile:
    lines_gen = islice(infile, N)
</code></pre>

<p><code>lines_gen</code> is a generator object, that gives you each line of the file and can be used in a loop like this:</p>

<pre><code>for line in lines_gen:
    print line
</code></pre>

<p>Both solutions give you up to N lines (or fewer, if the file doesn't have that much).</p>
<br /><b>#1</b><br /><p>This code will work with any count of lines in file and any <code>N</code>. If you have <code>1100 lines</code> in file and <code>N = 200</code>, you will get 5 times to process chunks of 200 lines and one time with 100 lines.</p>

<pre><code>with open(filename, 'r') as infile:
    lines = []
    for line in infile:
        lines.append(line)
        if len(lines) &gt; N:
            process(lines)
            lines = []
    if len(lines) &gt; 0:
        process(lines)
</code></pre>
<br /><b>#2</b><br /><p>I think you should be using chunks instead of specifying the number of lines to read. It makes your code more robust and generic. Even if the lines are big, using chunk will upload only the assigned amount of data into memory.</p>

<p>Refer to <a href="http://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python">this</a> link</p>
<br /><b>#3</b><br /><p>File object is an iterator over lines in Python. So you could use a general <a href="http://docs.python.org/library/itertools.html#recipes" rel="nofollow">itertools recipe</a> (see <a href="http://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks/434411#434411">What is the most “pythonic” way to iterate over a list in chunks?</a>):</p>

<pre><code>from itertools import izip_longest

def grouper(iterable, n, fillvalue=None):
    args = [iter(iterable)] * n
    return izip_longest(*args, fillvalue=fillvalue)
</code></pre>

<h3>Example</h3>

<pre><code>with open(filename) as f:
     for lines in grouper(f, N, ''):
         assert len(lines) == N
         # process lines here
</code></pre>
<br /><b>#4</b><br /><p>How about a for loop?</p>

<pre><code>with open(filename, 'r') as infile:
    while not EOF:
        lines = []
        for i in range(next N lines):
            lines.append(infile.readline())
        process(lines)
</code></pre>
<br /><b>#5</b><br /><p>maybe:  </p>

<pre><code>for x in range(N):
  lines.append(f.readline())
</code></pre>
<br /><b>#6</b><br /><p>You may have to do something as simple as:</p>

<pre><code>lines = [infile.readline() for _ in range(N)]
</code></pre>

<p><strong>Update</strong>  after comments:</p>

<pre><code>lines = [line for line in [infile.readline() for _ in range(N)] if len(line) ]
</code></pre>
<br />