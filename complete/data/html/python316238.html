<h3>Question (ID-316238):</h3><h2>python float to Decimal conversion</h2><p>Python Decimal doesn't support being constructed from float, it expects that you have to convert float to a string first.  </p>

<p>This is very inconvenient since standard string formattors for float require that you specify number of decimal places rather than significant places.  So if you have a number that could have as many as 15 decimal places you need to format as Decimal(""%.15f"% my_float), which will give you garbage at the 15th decimal place if you also have any significant digits before decimal.</p>

<p>Can someone suggest a good way to convert from float to Decimal preserving value as the user has entered, perhaps limiting number of significant digits that can be supported.</p>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><p>You said in your question: </p>

<blockquote>
  <p>Can someone suggest a good way to
  convert from float to Decimal
  <strong>preserving value as the user has
  entered</strong></p>
</blockquote>

<p>But every time the user enters a value, it is entered as a string, not as a float. You are converting it to a float somewhere. Convert it to a Decimal directly instead and no precision will be lost.</p>
<br /><b>#1</b><br /><h3>Python &lt;2.7</h3>

<pre><code>"%.15g" % f
</code></pre>

<p>Or in Python 3.0:</p>

<pre><code>format(f, ".15g")
</code></pre>

<h3>Python 2.7+, 3.2+</h3>

<p>Just pass the float to <code>Decimal</code> constructor directly.</p>
<br /><b>#2</b><br /><p>When you say "preserving value as the user has entered", why not just store the user-entered value as a string, and pass that to the Decimal constructor?</p>
<br /><b>#3</b><br /><p>Python does support Decimal creation from a float. You just cast it as a string first. But the precision loss doesn't occur with string conversion. The float you are converting doesn't have that kind of precision in the first place. (Otherwise you wouldn't need Decimal)</p>

<p>I think the confusion here is that <em>we can create float literals in decimal format</em>, but as soon as the interpreter consumes that literal the inner representation becomes a floating point number.</p>
<br /><b>#4</b><br /><p>The "official" string representation of a float is given by the repr() built-in:</p>

<pre><code>&gt;&gt;&gt; repr(1.5)
'1.5'
&gt;&gt;&gt; repr(12345.678901234567890123456789)
'12345.678901234567'
</code></pre>

<p>You can use repr() instead of a formatted string, the result won't contain any unnecessary garbage.</p>
<br /><b>#5</b><br /><p>The "right" way to do this was documented in 1990 by <a href="http://portal.acm.org/citation.cfm?id=93542&amp;type=proceeding" rel="nofollow">Steele and White's and
Clinger's PLDI 1990</a> papers.</p>

<p>You might also look at <a href="http://stackoverflow.com/questions/286061/python-decimal-place-issues-with-floats">this</a> SO discussion about Python Decimal, including my suggestion to try using something like <a href="http://www.ics.uci.edu/~eppstein/numth/frap.c" rel="nofollow">frap</a> to rationalize a float.</p>
<br /><b>#6</b><br /><p>I suggest this</p>

<pre><code>&gt;&gt;&gt; a = 2.111111
&gt;&gt;&gt; a
2.1111110000000002
&gt;&gt;&gt; str(a)
'2.111111'
&gt;&gt;&gt; decimal.Decimal(str(a))
Decimal('2.111111')
</code></pre>
<br />