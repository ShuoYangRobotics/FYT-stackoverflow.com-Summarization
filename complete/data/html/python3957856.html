<h3>Question (ID-3957856):</h3><h2>Determine if a Python list is 95% the same?</h2><p><a href="http://stackoverflow.com/questions/3787908/python-determine-if-all-items-of-a-list-are-the-same-item">This question</a> asks how to determine if every element in a list is the same.  How would I go about determining if 95% of the elements in a list are the same in a reasonably efficient way?  For example:</p>

<pre><code>&gt;&gt;&gt; ninety_five_same([1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])
True
&gt;&gt;&gt; ninety_five_same([1,1,1,1,1,1,2,1]) # only 80% the same
False
</code></pre>

<p>This would need to be somewhat efficient because the lists may be very large.</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><pre><code>&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; lst = [1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
&gt;&gt;&gt; _, freq = Counter(lst).most_common(1)[0]
&gt;&gt;&gt; len(lst)*.95 &lt;= freq
True
</code></pre>
<br /><b>#1</b><br /><p>Actually, there's an easy linear solution for similar problem, only with 50% constraint instead of 95%. <a href="http://stackoverflow.com/questions/3740371/finding-the-max-repeated-element-in-an-array">Check this question</a>, it's just a few lines of code.  </p>

<p>It will work for you as well, only in the end you check that selected element satisfies 95% threshold, not 50%. (Although, as <strong>Thilo</strong> notes, it's not necessary if <code>currentCount &gt;= n*0.95</code> already.)</p>

<p>I'll also post Python code from <strong>st0le</strong>'s answer, to show everybody how difficult it is.</p>

<pre><code>currentCount = 0
currentValue = lst[0]
for val in lst:
   if val == currentValue:
      currentCount += 1
   else:
      currentCount -= 1

   if currentCount == 0:
      currentValue = val
      currentCount = 1
</code></pre>

<p>If you're looking for explanation, I think <strong>Nabb</strong> has got <a href="http://stackoverflow.com/questions/3740371/finding-the-max-repeated-element-in-an-array/3740548#3740548">the best one</a>.</p>
<br /><b>#2</b><br /><pre><code>def ninety_five_same(lst):
    freq = collections.defaultdict(int)
    for x in lst:
        freq[x] += 1
    freqsort = sorted(freq.itervalues())
    return freqsort[-1] &gt;= .95 * sum(freqsort)
</code></pre>

<p>Assuming perfect hash table performance and a good sorting algorithm, this runs in O(<em>n</em> + <em>m</em> lg <em>m</em>), where <em>m</em> is the number of distinct items. O(<em>n</em> lg <em>n</em>) worst case.</p>

<p><strong>Edit</strong>: here's an O(<em>n</em> + <em>m</em>), single-pass version (assuming <em>m</em> &lt;&lt; <em>n</em>):</p>

<pre><code>def ninety_five_same(lst):
    freq = collections.defaultdict(int)
    for x in lst:
        freq[x] += 1
    freq = freq.values()
    return max(freq) &gt;= .95 * sum(freq)
</code></pre>

<p>Memory use is O(<em>m</em>). <code>max</code> and <code>sum</code> can be replaced by a single loop.</p>
<br /><b>#3</b><br /><p>This is even less efficient than checking if every element is the same.</p>

<p>The algorithm is roughly the same, go through every element in the list and count those that do not match the expected one (with the extra difficulty of knowing which one is the expected one). However, this time, you cannot just return false when you meet the first mismatch, you have to continue until you have enough mismatches to make up a 5% error rate.</p>

<p>Come to think of it, figuring out which element is the "right" one is probably not so easy, and involves counting every value up to the point where you can be sure that 5% are misplaced.</p>

<p>Consider a list with 10.000 elements of which 99% are 42: </p>

<pre><code>  (1,2,3,4,5,6,7,8,9,10, ... , 100, 42,42, 42, 42 .... 42)
</code></pre>

<p>So I think you would have to start out building a frequency table for at least the first 5% of the table.</p>
<br /><b>#4</b><br /><pre><code>def ninety_five_same(l):
  return max([l.count(i) for i in set(l)])*20 &gt;= 19*len(l)
</code></pre>

<p>Also eliminating the problem with with accuracy of float division.</p>
<br /><b>#5</b><br /><p>Think about your list as a bucket of red and black balls. </p>

<p>If you have one red ball in a bucket of ten balls, and you pick a ball at random and put it back in the bucket, and then repeat that sample-and-replace step a thousand times, how many times out of a thousand do you expect to observe a red ball, on average?</p>

<p>Check out the <a href="http://en.wikipedia.org/wiki/Binomial_distribution" rel="nofollow">Binomial</a> distribution and check out <a href="http://en.wikipedia.org/wiki/Normal_distribution#Standard_deviation_and_confidence_intervals" rel="nofollow">confidence intervals</a>. If you have a very long list and want to do things relatively efficiently, sampling is the way to go.</p>
<br /><b>#6</b><br /><pre><code>lst = [1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
#lst = [1, 2, 1, 4, 1]
#lst = [1, 2, 1, 4]

length = len(lst)
currentValue = lst[0]
lst.pop(0)
currentCount = 1

for val in lst:
   if currentCount == 0:
      currentValue = val

   if val == currentValue:
      currentCount += 1
   else:
      currentCount -= 1

percent = (currentCount * 50.0 / length + 50)
epsilon = 0.1
if (percent - 50 &gt; epsilon):
    print "Percent %g%%" % percent
else:
    print "No majority"
</code></pre>

<p>Note: epsilon has a "random" value, chose something depending on the length of the array etc.
Nikita Rybak's solution with <code>currentCount &gt;= n*0.95</code> won't work, because the value of currentCount differs depending on the order of elements, but <strong>the above does work</strong>.</p>

<pre><code>C:\Temp&gt;a.py
[2, 1, 1, 4, 1]
currentCount = 1

C:\Temp&gt;a.py
[1, 2, 1, 4, 1]
currentCount = 2
</code></pre>
<br /><b>#7</b><br /><p>sort as general solution probably is heavy, but consider the exceptional well balanced nature of tim sort in Python, which utilize the existing order of the list. I would suggest to sort the list (or copy of it with sorted, but that copy will hurt the performance). Scan from end and front to find the same element or reach scan length > 5 %, otherwise list is 95% similar with the element found.</p>

<p>Taking random elements as candidates and taking count of them by decreasing order of frequency would not probably also be so bad until found count > 95 % or the total of counts goes over 5%.</p>
<br />