<h3>Question (ID-1477569):</h3><h2>Python: time a method call and stop it if time is exceeded</h2><p>I need to dynamically load code (comes as source), run it and get the results. The code that I load always includes a run method, which returns the needed results. Everything looks ridiculously easy, as usual in Python, since I can do</p>

<pre><code>exec(source) #source includes run() definition
result = run(params)
#do stuff with result
</code></pre>

<p>The only problem is, the run() method in the dynamically generated code can potentially not terminate, so I need to only run it for up to x seconds. I could spawn a new thread for this, and specify a time for .join() method, but then I cannot easily get the result out of it (or can I). Performance is also an issue to consider, since all of this is happening in a long while loop</p>

<p>Any suggestions on how to proceed?</p>

<p>Edit: to clear things up per dcrosta's request: the loaded code is not untrusted, but generated automatically on the machine. The purpose for this is genetic programming.</p>
<br /><h3>Answers (Total-6):</h3><b>#0</b><br /><p>You could also use Stackless Python, as it allows for <a href="http://www.stackless.com/wiki/Scheduling" rel="nofollow">cooperative scheduling</a> of microthreads. Here you can specify a maximum number of instructions to execute before returning. Setting up the routines and getting the return value out is a little more tricky though.</p>
<br /><b>#1</b><br /><p>The only "really good" solutions -- imposing essentially no overhead -- are going to be based on SIGALRM, either directly or through a nice abstraction layer; but as already remarked Windows does not support this. Threads are no use, not because it's hard to get results out (that would be trivial, with a Queue!), but because forcibly terminating a runaway thread in a nice cross-platform way is unfeasible.</p>

<p>This leaves high-overhead <code>multiprocessing</code> as the only viable <strong>cross-platform</strong> solution. You'll want a process pool to reduce process-spawning overhead (since presumably the need to kill a runaway function is only occasional, most of the time you'll be able to reuse an existing process by sending it new functions to execute). Again, Queue (the multiprocessing kind) makes getting results back easy (albeit with a modicum more caution than for the threading case, since in the multiprocessing case deadlocks are possible).</p>

<p>If you don't need to strictly serialize the executions of your functions, but rather can arrange your architecture to try two or more of them in parallel, AND are running on a multi-core machine (or multiple machines on a fast LAN), then suddenly multiprocessing becomes a high-performance solution, easily paying back for the spawning and IPC overhead and more, exactly because you can exploit as many processors (or nodes in a cluster) as you can use.</p>
<br /><b>#2</b><br /><p>You could use the <a href="http://docs.python.org/library/multiprocessing.html" rel="nofollow">multiprocessing</a> library to run the code in a separate process, and call .join() on the process to wait for it to finish, with the timeout parameter set to whatever you want. The library provides several ways of getting data back from another process - using a Value object (seen in the Shared Memory example on that page) is probably sufficient. You can use the terminate() call on the process if you really need to, though it's not recommended.</p>
<br /><b>#3</b><br /><p>Executing untrusted code is dangerous, and should usually be avoided unless it's impossible to do so. I think you're right to be worried about the time of the run() method, but the run() method could do other things as well: delete all your files, open sockets and make network connections, begin cracking your password and email the result back to an attacker, etc.</p>

<p>Perhaps if you can give some more detail on what the dynamically loaded code does, the SO community can help suggest alternatives.</p>
<br /><b>#4</b><br /><p>a quick google for "python timeout" reveals a <a href="http://nick.vargish.org/clues/python-tricks.html" rel="nofollow">TimeoutFunction</a> class</p>
<br /><b>#5</b><br /><blockquote>
  <p>I could spawn a new thread for this, and specify a time for .join() method, but then I cannot easily get the result out of it </p>
</blockquote>

<p>If the timeout expires, that means the method didn't finish, so there's no result to get.  If you have incremental results, you can store them somewhere and read them out however you like (keeping threadsafety in mind).</p>

<p>Using SIGALRM-based systems is dicey, because it can deliver async signals at any time, even during an except or finally handler where you're not expecting one.  (Other languages deal with this better, unfortunately.)  For example:</p>

<pre><code>try:
    # code
finally:
    cleanup1()
    cleanup2()
    cleanup3()
</code></pre>

<p>A signal passed up via SIGALRM might happen during cleanup2(), which would cause cleanup3() to never be executed.  Python simply does not have a way to terminate a running thread in a way that's both uncooperative and safe.</p>

<p>You should just have the code check the timeout on its own.</p>

<pre><code>import threading
from datetime import datetime, timedelta

local = threading.local()
class ExecutionTimeout(Exception): pass

def start(max_duration = timedelta(seconds=1)):
    local.start_time = datetime.now()
    local.max_duration = max_duration

def check():
    if datetime.now() - local.start_time &gt; local.max_duration:
        raise ExecutionTimeout()

def do_work():
    start()
    while True:
        check()
        # do stuff here
    return 10

try:
    print do_work()
except ExecutionTimeout:
    print "Timed out"
</code></pre>

<p>(Of course, this belongs in a module, so the code would actually look like "timeout.start()"; "timeout.check()".)</p>

<p>If you're generating code dynamically, then generate a timeout.check() call at the start of each loop.</p>
<br />