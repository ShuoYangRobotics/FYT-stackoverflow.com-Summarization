<h3>Question (ID-5002783):</h3><h2>Best Python clustering library to use for product data analysis</h2><p>I have a collection of alphanumeric product codes of various products. Similar products have no intrinsic similarity in their codes, ie product code "A123" might mean "Harry Potter Volume 1 DVD" and "B123" might mean "Kellogs Corn Flakes". I also do not actually have the description or identify of the product. All I have is an "owner" of this code. My data, therefore,  looks (in a non-normal way) something like this:</p>

<p>Owner1: ProductCodes A123,B124,W555,M221,M556,127,102</p>

<p>Owner2: ProductCode D103,Z552,K112,L3254,223,112</p>

<p>Owner3: ProductCode G123</p>

<p>....</p>

<p>I have huge (ie Terabytes) sets of this data.</p>

<p>I assume that an owner would - for the majority - have an undetermined number of groups of similar products - ie an owner might have just 2 groups - all the DVDs and books of Harry Potter, but also a collection of  "Iron Maiden" cds. I would like to analyse this data and determine distance functions between product codes so I can start making assumptions about "how close" product codes are to each other and also cluster product codes (so I can also identify how many groups an owner has). I have started doing some research on textual clustering algorithms but there are numerous ones to choose from and I'm not sure on which one(s) work best with this scenario.</p>

<p>Can someone point me towards the most appropriate python based clustering functions / libraries to use please ?!</p>
<br /><h3>Answers (Total-7):</h3><b>#0</b><br /><p>What you have is a bipartite graph. As an initial stab, it sounds like you are going to treat neighbour lists as zero-one vectors between which you define some kind of similarity/correlation. This could be a normalised Hamming distance for example. Depending on which way you do that you will obtain a graph on a single domain -- either product codes or owners. It will shortly become clear why I've cast everything in the language of graphs, bear with me. Now why do you insist on a Python implementation? Clustering large scale data is time and memory consuming. To pull the cat out of the bag, I have written and still maintain a graph clustering algorithm, used quite widely in bioinformatics. Is is threaded, accepts weighted graphs, and has been used for graphs with millions of nodes and towards a billion of edges. Refer to <a href="http://micans.org/mcl/" rel="nofollow">http://micans.org/mcl/</a> for more information. Of course, if you trawl stackoverflow and stackexchange there is quite a few threads that may be of interest to you. I would recommend the Louvain method as well, except that I am not sure whether it accepts weighted networks, which you will probably produce.</p>
<br /><b>#1</b><br /><p>R language <a href="http://cran.r-project.org/web/views/Cluster.html" rel="nofollow">has many packages for finding groups in data</a>, and there are python bindings to R, called <a href="http://rpy.sourceforge.net/" rel="nofollow">RPy</a>. R provides several algorithms already mentioned here and also known for good performance on large datasets. </p>
<br /><b>#2</b><br /><p>I think you can use pycluster also change algorithm for your problem</p>

<ul>
<li><a href="http://bonsai.hgc.jp/~mdehoon/software/cluster/software.htm" rel="nofollow">http://bonsai.hgc.jp/~mdehoon/software/cluster/software.htm</a></li>
<li><a href="http://cran.r-project.org/web/views/Cluster.html" rel="nofollow">http://cran.r-project.org/web/views/Cluster.html</a></li>
</ul>

<p>also i think you better see this <a href="http://www.dennogumi.org/2007/11/data-clustering-with-python" rel="nofollow">http://www.dennogumi.org/2007/11/data-clustering-with-python</a></p>
<br /><b>#3</b><br /><p>I don't know much about your problem domain. But PyCluster is pretty decent clustering package which works good on large datasets:
<a href="http://bonsai.hgc.jp/~mdehoon/software/cluster/software.htm" rel="nofollow">http://bonsai.hgc.jp/~mdehoon/software/cluster/software.htm</a></p>

<p>Hope it helps.</p>
<br /><b>#4</b><br /><p>I don't know of an off-the-shelf lib, sorry. There are big libs for full-text search and similarity,
but for bit sets you'll have to roll your own (as far as i know).
A couple  of suggestions anyway:</p>

<ul>
<li><p>bitset approach: first get say 10k owners x 100k products, or 100k x 10k, in memory, to play with.
You could use <a href="http://stackoverflow.com/questions/3027925/best-way-in-python-to-determine-all-possible-intersections-in-a-matrix">bitarray</a> to make a big array of 10k x 100k bits.
But then, what do you want to do with it ?<br>
To find similar pairs among N objects (either owners or products),
you have to look at all N*(N-1)/2 pairs, which is a lot;<br>
or, there must be some structure in tha data that allows early pruning / hierarchical similarity;<br>
or, google "greedy clustering" Python &mdash; don't see an  off-the-shelf lib.</p></li>
<li><p>how do you define "similarity" of owners / of products ? There are lots of possibilities &mdash; number in common, ratio in common, <a href="http://en.wikipedia.org/wiki/TF_IDF" rel="nofollow">tf-idf</a> ...</p></li>
</ul>

<p>(Added): have you looked at <a href="http://mahout.apache.org" rel="nofollow">Mahout</a>'s recommendation system API,
is that about what you're looking for ?<br>
<a href="http://stackoverflow.com/questions/4819437/javas-mahout-equivalent-in-python">This</a> SO question
says there's no Python equivalent, which leaves two choices:<br>
a) ask if anyone has used Mahout from Jython,
or b) if you can't lick 'em, join 'em.</p>
<br /><b>#5</b><br /><p>You can try to do clustering using the <a href="http://en.wikipedia.org/wiki/K-means_clustering" rel="nofollow">k-means clustering algorithm</a> and its scipy implementation available in <a href="http://scikit-learn.sourceforge.net/modules/generated/scikits.learn.cluster.KMeans.html" rel="nofollow">scikits.learn.cluster.KMeans</a>.</p>
<br /><b>#6</b><br /><p>Try <a href="http://ask.github.com/celery/getting-started/introduction.html" rel="nofollow">Celery</a>, it's a distributed job queue system.  You can put all your task in the queue, and add work nodes as much as you want.</p>

<p><img src="http://i.stack.imgur.com/51UjA.jpg" alt="enter image description here"></p>

<p>It should fit your needs.</p>
<br />