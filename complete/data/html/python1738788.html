<h3>Question (ID-1738788):</h3><h2>Python: Split unicode string on word boundaries</h2><p>I need to take a string, and shorten it to 140 characters.</p>

<p>Currently I am doing:</p>

<pre><code>if len(tweet) &gt; 140:
    tweet = re.sub(r"\s+", " ", tweet) #normalize space
    footer = "… " + utils.shorten_urls(post['url'])
    avail = 140 - len(footer)
    words = tweet.split()
    result = ""
    for word in words:
        word += " "
        if len(word) &gt; avail:
            break
        result += word
        avail -= len(word)
    tweet = (result + footer).strip()
    assert len(tweet) &lt;= 140
</code></pre>

<p>So this works great for English, and English like strings, but fails for a Chinese string because <code>tweet.split()</code> just returns one array:</p>

<pre><code>&gt;&gt;&gt; s = u"简讯：新華社報道，美國總統奧巴馬乘坐的「空軍一號」專機晚上10時42分進入上海空域，預計約30分鐘後抵達浦東國際機場，開展他上任後首次訪華之旅。"
&gt;&gt;&gt; s
u'\u7b80\u8baf\uff1a\u65b0\u83ef\u793e\u5831\u9053\uff0c\u7f8e\u570b\u7e3d\u7d71\u5967\u5df4\u99ac\u4e58\u5750\u7684\u300c\u7a7a\u8ecd\u4e00\u865f\u300d\u5c08\u6a5f\u665a\u4e0a10\u664242\u5206\u9032\u5165\u4e0a\u6d77\u7a7a\u57df\uff0c\u9810\u8a08\u7d0430\u5206\u9418\u5f8c\u62b5\u9054\u6d66\u6771\u570b\u969b\u6a5f\u5834\uff0c\u958b\u5c55\u4ed6\u4e0a\u4efb\u5f8c\u9996\u6b21\u8a2a\u83ef\u4e4b\u65c5\u3002'
&gt;&gt;&gt; s.split()
[u'\u7b80\u8baf\uff1a\u65b0\u83ef\u793e\u5831\u9053\uff0c\u7f8e\u570b\u7e3d\u7d71\u5967\u5df4\u99ac\u4e58\u5750\u7684\u300c\u7a7a\u8ecd\u4e00\u865f\u300d\u5c08\u6a5f\u665a\u4e0a10\u664242\u5206\u9032\u5165\u4e0a\u6d77\u7a7a\u57df\uff0c\u9810\u8a08\u7d0430\u5206\u9418\u5f8c\u62b5\u9054\u6d66\u6771\u570b\u969b\u6a5f\u5834\uff0c\u958b\u5c55\u4ed6\u4e0a\u4efb\u5f8c\u9996\u6b21\u8a2a\u83ef\u4e4b\u65c5\u3002']
</code></pre>

<p>How should I do this so it handles I18N? Does this make sense in all languages?</p>

<p>I'm on python 2.5.4 if that matters.</p>
<br /><h3>Answers (Total-8):</h3><b>#0</b><br /><p>For word segmentation in Chinese, and other advanced tasks in processing natural language, consider <a href="http://nltk.googlecode.com/svn/trunk/doc/book/ch03.html#word-segmentation" rel="nofollow">NLTK</a> as a good starting point if not a complete solution -- it's a rich Python-based toolkit, particularly good for learning about NL processing techniques (and not rarely good enough to offer you viable solution to some of these problems).</p>
<br /><b>#1</b><br /><p>Chinese doesn't usually have whitespace between words, and the symbols can have different meanings depending on context. You will have to understand the text in order to split it at a word boundary. In other words, what you are trying to do is not easy in general.</p>
<br /><b>#2</b><br /><p>the <a href="http://docs.python.org/library/re.html#re.U" rel="nofollow"><code>re.U</code> flag</a> will treat <code>\s</code> according to the Unicode character properties database.</p>

<p>The given string, however, doesn't apparently contain any white space characters according to python's unicode database:</p>

<pre><code>&gt;&gt;&gt; x = u'\u7b80\u8baf\uff1a\u65b0\u83ef\u793e\u5831\u9053\uff0c\u7f8e\u570b\u7e3d\u7d71\u5967\u5df4\u99ac\u4e58\u5750\u7684\u300c\u7a7a\u8ecd\u4e00\u865f\u300d\u5c08\u6a5f\u665a\u4e0a10\u664242\u5206\u9032\u5165\u4e0a\u6d77\u7a7a\u57df\uff0c\u9810\u8a08\u7d0430\u5206\u9418\u5f8c\u62b5\u9054\u6d66\u6771\u570b\u969b\u6a5f\u5834\uff0c\u958b\u5c55\u4ed6\u4e0a\u4efb\u5f8c\u9996\u6b21\u8a2a\u83ef\u4e4b\u65c5\u3002'
&gt;&gt;&gt; re.compile(r'\s+', re.U).split(x)
[u'\u7b80\u8baf\uff1a\u65b0\u83ef\u793e\u5831\u9053\uff0c\u7f8e\u570b\u7e3d\u7d71\u5967\u5df4\u99ac\u4e58\u5750\u7684\u300c\u7a7a\u8ecd\u4e00\u865f\u300d\u5c08\u6a5f\u665a\u4e0a10\u664242\u5206\u9032\u5165\u4e0a\u6d77\u7a7a\u57df\uff0c\u9810\u8a08\u7d0430\u5206\u9418\u5f8c\u62b5\u9054\u6d66\u6771\u570b\u969b\u6a5f\u5834\uff0c\u958b\u5c55\u4ed6\u4e0a\u4efb\u5f8c\u9996\u6b21\u8a2a\u83ef\u4e4b\u65c5\u3002']
</code></pre>
<br /><b>#3</b><br /><p>This punts the word-breaking decision to the re module, but it may work well enough for you.</p>

<pre><code>import re

def shorten(tweet, footer="", limit=140):
    """Break tweet into two pieces at roughly the last word break
    before limit.
    """
    lower_break_limit = limit / 2
    # limit under which to assume breaking didn't work as expected

    limit -= len(footer)

    tweet = re.sub(r"\s+", " ", tweet.strip())
    m = re.match(r"^(.{,%d})\b(?:\W|$)" % limit, tweet, re.UNICODE)
    if not m or m.end(1) &lt; lower_break_limit:
        # no suitable word break found
        # cutting at an arbitrary location,
        # or if len(tweet) &lt; lower_break_limit, this will be true and
        # returning this still gives the desired result
        return tweet[:limit] + footer
    return m.group(1) + footer
</code></pre>
<br /><b>#4</b><br /><p>After speaking with some native Cantonese, Mandarin, and Japanese speakers it seems that the correct thing to do is hard, but my current algorithm still makes sense to them in the context of internet posts. </p>

<p>Meaning, they are used to the "split on space and add … at the end" treatment.</p>

<p>So I'm going to be lazy and stick with it, until I get complaints from people that don't understand it.</p>

<p>The only change to my original implementation would be to not force a space on the last word since it is unneeded in any language (and use the unicode character … <code>&amp;#x2026</code> instead of ... <code>three dots</code> to save 2 characters)</p>
<br /><b>#5</b><br /><p>I tried out the solution with PyAPNS for push notifications and just wanted to share what worked for me.  The issue I had is that truncating at 256 bytes in UTF-8 would result in the notification getting dropped.  I had to make sure the notification was encoded as "unicode_escape" to get it to work.  I'm assuming this is because the result is sent as JSON and not raw UTF-8.  Anyways here is the function that worked for me:</p>

<pre><code>def unicode_truncate(s, length, encoding='unicode_escape'):
    encoded = s.encode(encoding)[:length]
    return encoded.decode(encoding, 'ignore')
</code></pre>
<br /><b>#6</b><br /><p>Basically, in CJK (Except Korean with spaces), you need dictionary look-ups to segment words properly.  Depending on your exact definition of "word", Japanese can be more difficult than that, since not all inflected variants of a word (i.e. "行こう" vs. "行った") will appear in the dictionary.  Whether it's worth the effort depends upon your application.</p>
<br /><b>#7</b><br /><p>Save two characters and use an elipsis (<code>…</code>, <a href="http://www.fileformat.info/info/unicode/char/2026/index.htm" rel="nofollow">0x2026</a>) instead of three dots!</p>
<br />