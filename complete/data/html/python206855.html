<h3>Question (ID-206855):</h3><h2>Scrape a dynamic website</h2><p>What is the best method to scrape a dynamic website where most of the content is generated by what appears to be ajax requests?  I have previous experience with a Mechanize, BeautifulSoup, and python combo, but I am up for something new.</p>

<p>--Edit--
For more detail: I'm trying to scrape the CNN <a href="http://www.cnn.com/ELECTION/2008/primaries/results/state/" rel="nofollow" title="primary database">primary database</a>.  There is a wealth of information there, but there doesn't appear to be an api.</p>
<br /><h3>Answers (Total-9):</h3><b>#0</b><br /><p>This is a difficult problem because you either have to reverse engineer the javascript on a per-site basis, or implement a javascript engine and run the scripts (which has its own difficulties and pitfalls).</p>

<p>It's a heavy weight solution, but I've seen people doing this with greasemonkey scripts - allow Firefox to render everything and run the javascript, and then scrape the elements.  You can even initiate user actions on the page if needed.</p>


<br /><b>#1</b><br /><p>The best solution that I found was to use Firebug to monitor XmlHttpRequests, and then to use a script to resend them.</p>
<br /><b>#2</b><br /><p>Adam Davis's advice is solid.</p>

<p>I would additionally suggest that you try to "reverse-engineer" what the JavaScript is doing, and instead of trying to scrape the page, you issue the HTTP requests that the JavaScript is issuing and interpret the results yourself (most likely in JSON format, nice and easy to parse).  This strategy could be anything from trivial to a total nightmare, depending on the complexity of the JavaScript.</p>

<p>The best possibility, of course, would be to convince the website's maintainers to implement a developer-friendly API.  All the cool kids are doing it these days 8-)  Of course, they might not  want their data scraped in an automated fashion... in which case you can expect a cat-and-mouse game of making their page increasingly difficult to scrape :-(</p>
<br /><b>#3</b><br /><p>Selenium IDE, a tool for testing, is something I've used for a lot of screen-scraping. There are a few things it doesn't handle well (Javascript window.alert() and popup windows in general), but it does its work on a page by actually triggering the click events and typing into the text boxes. Because the IDE portion runs in Firefox, you don't have to do all of the management of sessions, etc. as Firefox takes care of it. The IDE records and plays tests back.</p>

<p>It also exports C#, PHP, Java, etc. code to build compiled tests/scrapers that are executed on the Selenium server. I've done that for more than a few of my Selenium scripts, which makes things like storing the scraped data in a database much easier.</p>

<p>Scripts are fairly simple to write and alter, being made up of things like ("clickAndWait","submitButton"). Worth a look given what you're describing.</p>
<br /><b>#4</b><br /><p>There is a bit of a learning curve, but tools like Pamie (Python) or Watir (Ruby) will let you latch into the IE web browser and get at the elements. This turns out to be easier than Mechanize and other HTTP level tools since you don't have to emulate the browser, you just ask the browser for the html elements. And it's going to be way easier than reverse engineering the Javascript/Ajax calls. If needed you can also use tools like beatiful soup in conjunction with Pamie.</p>
<br /><b>#5</b><br /><p>i found the IE Webbrowser control have all kinds of quirks and workarounds that would justify some high quality software to take care of all those inconsistencies, layered around the shvwdoc.dll api and mshtml and provide a framework. </p>
<br /><b>#6</b><br /><p>This seems like it's a pretty common problem.  I wonder why someone hasn't anyone developed a programmatic browser?  I'm envisioning a Firefox you can call from the command line with a URL as an argument and it will load the page, run all of the initial page load JS events and save the resulting file.</p>

<p>I mean Firefox, and other browsers already do this, why can't we simply strip off the UI stuff?  </p>
<br /><b>#7</b><br /><p>Probably the easiest way is to use IE webbrowser control in C# (or any other language). You have access to all the stuff inside browser out of the box + you dont need to care about cookies, SSL and so on.</p>
<br /><b>#8</b><br /><p>The first question I have: can you confirm that the site absolutely doesn't work without Javascript? You'd be surprised how many sites are still usable if you turn javascript off. You'll probably be directed to a slightly different set of urls. </p>
<br />