<h3>Question (ID-964459):</h3><h2>how to remove text between <script> and </script> using python?</h2><p>how to remove text between <code>&lt;script&gt;</code> and <code>&lt;/script&gt;</code> using python?</p>
<br /><h3>Answers (Total-9):</h3><b>#0</b><br /><p>You can use <a href="http://www.crummy.com/software/BeautifulSoup/" rel="nofollow">BeautifulSoup</a> with this (and other) methods:</p>

<pre><code>soup = BeautifulSoup(source.lower())
to_extract = soup.findAll('script')
for item in to_extract:
    item.extract()
</code></pre>

<p>This actually removes the nodes from the HTML.  If you wanted to leave the empty <code>&lt;script&gt;&lt;/script&gt;</code> tags you'll have to work with the <code>item</code> attributes rather than just extracting it from the soup.</p>
<br /><b>#1</b><br /><p>Are you trying to prevent <a href="http://en.wikipedia.org/wiki/Cross-site%5Fscripting" rel="nofollow">XSS</a>? Just eliminating the <code>&lt;script&gt;</code> tags will not solve all possible attacks! Here's a great list of the many ways (some of them very creative) that you could be vulnerable <a href="http://ha.ckers.org/xss.html" rel="nofollow">http://ha.ckers.org/xss.html</a>. After reading this page you should understand why just elimintating the <code>&lt;script&gt;</code> tags using a regular expression is not robust enough. The python library <a href="http://codespeak.net/lxml/lxmlhtml.html#cleaning-up-html" rel="nofollow">lxml</a> has a function that will robustly clean your HTML to make it safe to display.</p>

<p><strong>If</strong> you are sure that you just want to eliminate the <code>&lt;script&gt;</code> tags this code in lxml should work:</p>

<pre><code>from lxml.html import parse

root = parse(filename_or_url).getroot()
for element in root.iter("script"):
    element.drop_tree()
</code></pre>

<p><strong>Note:</strong> I downvoted all the solutions using regular expresions. See here why you shouldn't parse HTML using regular expressions: <a href="http://stackoverflow.com/questions/590747/using-regular-expressions-to-parse-html-why-not">http://stackoverflow.com/questions/590747/using-regular-expressions-to-parse-html-why-not</a></p>

<p><strong>Note 2:</strong> Another SO question showing HTML that is impossible to parse with regular expressions: <a href="http://stackoverflow.com/questions/701166/can-you-provide-some-examples-of-why-it-is-hard-to-parse-xml-and-html-with-a-rege">http://stackoverflow.com/questions/701166/can-you-provide-some-examples-of-why-it-is-hard-to-parse-xml-and-html-with-a-rege</a></p>
<br /><b>#2</b><br /><p>I don't know Python good enough to tell you a solution. But if you want to use that to sanitize the user input you have to be very, very careful. Removing stuff between  and  just doesn't catch everything. Maybe you can have a look at existing solutions (I assume Django includes something like this).</p>
<br /><b>#3</b><br /><p>If you're removing everything between <code>&lt;script&gt;</code> and <code>&lt;/script&gt;</code> why not just remove the entire node? </p>

<p>Are you expecting a resig-style src and body?</p>
<br /><b>#4</b><br /><p><a href="http://diveintopython3.org/xml.html" rel="nofollow">Element Tree</a> is the best simplest and sweetest package to do this. Yes, there are other ways to do it too; but don't use any 'coz they suck! (via Mark Pilgrim)</p>
<br /><b>#5</b><br /><p>You can do this with the <a href="http://docs.python.org/library/htmlparser.html" rel="nofollow">HTMLParser</a> module (complicated) or use regular expressions:</p>

<pre><code>import re
content = "asdf &lt;script&gt; bla &lt;/script&gt; end"
x=re.search("&lt;script&gt;.*?&lt;/script&gt;", content, re.DOTALL)
span = x.span() # gives (5, 27)

stripped_content = content[:span[0]] + content[span[1]:]
</code></pre>

<p>EDIT: re.DOTALL, thanks to tgray</p>
<br /><b>#6</b><br /><pre><code>example_text = "This is some text &lt;script&gt; blah blah blah &lt;/script&gt; this is some more text."

import re
myre = re.compile("(^.*)&lt;script&gt;(.*)&lt;/script&gt;(.*$)")
result = myre.match(example_text)
result.groups()
  &lt;52&gt; ('This is some text ', ' blah blah blah ', ' this is some more text.')

# Text between &lt;script&gt; .. &lt;/script&gt;
result.group(2)
  &lt;56&gt; 'blah blah blah'

# Text outside of &lt;script&gt; .. &lt;/script&gt;
result.group(1)+result.group(3)
  &lt;57&gt; 'This is some text  this is some more text.'
</code></pre>
<br /><b>#7</b><br /><p>If you don't want to import any modules:</p>

<pre><code>string = "&lt;script&gt; this is some js. begone! &lt;/script&gt;"

string = string.split(' ')

for i, s in enumerate(string):
    if s == '&lt;script&gt;' or s == '&lt;/script&gt;' :
    	del string[i]

print ' '.join(string)
</code></pre>
<br /><b>#8</b><br /><p>According to answers posted by Pev and wr, why not to upgrade a regular expression, e.g.:</p>

<pre><code>pattern = r"(?is)&lt;script[^&gt;]*&gt;(.*?)&lt;/script&gt;"
text = """&lt;script&gt;foo bar  
baz bar foo  &lt;/script&gt;"""
re.sub(pattern, '', text)
</code></pre>

<p>(?is) - added to ignore case and allow new lines in text. This version should also support script tags with attributes.</p>

<p>EDIT: I can't add any comments yet, so I'm just editing my answer. I totally agree with the comment below, regexps are totally wrong for such tasks and b. soup ot lxml are a lot better. But question asked gave just a simple example and regexps should be enough for such simple task. Using Beautiful Soup for a simple text removing could just be too much (overload? I don't how to express what I mean, excuse my english).</p>

<p>BTW I made a mistake, the code should look like this:</p>

<pre><code>pattern = r"(?is)(&lt;script[^&gt;]*&gt;)(.*?)(&lt;/script&gt;)"
text = """&lt;script&gt;foo bar  
baz bar foo  &lt;/script&gt;"""
re.sub(pattern, '\1\3', text)
</code></pre>
<br />