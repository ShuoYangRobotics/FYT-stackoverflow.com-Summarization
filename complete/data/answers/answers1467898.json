[[{"text": ["I have a hard time believing that any script without any prior knowledge of the data (unlike MySql which has such info pre-loaded), would be faster than a SQL approach."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Aside from the time spent parsing the input, the script needs to \"keep\" sorting the order by array etc..."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["The following is a first guess at what should work decently fast in SQL, assuming a index (*) on the table's aa, bb, cc columns, in that order.", "(A possible alternative would be an \"aa, bb DESC, cc\" index"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["(*) This index could be clustered or not, not affecting the following query.", "Choice of clustering or not, and of needing an \"aa,bb,cc\" separate index depends on use case, on the size of the rows in table etc.", "etc."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n SELECT T1.aa, T1.bb, T1.cc , COUNT(*)\nFROM tblAbc T1\nLEFT OUTER JOIN tblAbc T2 ON T1.aa = T2.aa AND \n         (T1.bb &lt; T2.bb OR(T1.bb = T2.bb AND T1.cc &lt; T2.cc))\nGROUP BY T1.aa, T1.bb, T1.cc\nHAVING COUNT(*) &lt; 5  -- trick, remember COUNT(*) goes 1,1,2,3,...\nORDER BY T1.aa, T1.bb, T1.cc, COUNT(*) DESC\n</code>\n</pre>\n", "senID": 4}, {"text": ["The idea is to get a count of how many records, within a given aa value are smaller than self.", "There is a small trick however: we need to use LEFT OUTER join, lest we discard the record with the biggest bb value or the last one (which may happen to be one of the top 5).", "As a result of left joining it, the COUNT(*) value counts 1, 1, 2, 3, 4 etc.", "and the HAVING test therefore is \"&lt;5\" to effectively pick the top 5."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["To emulate the sample output of the OP, the ORDER BY uses DESC on the COUNT(), which could be removed to get a more traditional top 5 type of listing.", "Also, the COUNT() in the select list can be removed if so desired, this doesn't impact the logic of the query and the ability to properly sort."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["Also note that this query is deterministic in terms of the dealing with ties, i,e, when a given set of records have a same value for bb (within an aa group); I think the Python program may provide slightly different outputs when the order of the input data is changed, that is because of its occasional truncating of the sorting dictionary."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["Real solution: A SQL-based procedural approach"], "childNum": 1, "tag": "p", "senID": 8, "childList": [{"text": "Real solution: A SQL-based procedural approach", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["The self-join approach described above demonstrates how declarative statements can be used to express the OP's requirement.", "However this approach is naive in a sense that its performance is roughly bound to the sum of the squares of record counts within each aa 'category'.", "(not O(n^2) but roughly O((n/a)^2) where a is the number of different values for the aa column)  In other words it performs well with data such that on average the number of records associated with a given aa value doesn't exceed a few dozens.", "If the data is such that the aa column is not selective, the following  approach is much -much!- better suited.", "It leverages SQL's efficient sorting framework, while implementing a simple algorithm that would be hard to express in declarative fashion.", "This approach could further be improved for datasets with particularly huge number of records each/most aa 'categories' by introducing a simple binary search of the next aa value, by looking ahead (and sometimes back...) in the cursor.", "For cases where the number of aa 'categories' relative to the overall row count in tblAbc is low, see yet another approach, after this next one."], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"code": "<pre>\n<code>\n DECLARE @aa AS VARCHAR(10), @bb AS INT, @cc AS VARCHAR(10)\nDECLARE @curAa AS VARCHAR(10)\nDECLARE @Ctr AS INT\n\nDROP TABLE  tblResults;\nCREATE TABLE tblResults\n(  aa VARCHAR(10),\n   bb INT,\n   cc VARCHAR(10)\n);\n\nDECLARE abcCursor CURSOR \n  FOR SELECT aa, bb, cc\n  FROM tblABC\n  ORDER BY aa, bb DESC, cc\n  FOR READ ONLY;\n\nOPEN abcCursor;\n\nSET @curAa = ''\n\nFETCH NEXT FROM abcCursor INTO @aa, @bb, @cc;\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    IF @curAa &lt;&gt; @aa\n    BEGIN\n       SET @Ctr = 0\n       SET @curAa = @aa\n    END\n    IF @Ctr &lt; 5\n    BEGIN\n       SET @Ctr = @Ctr + 1;\n       INSERT tblResults VALUES(@aa, @bb, @cc);\n    END\n    FETCH NEXT FROM AbcCursor INTO @aa, @bb, @cc;\nEND;\n\nCLOSE abcCursor;\nDEALLOCATE abcCursor;\n\nSELECT * from tblResults\nORDER BY aa, bb, cc    -- OR .. bb DESC ... for a more traditional order.\n</code>\n</pre>\n", "senID": 10}, {"text": ["Alternative to the above for cases when aa is very unselective.", "In other words, when we have relatively few aa 'categories'.", "The idea is to go through the list of distinct categories and to run a \"LIMIT\" (MySql) \"TOP\" (MSSQL) query for each of these values.", "For reference purposes, the following ran in 63 seconds for tblAbc of 61 Million records divided in 45 aa values, on MSSQL 8.0, on a relatively old/weak host."], "childNum": 1, "tag": "p", "senID": 11, "childList": [{"text": "Alternative to the above", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n DECLARE @aa AS VARCHAR(10)\nDECLARE @aaCount INT\n\nDROP TABLE  tblResults;\nCREATE TABLE tblResults\n(  aa VARCHAR(10),\n   bb INT,\n   cc VARCHAR(10)\n);\n\nDECLARE aaCountCursor CURSOR \n  FOR SELECT aa, COUNT(*)\n  FROM tblABC\n  GROUP BY aa\n  ORDER BY aa\n  FOR READ ONLY;\nOPEN aaCountCursor;\n\n\nFETCH NEXT FROM aaCountCursor INTO @aa, @aaCount\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    INSERT tblResults \n       SELECT TOP 5 aa, bb, cc\n       FROM tblproh\n       WHERE aa = @aa\n       ORDER BY aa, bb DESC, cc\n\n    FETCH NEXT FROM aaCountCursor INTO @aa, @aaCount;\nEND;\n\nCLOSE aaCountCursor\nDEALLOCATE aaCountCursor\n\nSELECT * from tblResults\nORDER BY aa, bb, cc    -- OR .. bb DESC ... for a more traditional order.\n</code>\n</pre>\n", "senID": 12}, {"text": ["On the question of needing an index or not.", "(cf OP's remark)\nWhen merely running a \"SELECT * FROM myTable\", a table scan is effectively the fastest appraoch, no need to bother with indexes.", "However, the main reason why SQL is typically better suited for this kind of things (aside from being the repository where the data has been accumulating in the first place, whereas any external solution needs to account for the time to export the relevant data), is that it can rely on indexes to avoid scanning.", "Many general purpose languages are far better suited to handle raw processing, but they are fighting an unfair battle with SQL because they need to rebuilt any prior knowledge of the data which SQL has gathered in the course of its data collection / import phase.", "Since sorting is a typically a time and sometimes space consuming task, SQL and its relatively slower processing power often ends up ahead of alternative solutions."], "childNum": 1, "tag": "p", "senID": 13, "childList": [{"text": "On the question of needing an index or not", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Also, even without pre-built indexes, modern query optimizers may decide on a plan that involves the creation of a temporary index.", "And, because sorting is an intrinsic part of DDMS, the SQL servers are generally efficient in that area."], "childNum": 0, "tag": "p", "senID": 14, "childList": []}, {"text": ["So... Is SQL better?"], "childNum": 1, "tag": "p", "senID": 15, "childList": [{"text": "So... Is SQL better?", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["This said, if we are trying to compare SQL and other languages for pure ETL jobs, i.e.", "for dealing with heaps (unindexed tables) as its input to perform various transformations and filtering, it is likely that multi-thread-able utilities written in say C, and leveraging efficient sorting libaries, would likely be faster.", "The determining question to decide on a SQL vs. Non-SQL approach is where the data is located and where should it eventually reside.", "If we merely to convert a file to be supplied down \"the chain\" external programs are better suited.", "If we have or need the data in a SQL server, there are only rare cases that make it worthwhile exporting and processing externally."], "childNum": 1, "tag": "p", "senID": 16, "childList": [{"text": "heaps", "childNum": 0, "tag": "em", "pos": 1, "childList": []}]}], [{"text": ["You could use smarter data structures and still use python.", "I've ran your reference implementation and my python implementation on my machine and even compared the output to be sure in results."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["This is yours:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n $ time python ./ref.py  &lt; data-large.txt  &gt; ref-large.txt\n\nreal 1m57.689s\nuser 1m56.104s\nsys 0m0.573s\n</code>\n</pre>\n", "senID": 2}, {"text": ["This is mine:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n $ time python ./my.py  &lt; data-large.txt  &gt; my-large.txt\n\nreal 1m35.132s\nuser 1m34.649s\nsys 0m0.261s\n$ diff my-large.txt ref-large.txt \n$ echo $?\n0\n</code>\n</pre>\n", "senID": 4}, {"text": ["And this is the source:"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n #!/usr/bin/python\n# -*- coding: utf-8; -*-\nimport sys\nimport heapq\n\ntop_5 = {}\n\nfor line in sys.stdin:\n    aa, bb, cc = line.split()\n\n    # We want the top 5 for each distinct value of aa.  There are\n    # hundreds of thousands of values of aa.\n    bb = float(bb)\n    if aa not in top_5: top_5[aa] = []\n    current = top_5[aa]\n    if len(current) &lt; 5:\n        heapq.heappush(current, (bb, cc))\n    else:\n        if current[0] &lt; (bb, cc):\n            heapq.heapreplace(current, (bb, cc))\n\nfor aa in top_5:\n    current = top_5[aa]\n    while len(current) &gt; 0:\n        bb, cc = heapq.heappop(current)\n        print aa, bb, cc\n</code>\n</pre>\n", "senID": 6}, {"text": ["Update: Know your limits.", "I've also timed a noop code, to know the fastest possible python solution with code similar to the original:"], "childNum": 1, "tag": "p", "senID": 7, "childList": [{"text": "Update:", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n $ time python noop.py &lt; data-large.txt  &gt; noop-large.txt\n\nreal    1m20.143s\nuser    1m19.846s\nsys 0m0.267s\n</code>\n</pre>\n", "senID": 8}, {"text": ["And the noop.py itself:"], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"code": "<pre>\n<code>\n #!/usr/bin/python\n# -*- coding: utf-8; -*-\nimport sys\nimport heapq\n\ntop_5 = {}\n\nfor line in sys.stdin:\n    aa, bb, cc = line.split()\n\n    bb = float(bb)\n    if aa not in top_5: top_5[aa] = []\n    current = top_5[aa]\n    if len(current) &lt; 5:\n        current.append((bb, cc))\n\nfor aa in top_5:\n    current = top_5[aa]\n    current.sort()\n    for bb, cc in current[-5:]:\n        print aa, bb, cc\n</code>\n</pre>\n", "senID": 10}], [{"text": ["This is a sketch in Common Lisp"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Note that for long files there is a penalty for using READ-LINE, because it conses a fresh string for each line.", "Then use one of the derivatives of READ-LINE that are floating around that are using a line buffer.", "Also you might check if you want the hash table be case sensitive or not."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["second version"], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "second version", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Splitting the string is no longer needed, because we do it here.", "It is low level code, in the hope that some speed gains will be possible.", "It checks for one or more spaces as field delimiter and also tabs."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n (defun read-a-line (stream)\n  (let ((line (read-line stream nil nil)))\n    (flet ((delimiter-p (c)\n             (or (char= c #\\space) (char= c #\\tab))))\n      (when line\n        (let* ((s0 (position-if     #'delimiter-p line))\n               (s1 (position-if-not #'delimiter-p line :start s0))\n               (s2 (position-if     #'delimiter-p line :start (1+ s1)))\n               (s3 (position-if     #'delimiter-p line :from-end t)))\n          (values (subseq line 0 s0)\n                  (list (read-from-string line nil nil :start s1 :end s2)\n                        (subseq line (1+ s3)))))))))\n</code>\n</pre>\n", "senID": 4}, {"text": ["Above function returns two values: the key and a list of the rest."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n (defun dbscan (top-5-table stream)\n   \"get triples from each line and put them in the hash table\"\n  (loop with aa = nil and bbcc = nil do\n    (multiple-value-setq (aa bbcc) (read-a-line stream))\n    while aa do\n    (setf (gethash aa top-5-table)\n          (let ((l (merge 'list (gethash aa top-5-table) (list bbcc)\n                          #'&gt; :key #'first)))\n             (or (and (nth 5 l) (subseq l 0 5)) l)))))\n\n\n(defun dbprint (table output)\n  \"print the hashtable contents\"\n  (maphash (lambda (aa value)\n              (loop for (bb cc) in value\n                    do (format output \"~a ~a ~a~%\" aa bb cc)))\n           table))\n\n(defun dbsum (input &amp;optional (output *standard-output*))\n  \"scan and sum from a stream\"\n  (let ((top-5-table (make-hash-table :test #'equal)))\n    (dbscan top-5-table input)\n    (dbprint top-5-table output)))\n\n\n(defun fsum (infile outfile)\n   \"scan and sum a file\"\n   (with-open-file (input infile :direction :input)\n     (with-open-file (output outfile\n                      :direction :output :if-exists :supersede)\n       (dbsum input output))))\n</code>\n</pre>\n", "senID": 6}, {"text": ["some test data"], "childNum": 1, "tag": "p", "senID": 7, "childList": [{"text": "some test data", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n (defun create-test-data (&amp;key (file \"/tmp/test.data\") (n-lines 100000))\n  (with-open-file (stream file :direction :output :if-exists :supersede)\n    (loop repeat n-lines\n          do (format stream \"~a ~a ~a~%\"\n                     (random 1000) (random 100.0) (random 10000)))))\n</code>\n</pre>\n", "senID": 8}, {"text": ["; (create-test-data)"], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"code": "<pre>\n<code>\n (defun test ()\n  (time (fsum \"/tmp/test.data\" \"/tmp/result.data\")))\n</code>\n</pre>\n", "senID": 10}, {"text": ["third version, LispWorks"], "childNum": 1, "tag": "p", "senID": 11, "childList": [{"text": "third version, LispWorks", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Uses some SPLIT-STRING and PARSE-FLOAT functions, otherwise generic CL."], "childNum": 0, "tag": "p", "senID": 12, "childList": []}, {"code": "<pre>\n<code>\n (defun fsum (infile outfile)\n  (let ((top-5-table (make-hash-table :size 50000000 :test #'equal)))\n    (with-open-file (input infile :direction :input)\n      (loop for line = (read-line input nil nil)\n            while line do\n            (destructuring-bind (aa bb cc) (split-string '(#\\space #\\tab) line)\n              (setf bb (parse-float bb))\n              (let ((v (gethash aa top-5-table)))\n                (unless v\n                  (setf (gethash aa top-5-table)\n                        (setf v (make-array 6 :fill-pointer 0))))\n                (vector-push (cons bb cc) v)\n                (when (&gt; (length v) 5)\n                  (setf (fill-pointer (sort v #'&gt; :key #'car)) 5))))))\n    (with-open-file (output outfile :direction :output :if-exists :supersede)\n      (maphash (lambda (aa value)\n                 (loop for (bb . cc) across value do\n                       (format output \"~a ~f ~a~%\" aa bb cc)))\n               top-5-table))))\n</code>\n</pre>\n", "senID": 13}], [{"text": ["This took 45.7s on my machine with 27M rows of data that looked like this:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n 42 0.49357 0\n96 0.48075 1\n27 0.640761 2\n8 0.389128 3\n75 0.395476 4\n24 0.212069 5\n80 0.121367 6\n81 0.271959 7\n91 0.18581 8\n69 0.258922 9\n</code>\n</pre>\n", "senID": 1}, {"text": ["Your script took 1m42 on this data, the c++ example too 1m46 (g++ t.cpp -o t to compile it, I don't know anything about c++)."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Java 6, not that it matters really.", "Output isn't perfect, but it's easy to fix."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n package top5;\n\nimport java.io.BufferedReader;\nimport java.io.FileReader;\nimport java.util.Arrays;\nimport java.util.Map;\nimport java.util.TreeMap;\n\npublic class Main {\n\n    public static void main(String[] args) throws Exception {\n        long start  = System.currentTimeMillis();\n        Map&lt;String, Pair[]&gt; top5map = new TreeMap&lt;String, Pair[]&gt;();\n        BufferedReader br = new BufferedReader(new FileReader(\"/tmp/file.dat\"));\n\n        String line = br.readLine();\n        while(line != null) {\n            String parts[] = line.split(\" \");\n\n            String key = parts[0];\n            double score = Double.valueOf(parts[1]);\n            String value = parts[2];\n            Pair[] pairs = top5map.get(key);\n\n            boolean insert = false;\n            Pair p = null;\n            if (pairs != null) {\n                insert = (score &gt; pairs[pairs.length - 1].score) || pairs.length &lt; 5;\n            } else {\n                insert = true;\n            }\n            if (insert) {\n                p = new Pair(score, value);\n                if (pairs == null) {\n                    pairs = new Pair[1];\n                    pairs[0] = new Pair(score, value);\n                } else {\n                    if (pairs.length &lt; 5) {\n                        Pair[] newpairs = new Pair[pairs.length + 1];\n                        System.arraycopy(pairs, 0, newpairs, 0, pairs.length);\n                        pairs = newpairs;\n                    }\n                    int k = 0;\n                    for(int i = pairs.length - 2; i &gt;= 0; i--) {\n                        if (pairs[i].score &lt;= p.score) {\n                            pairs[i + 1] = pairs[i];\n                        } else {\n                            k = i + 1;\n                            break;\n                        }\n                    }\n                    pairs[k] = p;\n                }\n                top5map.put(key, pairs);\n            }\n            line = br.readLine();\n        }\n        for(Map.Entry&lt;String, Pair[]&gt; e : top5map.entrySet()) {\n            System.out.print(e.getKey());\n            System.out.print(\" \");\n            System.out.println(Arrays.toString(e.getValue()));\n        }\n        System.out.println(System.currentTimeMillis() - start);\n    }\n\n    static class Pair {\n        double score;\n        String value;\n\n        public Pair(double score, String value) {\n            this.score = score;\n            this.value = value;\n        }\n\n        public int compareTo(Object o) {\n            Pair p = (Pair) o;\n            return (int)Math.signum(score - p.score);\n        }\n\n        public String toString() {\n            return String.valueOf(score) + \", \" + value;\n        }\n    }\n}\n</code>\n</pre>\n", "senID": 4}, {"text": ["AWK script to fake the data:"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n BEGIN {\n for (i = 0; i &lt; 27000000; i++) {\n  v = rand();\n  k = int(rand() * 100);\n  print k \" \" v \" \" i;\n }\n exit;\n}\n</code>\n</pre>\n", "senID": 6}], [{"text": ["Pretty straightforward Caml (27 * 10^6 rows -- 27 sec, C++ by hrnt -- 29 sec)"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n open Printf\nopen ExtLib\n\nlet (&gt;&gt;) x f = f x\nlet cmp x y = compare (fst x : float) (fst y)\nlet wsp = Str.regexp \"[ \\t]+\"\n\nlet () =\n  let all = Hashtbl.create 1024 in\n  Std.input_lines stdin &gt;&gt; Enum.iter (fun line -&gt;\n    let [a;b;c] = Str.split wsp line in\n    let b = float_of_string b in\n    try\n      match Hashtbl.find all a with\n      | [] -&gt; assert false\n      | (bmin,_) as prev::tl -&gt; if b &gt; bmin then\n        begin\n          let m = List.sort ~cmp ((b,c)::tl) in\n          Hashtbl.replace all a (if List.length tl &lt; 4 then prev::m else m)\n        end\n    with Not_found -&gt; Hashtbl.add all a [b,c]\n  );\n  all &gt;&gt; Hashtbl.iter (fun a -&gt; List.iter (fun (b,c) -&gt; printf \"%s %f %s\\n\" a b c))\n</code>\n</pre>\n", "senID": 1}], [{"text": ["Here is a C++ solution.", "I didn't have a lot of data to test it with, however, so I don't know how fast it actually is."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["[edit] Thanks to the test data provided by the awk script in this thread, I\nmanaged to clean up and speed up the code a bit.", "I am not trying to find out the fastest possible version - the intent is to provide a reasonably fast version that isn't as ugly as people seem to think STL solutions can be. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["This version should be about twice as fast as the first version (goes through 27 million lines in about 35 seconds).", "Gcc users, remember to\ncompile this with -O2."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n #include &lt;map&gt;\n#include &lt;iostream&gt;\n#include &lt;functional&gt;\n#include &lt;utility&gt;\n#include &lt;string&gt;\nint main() {\n  using namespace std;\n  typedef std::map&lt;string, std::multimap&lt;double, string&gt; &gt; Map;\n  Map m;\n  string aa, cc;\n  double bb;\n  std::cin.sync_with_stdio(false); // Dunno if this has any effect, but anyways.\n\n  while (std::cin &gt;&gt; aa &gt;&gt; bb &gt;&gt; cc)\n    {\n      if (m[aa].size() == 5)\n        {\n          Map::mapped_type::iterator iter = m[aa].begin();\n          if (bb &lt; iter-&gt;first)\n            continue;\n          m[aa].erase(iter);\n        }\n      m[aa].insert(make_pair(bb, cc));\n    }\n  for (Map::const_iterator iter = m.begin(); iter != m.end(); ++iter)\n    for (Map::mapped_type::const_iterator iter2 = iter-&gt;second.begin();\n         iter2 != iter-&gt;second.end();\n         ++iter2)\n      std::cout &lt;&lt; iter-&gt;first &lt;&lt; \" \" &lt;&lt; iter2-&gt;first &lt;&lt; \" \" &lt;&lt; iter2-&gt;second &lt;&lt;\n std::endl;\n\n}\n</code>\n</pre>\n", "senID": 3}], [{"text": ["Interestingly, the original Python solution is by far the cleanest looking (although the C++  example comes close). "], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "looking", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["How about using Pyrex or Psyco on your original code?"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Of all the programs in this thread that I've tested so far, the OCaml version is the fastest and also among the shortest.", "(Line-of-code-based measurements are a little fuzzy, but it's not clearly longer than the Python version or the C or C++ versions, and it is clearly faster."], "childNum": 5, "tag": "p", "senID": 0, "childList": [{"text": "all", "childNum": 0, "tag": "em", "pos": 0, "childList": []}, {"text": "the OCaml version is the fastest", "childNum": 0, "tag": "strong", "pos": -1, "childList": []}, {"text": "shortest", "childNum": 0, "tag": "strong", "pos": -1, "childList": []}, {"text": "clearly longer", "childNum": 0, "tag": "em", "pos": 1, "childList": []}, {"text": "is", "childNum": 0, "tag": "em", "childList": []}]}, {"text": ["Here are the timings for the different versions so far, running on a 27-million-row 630-megabyte input data file.", "I'm on Ubuntu Intrepid Ibex on a dual-core 1.6GHz Celeron, running a 32-bit version of the OS (the Ethernet driver was broken in the 64-bit version).", "I ran each program five times and report the range of times those five tries took.", "I'm using Python 2.5.2, OpenJDK 1.6.0.0, OCaml 3.10.2, GCC 4.3.2, SBCL 1.0.8.debian, and Octave 3.0.1."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"tag": "ul", "num": 15, "lis": [{"text": "SquareCog's Pig version: not yet tested (because I can't just ", "tag": "none", "senID": 2}, {"text": "mjv's pure SQL version: not yet tested, but I predict a runtime of several days; ", "tag": "none", "senID": 3}, {"text": "ygrek's OCaml version: ", "tag": "none", "senID": 4}, {"text": "My Python version: ", "tag": "none", "senID": 5}, {"text": "abbot's heap-based Python version: ", "tag": "none", "senID": 6}, {"text": "My C version below, composed with GNU ", "tag": "none", "senID": 7}, {"text": "hrnt's C++ version: ", "tag": "none", "senID": 8}, {"text": "mjv's alternative SQL-based procedural approach: not yet tested, ", "tag": "none", "senID": 9}, {"text": "mjv's first SQL-based procedural approach: not yet tested, ", "tag": "none", "senID": 10}, {"text": "peufeu's ", "tag": "none", "senID": 11}, {"text": "Rainer Joswig's Common Lisp version: ", "tag": "none", "senID": 12}, {"text": "abbot's ", "tag": "none", "senID": 13}, {"text": "Will Hartung's Java version: ", "tag": "none", "senID": 14}, {"text": "Greg's Matlab version: doesn't work.", "tag": "none", "senID": 15}, {"text": "Schuyler Erle's suggestion of using Pyrex on one of the Python versions: not yet tried.", "tag": "none", "senID": 16}]}, {"text": ["I supect abbot's version comes out relatively worse for me than for them because the real dataset has a highly nonuniform distribution: as I said, some aa values (\u201cplayers\u201d) have thousands of lines, while others only have one."], "childNum": 1, "tag": "p", "senID": 17, "childList": [{"text": "aa", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["About Psyco: I applied Psyco to my original code (and abbot's version) by putting it in a main function, which by itself cut the time down to about 140 seconds, and calling psyco.full() before calling main().", "This added about four lines of code."], "childNum": 3, "tag": "p", "senID": 18, "childList": [{"text": "main", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "psyco.full()", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "main()", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"text": ["I can almost solve the problem using GNU sort, as follows:"], "childNum": 2, "tag": "p", "senID": 19, "childList": [{"text": "almost", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "sort", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n kragen@inexorable:~/devel$ time LANG=C sort -nr infile -o sorted\n\nreal    1m27.476s\nuser    0m59.472s\nsys 0m8.549s\nkragen@inexorable:~/devel$ time ./top5_sorted_c &lt; sorted &gt; outfile\n\nreal    0m5.515s\nuser    0m4.868s\nsys 0m0.452s\n</code>\n</pre>\n", "senID": 20}, {"text": ["Here top5_sorted_c is this short C program:"], "childNum": 1, "tag": "p", "senID": 21, "childList": [{"text": "top5_sorted_c", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n #include &lt;ctype.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;stdlib.h&gt;\n\nenum { linesize = 1024 };\n\nchar buf[linesize];\nchar key[linesize];             /* last key seen */\n\nint main() {\n  int n = 0;\n  char *p;\n\n  while (fgets(buf, linesize, stdin)) {\n    for (p = buf; *p &amp;&amp; !isspace(*p); p++) /* find end of key on this line */\n      ;\n    if (p - buf != strlen(key) || 0 != memcmp(buf, key, p - buf)) \n      n = 0;                    /* this is a new key */\n    n++;\n\n    if (n &lt;= 5)               /* copy up to five lines for each key */\n      if (fputs(buf, stdout) == EOF) abort();\n\n    if (n == 1) {               /* save new key in `key` */\n      memcpy(key, buf, p - buf);\n      key[p-buf] = '\\0';\n    }\n  }\n  return 0;\n}\n</code>\n</pre>\n", "senID": 22}, {"text": ["I first tried writing that program in C++ as follows, and I got runtimes which were substantially slower, at 33.6\u00b12.3 seconds instead of 5.5\u00b10.1 seconds:"], "childNum": 0, "tag": "p", "senID": 23, "childList": []}, {"code": "<pre>\n<code>\n #include &lt;map&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\nint main() {\n  using namespace std;\n  int n = 0;\n  string prev, aa, bb, cc;\n\n  while (cin &gt;&gt; aa &gt;&gt; bb &gt;&gt; cc) {\n    if (aa != prev) n = 0;\n    ++n;\n    if (n &lt;= 5) cout &lt;&lt; aa &lt;&lt; \" \" &lt;&lt; bb &lt;&lt; \" \" &lt;&lt; cc &lt;&lt; endl;\n    prev = aa;\n  }\n  return 0;\n}\n</code>\n</pre>\n", "senID": 24}, {"text": ["I did say almost.", "The problem is that sort -n does okay for most of the data, but it fails when it's trying to compare 0.33 with 3.78168e-05.", "So to get this kind of performance and actually solve the problem, I need a better sort."], "childNum": 4, "tag": "p", "senID": 25, "childList": [{"text": "almost", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "sort -n", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "0.33", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "3.78168e-05", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"text": ["Anyway, I kind of feel like I'm whining, but the sort-and-filter approach is about 5\u00d7 faster than the Python program, while the elegant STL program from hrnt is actually a little slower \u2014 there seems to be some kind of gross inefficiency in &lt;iostream&gt;.", "I don't know where the other 83% of the runtime is going in that little C++ version of the filter, but it isn't going anywhere useful, which makes me suspect I don't know where it's going in hrnt's std::map version either.", "Could that version be sped up 5\u00d7 too?", "Because that would be pretty cool.", "Its working set might be bigger than my L2 cache, but as it happens it probably isn't."], "childNum": 3, "tag": "p", "senID": 26, "childList": [{"text": "&lt;iostream&gt;", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "std::map", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "might", "childNum": 0, "tag": "em", "pos": 4, "childList": []}]}, {"text": ["Some investigation with callgrind says my filter program in C++ is executing 97% of its instructions inside of operator &gt;&gt;.", "I can identify at least 10 function calls per input byte, and cin.sync_with_stdio(false); doesn\u2019t help.", "This probably means I could get hrnt\u2019s C program to run substantially faster by parsing input lines more efficiently."], "childNum": 2, "tag": "p", "senID": 27, "childList": [{"text": "operator &gt;&gt;", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "cin.sync_with_stdio(false);", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}, {"text": ["Edit: kcachegrind claims that hrnt\u2019s program executes 62% of its instructions (on a small 157000 line input file) extracting doubles from an istream.", "A substantial part of this is because the istreams library apparently executes about 13 function calls per input byte when trying to parse a double.", "Insane.", "Could I be misunderstanding kcachegrind's output?"], "childNum": 3, "tag": "p", "senID": 28, "childList": [{"text": "double", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "istream", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "double", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"text": ["Anyway, any other suggestions?"], "childNum": 0, "tag": "p", "senID": 29, "childList": []}], [{"text": ["Has anybody tried doing this problem with just awk.", "Specifically 'mawk'?", "It should be faster than even Java and C++, according to this blog post: http://anyall.org/blog/2009/09/dont-mawk-awk-the-fastest-and-most-elegant-big-data-munging-language/ "], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://anyall.org/blog/2009/09/dont-mawk-awk-the-fastest-and-most-elegant-big-data-munging-language/", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://anyall.org/blog/2009/09/dont-mawk-awk-the-fastest-and-most-elegant-big-data-munging-language/"}]}, {"text": ["EDIT: Just wanted to clarify that the only claim being made in that blog post is that for a certain class of problems that are specifically suited to awk-style processing, the mawk virtual machine can beat 'vanilla' implementations in Java and C++. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Since you asked about Matlab, here's how I did something like what you're asking for.", "I tried to do it without any for loops, but I do have one because I didn't care to take a long time with it.", "If you were worried about memory then you could pull data from the stream in chunks with fscanf rather than reading the entire buffer."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n fid = fopen('fakedata.txt','r');\ntic\nA=fscanf(fid,'%d %d %d\\n');\nA=reshape(A,3,length(A)/3)';  %Matlab reads the data into one long column'\nNames = unique(A(:,1));\nfor i=1:length(Names)\n    indices = find(A(:,1)==Names(i));   %Grab all instances of key i\n    [Y,I] = sort(A(indices,2),1,'descend'); %sort in descending order of 2nd record\n    A(indices(I(1:min([5,length(indices(I))]))),:) %Print the top five\nend\ntoc\nfclose(fid)\n</code>\n</pre>\n", "senID": 1}], [{"text": ["Speaking of lower bounds on compute time :"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Let's analyze my algo above :"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n for each row (key,score,id) :\n    create or fetch a list of top scores for the row's key\n    if len( this list ) &lt; N\n        append current\n    else if current score &gt; minimum score in list\n        replace minimum of list with current row\n        update minimum of all lists if needed\n</code>\n</pre>\n", "senID": 2}, {"text": ["Let N be the N in top-N\nLet R be the number of rows in your data set\nLet K be the number of distinct keys"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["What assumptions can we make ?"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["R * sizeof( row ) > RAM or at least it's big enough that we don't want to load it all, use a hash to group by key, and sort each bin.", "For the same reason we don't sort the whole stuff."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["Kragen likes hashtables, so K * sizeof(per-key state) &lt;&lt; RAM, most probably it fits in L2/3 cache"], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["Kragen is not sorting, so K*N &lt;&lt; R ie each key has much more than N entries"], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["(note : A &lt;&lt; B means A is small relative to B)"], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"text": ["If the data has a random distribution, then "], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"text": ["after a small number of rows, the majority of rows will be rejected by the per-key minimum condition, the cost is 1 comparison per row."], "childNum": 0, "tag": "p", "senID": 10, "childList": []}, {"text": ["So the cost per row is 1 hash lookup + 1 comparison + epsilon * (list insertion + (N+1) comparisons for the minimum)"], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"text": ["If the scores have a random distribution (say between 0 and 1) and the conditions above hold, both epsilons will be very small."], "childNum": 0, "tag": "p", "senID": 12, "childList": []}, {"text": ["Experimental proof :"], "childNum": 0, "tag": "p", "senID": 13, "childList": []}, {"text": ["The 27 million rows dataset above produces 5933 insertions into the top-N lists.", "All other rows are rejected by a simple key lookup and comparison.", "epsilon = 0.0001"], "childNum": 0, "tag": "p", "senID": 14, "childList": []}, {"text": ["So roughly, the cost is 1 lookup + coparison per row, which takes a few  nanoseconds."], "childNum": 0, "tag": "p", "senID": 15, "childList": []}, {"text": ["On current hardware, there is no way this is not going to be negligible versus IO cost and especially parsing costs."], "childNum": 0, "tag": "p", "senID": 16, "childList": []}], [{"text": ["Here is one more OCaml version - targeted for speed - with custom parser on Streams.", "Too long, but parts of the parser are reusable.", "Thanks peufeu for triggering competition :)"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "peufeu", "childNum": 0, "tag": "strong", "pos": 2, "childList": []}]}, {"text": ["Speed :"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"tag": "ul", "num": 3, "lis": [{"text": "simple ocaml - 27 sec", "tag": "none", "senID": 2}, {"text": "ocaml with Stream parser - 15 sec", "tag": "none", "senID": 3}, {"text": "c with manual parser - 5 sec", "tag": "none", "senID": 4}]}, {"text": ["Compile with :"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n ocamlopt -pp camlp4o code.ml -o caml\n</code>\n</pre>\n", "senID": 6}, {"text": ["Code :"], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"code": "<pre>\n<code>\n open Printf\n\nlet cmp x y = compare (fst x : float) (fst y)\nlet digit c = Char.code c - Char.code '0'\n\nlet rec parse f = parser\n  | [&lt; a=int; _=spaces; b=float; _=spaces; \n       c=rest (Buffer.create 100); t &gt;] -&gt; f a b c; parse f t\n  | [&lt; &gt;] -&gt; ()\nand int = parser\n  | [&lt; ''0'..'9' as c; t &gt;] -&gt; int_ (digit c) t\n  | [&lt; ''-'; ''0'..'9' as c; t &gt;] -&gt; - (int_ (digit c) t)\nand int_ n = parser\n  | [&lt; ''0'..'9' as c; t &gt;] -&gt; int_ (n * 10 + digit c) t\n  | [&lt; &gt;] -&gt; n\nand float = parser\n  | [&lt; n=int; t=frem; e=fexp &gt;] -&gt; (float_of_int n +. t) *. (10. ** e)\nand frem = parser\n  | [&lt; ''.'; r=frem_ 0.0 10. &gt;] -&gt; r\n  | [&lt; &gt;] -&gt; 0.0\nand frem_ f base = parser\n  | [&lt; ''0'..'9' as c; t &gt;] -&gt; \n      frem_ (float_of_int (digit c) /. base +. f) (base *. 10.) t\n  | [&lt; &gt;] -&gt; f\nand fexp = parser\n  | [&lt; ''e'; e=int &gt;] -&gt; float_of_int e\n  | [&lt; &gt;] -&gt; 0.0\nand spaces = parser\n  | [&lt; '' '; t &gt;] -&gt; spaces t\n  | [&lt; ''\\t'; t &gt;] -&gt; spaces t\n  | [&lt; &gt;] -&gt; ()\nand crlf = parser\n  | [&lt; ''\\r'; t &gt;] -&gt; crlf t\n  | [&lt; ''\\n'; t &gt;] -&gt; crlf t\n  | [&lt; &gt;] -&gt; ()\nand rest b = parser\n  | [&lt; ''\\r'; _=crlf &gt;] -&gt; Buffer.contents b\n  | [&lt; ''\\n'; _=crlf &gt;] -&gt; Buffer.contents b\n  | [&lt; 'c; t &gt;] -&gt; Buffer.add_char b c; rest b t\n  | [&lt; &gt;] -&gt; Buffer.contents b\n\nlet () =\n  let all = Array.make 200 [] in\n  let each a b c =\n    assert (a &gt;= 0 &amp;&amp; a &lt; 200);\n    match all.(a) with\n    | [] -&gt; all.(a) &lt;- [b,c]\n    | (bmin,_) as prev::tl -&gt; if b &gt; bmin then\n      begin\n        let m = List.sort cmp ((b,c)::tl) in\n        all.(a) &lt;- if List.length tl &lt; 4 then prev::m else m\n      end\n  in\n  parse each (Stream.of_channel stdin);\n  Array.iteri \n    (fun a -&gt; List.iter (fun (b,c) -&gt; printf \"%i %f %s\\n\" a b c))\n    all\n</code>\n</pre>\n", "senID": 8}], [{"text": ["Isn't this just as simple as "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n SELECT DISTINCT aa, bb, cc FROM tablename ORDER BY bb DESC LIMIT 5\n</code>\n</pre>\n", "senID": 1}, {"text": [], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Of course, it's hard to tell what would be fastest without testing it against the data.", "And if this is something you need to run very fast, it might make sense to optimize your database to make the query faster, rather than optimizing the query."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["And, of course, if you need the flat file anyway, you might as well use that."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["The Pig version would go something like this (untested):"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n Data = LOAD '/my/data' using PigStorage() as (aa:int, bb:float, cc:chararray);\n grp = GROUP Data by aa;\n topK = FOREACH grp (\n     sorted = ORDER Data by bb DESC;\n     lim = LIMIT sorted 5;\n     GENERATE group as aa, lim;\n)\nSTORE topK INTO '/my/output' using PigStorage();\n</code>\n</pre>\n", "senID": 1}, {"text": ["Pig isn't optimized for performance; it's goal is to enable processing of multi-terabyte datasets using parallel execution frameworks.", "It does have a local mode, so you can try it, but I doubt it will beat your script."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["That was a nice lunch break challenge, he, he."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Top-N is a well-known database killer.", "As shown by the post above, there is no way to efficiently express it in common SQL."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["As for the various implementations, you got to keep in mind that the slow part in this is not the sorting or the top-N, it's the parsing of text.", "Have you looked at the source code for glibc's strtod() lately ?"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["For instance, I get, using Python :"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n Read data : 80.5  s\nMy TopN   : 34.41 s\nHeapTopN  : 30.34 s\n</code>\n</pre>\n", "senID": 4}, {"text": ["It is quite likely that you'll never get very fast timings, no matter what language you use, unless your data is in some format that is a lot faster to parse than text.", "For instance, loading the test data into postgres takes 70 s, and the majority of that is text parsing, too."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["If the N in your topN is small, like 5, a C implementation of my algorithm below would probably be the fastest.", "If N can be larger, heaps are a much better option."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["So, since your data is probably in a database, and your problem is getting at the data, not the actual processing, if you're really in need of a super fast TopN engine, what you should do is write a C module for your database of choice.", "Since postgres is faster for about anything, I suggest using postgres, plus it isn't difficult to write a C module for it."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["Here's my Python code :"], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"code": "<pre>\n<code>\n import random, sys, time, heapq\n\nROWS = 27000000\n\ndef make_data( fname ):\n    f = open( fname, \"w\" )\n    r = random.Random()\n    for i in xrange( 0, ROWS, 10000 ):\n        for j in xrange( i,i+10000 ):\n            f.write( \"%d    %f    %d\\n\" % (r.randint(0,100), r.uniform(0,1000), j))\n        print (\"write: %d\\r\" % i),\n        sys.stdout.flush()\n    print\n\ndef read_data( fname ):\n    for n, line in enumerate( open( fname ) ):\n        r = line.strip().split()\n        yield int(r[0]),float(r[1]),r[2]\n        if not (n % 10000 ):\n            print (\"read: %d\\r\" % n),\n            sys.stdout.flush()\n    print\n\ndef topn( ntop, data ):\n    ntop -= 1\n    assert ntop &gt; 0\n    min_by_key = {}\n    top_by_key = {}\n    for key,value,label in data:\n        tup = (value,label)\n        if key not in top_by_key:\n            # initialize\n            top_by_key[key] = [ tup ]\n        else:\n            top = top_by_key[ key ]\n            l    = len( top )\n            if l &gt; ntop:\n                # replace minimum value in top if it is lower than current value\n                idx = min_by_key[ key ]\n                if top[idx] &lt; tup:\n                    top[idx] = tup\n                    min_by_key[ key ] = top.index( min( top ) )\n            elif l &lt; ntop:\n                # fill until we have ntop entries\n                top.append( tup )\n            else:\n                # we have ntop entries in list, we'll have ntop+1\n                top.append( tup )\n                # initialize minimum to keep\n                min_by_key[ key ] = top.index( min( top ) )\n\n    # finalize:\n    return dict( (key, sorted( values, reverse=True )) for key,values in top_by_key.iteritems() )\n\ndef grouptopn( ntop, data ):\n    top_by_key = {}\n    for key,value,label in data:\n        if key in top_by_key:\n            top_by_key[ key ].append( (value,label) )\n        else:\n            top_by_key[ key ] = [ (value,label) ]\n\n    return dict( (key, sorted( values, reverse=True )[:ntop]) for key,values in top_by_key.iteritems() )\n\ndef heaptopn( ntop, data ):\n    top_by_key = {}\n    for key,value,label in data:\n        tup = (value,label)\n        if key not in top_by_key:\n            top_by_key[ key ] = [ tup ]\n        else:\n            top = top_by_key[ key ]\n            if len(top) &lt; ntop:\n                heapq.heappush(top, tup)\n            else:\n                if top[0] &lt; tup:\n                    heapq.heapreplace(top, tup)\n\n    return dict( (key, sorted( values, reverse=True )) for key,values in top_by_key.iteritems() )\n\ndef dummy( data ):\n    for row in data:\n        pass\n\nmake_data( \"data.txt\" )\n\nt = time.clock()\ndummy( read_data( \"data.txt\" ) )\nt_read = time.clock() - t\n\nt = time.clock()\ntop_result = topn( 5, read_data( \"data.txt\" ) )\nt_topn = time.clock() - t\n\nt = time.clock()\nhtop_result = heaptopn( 5, read_data( \"data.txt\" ) )\nt_htopn = time.clock() - t\n\n# correctness checking :\nfor key in top_result:\n    print key, \" : \", \"        \".join ((\"%f:%s\"%(value,label)) for (value,label) in    top_result[key])\n    print key, \" : \", \"        \".join ((\"%f:%s\"%(value,label)) for (value,label) in htop_result[key])\n\nprint\nprint \"Read data :\", t_read\nprint \"TopN :     \", t_topn - t_read\nprint \"HeapTopN : \", t_htopn - t_read\n\nfor key in top_result:\n    assert top_result[key] == htop_result[key]\n</code>\n</pre>\n", "senID": 9}], [{"text": ["Well, please grab a coffee and read the source code for strtod -- it's mindboggling, but needed, if you want to float -> text -> float to give back the same float you started with.... really..."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Parsing integers is a lot faster (not so much in python, though, but in C, yes)."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Anyway, putting the data in a Postgres table :"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n SELECT count( key ) FROM the dataset in the above program\n</code>\n</pre>\n", "senID": 3}, {"text": ["=> 7 s (so it takes 7 s to read the 27M records)"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"code": "<pre>\n<code>\n CREATE INDEX topn_key_value ON topn( key, value );\n</code>\n</pre>\n", "senID": 5}, {"text": ["191 s"], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"code": "<pre>\n<code>\n CREATE TEMPORARY TABLE topkeys AS SELECT key FROM topn GROUP BY key;\n</code>\n</pre>\n", "senID": 7}, {"text": ["12 s"], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"text": ["(You can use the index to get distinct values of 'key' faster too but it requires some light plpgsql hacking)"], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"code": "<pre>\n<code>\n CREATE TEMPORARY TABLE top AS SELECT (r).* FROM (SELECT (SELECT b AS r FROM topn b WHERE b.key=a.key ORDER BY value DESC LIMIT 1) AS r FROM topkeys a) foo;\n</code>\n</pre>\n", "senID": 10}, {"text": ["Temps : 15,310 ms"], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"code": "<pre>\n<code>\n INSERT INTO top SELECT (r).* FROM (SELECT (SELECT b AS r FROM topn b WHERE b.key=a.key ORDER BY value DESC LIMIT 1 OFFSET 1) AS r FROM topkeys a) foo;\n</code>\n</pre>\n", "senID": 12}, {"text": ["Temps : 17,853 ms"], "childNum": 0, "tag": "p", "senID": 13, "childList": []}, {"code": "<pre>\n<code>\n INSERT INTO top SELECT (r).* FROM (SELECT (SELECT b AS r FROM topn b WHERE b.key=a.key ORDER BY value DESC LIMIT 1 OFFSET 2) AS r FROM topkeys a) foo;\n</code>\n</pre>\n", "senID": 14}, {"text": ["Temps : 13,983 ms"], "childNum": 0, "tag": "p", "senID": 15, "childList": []}, {"code": "<pre>\n<code>\n INSERT INTO top SELECT (r).* FROM (SELECT (SELECT b AS r FROM topn b WHERE b.key=a.key ORDER BY value DESC LIMIT 1 OFFSET 3) AS r FROM topkeys a) foo;\n</code>\n</pre>\n", "senID": 16}, {"text": ["Temps : 16,860 ms"], "childNum": 0, "tag": "p", "senID": 17, "childList": []}, {"code": "<pre>\n<code>\n INSERT INTO top SELECT (r).* FROM (SELECT (SELECT b AS r FROM topn b WHERE b.key=a.key ORDER BY value DESC LIMIT 1 OFFSET 4) AS r FROM topkeys a) foo;\n</code>\n</pre>\n", "senID": 18}, {"text": ["Temps : 17,651 ms"], "childNum": 0, "tag": "p", "senID": 19, "childList": []}, {"code": "<pre>\n<code>\n INSERT INTO top SELECT (r).* FROM (SELECT (SELECT b AS r FROM topn b WHERE b.key=a.key ORDER BY value DESC LIMIT 1 OFFSET 5) AS r FROM topkeys a) foo;\n</code>\n</pre>\n", "senID": 20}, {"text": ["Temps : 19,216 ms"], "childNum": 0, "tag": "p", "senID": 21, "childList": []}, {"code": "<pre>\n<code>\n SELECT * FROM top ORDER BY key,value;\n</code>\n</pre>\n", "senID": 22}, {"text": ["As you can see computing the top-n is extremely fast (provided n is small) but creating the (mandatory) index is extremely slow because it involves a full sort."], "childNum": 0, "tag": "p", "senID": 23, "childList": []}, {"text": ["Your best bet is to use a format that is fast to parse (either binary, or write a custom C aggregate for your database, which would be the best choice IMHO).", "The runtime in the C program shouldn't be more than 1s if python can do it in 1 s."], "childNum": 0, "tag": "p", "senID": 24, "childList": []}], [{"text": ["I love lunch break challenges.", "Here's a 1 hour implementation."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["OK, when you don't want do some extremely exotic crap like additions, nothing stops you from using a custom base-10 floating point format whose only implemented operator is comparison, right ?", "lol."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["I had some fast-atoi code lying around from a previous project, so I just imported that."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["http://www.copypastecode.com/11541/"], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "http://www.copypastecode.com/11541/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.copypastecode.com/11541/"}]}, {"text": ["This C source code takes about 6.6 seconds to parse the 580MB of input text (27 million lines), half of that time is fgets, lol.", "Then it takes approximately 0.05 seconds to compute the top-n, but I don't know for sure, since the time it takes for the top-n is less than the timer noise."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["You'll be the one to test it for correctness though XDDDDDDDDDDD"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["Interesting huh ?"], "childNum": 0, "tag": "p", "senID": 6, "childList": []}], [{"text": ["Pick \"top 5\" would look something like this.", "Note that there's no sorting.", "Nor does any list in the top_5 dictionary ever grow beyond 5 elements."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n from collections import defaultdict\nimport sys\n\ndef keep_5( aList, aPair ):\n    minbb= min( bb for bb,cc in aList )\n    bb, cc = aPair\n    if bb &lt; minbb: return aList\n    aList.append( aPair )\n    min_i= 0\n    for i in xrange(1,6):\n        if aList[i][0] &lt; aList[min_i][0]\n            min_i= i\n    aList.pop(min_i)\n    return aList\n\n\ntop_5= defaultdict(list)\nfor row in sys.stdin:\n    aa, bb, cc = row.split()\n    bb = float(bb)\n    if len(top_5[aa]) &lt; 5:\n        top_5[aa].append( (bb,cc) )\n    else:\n        top_5[aa]= keep_5( top_5[aa], (bb,cc) )\n</code>\n</pre>\n", "senID": 1}]]