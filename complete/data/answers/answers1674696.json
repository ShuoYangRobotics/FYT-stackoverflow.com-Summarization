[[{"text": ["Looks like you need the \"workers\" to be separate processes (at least some of them, and therefore might as well make them all separate processes rather than bunches of threads divided into several processes).", "The multiprocessing module in Python 2.6 and later's standard library offers good facilities to spawn a pool of processes and communicate with them via FIFO \"queues\"; if for some reason you're stuck with Python 2.5 or even earlier there are versions of multiprocessing on the PyPi repository that you can download and use with those older versions of Python."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "multiprocessing", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/multiprocessing.html?highlight=multiprocessing#module-multiprocessing"}]}, {"text": ["The \"frontend\" can and should be pretty easily made to run with WSGI (with either Apache or Nginx), and it can deal with all communications to/from worker processes via multiprocessing, without the need to use HTTP, proxying, etc, for that part of the system; only the frontend would be a web app per se, the workers just receive, process and respond to units of work as requested by the frontend.", "This seems the soundest, simplest architecture to me."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "multiprocessing", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["There are other distributed processing approaches available in third party packages for Python, but multiprocessing is quite decent and has the advantage of being part of the standard library, so, absent other peculiar restrictions or constraints, multiprocessing is what I'd suggest you go for."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["The typical way to handle this sort of arrangement using threads in Python is to use the standard library module Queue.", "An example of using the Queue module for managing workers can be found here: Queue Example"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "Queue", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/queue.html"}, {"text": "Queue Example", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://effbot.org/librarybook/queue.htm"}]}], [{"text": ["There are many FastCGI modules with preforked mode and WSGI interface for python around, the most known is flup.", "My personal preference for such task is superfcgi with nginx.", "Both will launch several processes and will dispatch requests to them.", "12Mb is not as much to load them separately in each process, but if you'd like to share data among workers you need threads, not processes.", "Note, that heavy math in python with single process and many threads won't use several CPU/cores efficiently due to GIL.", "Probably the best approach is to use several processes (as much as cores you have) each running several threads (default mode in superfcgi)."], "childNum": 5, "tag": "p", "senID": 0, "childList": [{"text": "flup", "tag": "a", "pos": 0, "childList": [{"text": "flup", "tag": "code"}], "childNum": 1, "href": "http://trac.saddi.com/flup"}, {"text": "flup", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "superfcgi", "tag": "a", "pos": 1, "childList": [{"text": "superfcgi", "tag": "code"}], "childNum": 1, "href": "http://bitbucket.org/barbuza/superfcgi/"}, {"text": "superfcgi", "childNum": 0, "tag": "code", "pos": 5, "childList": []}, {"text": "superfcgi", "childNum": 0, "tag": "code", "childList": []}]}], [{"text": ["I think you can configure modwsgi/Apache so it will have several \"hot\" Python interpreters\nin separate processes ready to go at all times and also reuse them for new accesses\n(and spawn a new one if they are all busy).", "In this case you could load all the preprocessed data as module globals and they would\nonly get loaded once per process and get reused for each new access.", "In fact I'm not sure this isn't the default configuration\nfor modwsgi/Apache."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["The main problem here is that you might end up consuming\na lot of \"core\" memory (but that may not be a problem either).", "I think you can also configure modwsgi for single process/multiple\nthread -- but in that case you may only be using one CPU because\nof the Python Global Interpreter Lock (the infamous GIL), I think."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Don't be afraid to ask at the modwsgi mailing list -- they are very\nresponsive and friendly."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["You could use nginx load balancer to proxy to PythonPaste paster (which serves WSGI, for example Pylons), that launches each request as separate thread anyway."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["The most simple solution in this case is to use the webserver to do all the heavy lifting.", "Why should you handle threads and/or processes when the webserver will do all that for you?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["The standard arrangement in deployments of Python is:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["This is the architecture used Django and other popular web frameworks."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Another option is a queue table in the database.", "The worker processes run in a loop or off cron and poll the queue table for new jobs."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "", "childNum": 0, "tag": "br", "pos": 0, "childList": []}]}]]