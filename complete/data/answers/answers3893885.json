[[{"text": ["If it is \"pretty large\" file, then access the lines sequentially and don't read the whole file into memory:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n with open('largeFile', 'r') as inF:\n    for line in inF:\n        if 'myString' in line:\n            # do_something\n</code>\n</pre>\n", "senID": 1}], [{"text": ["You could do a simple find:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n f = open('file.txt', 'r')\nlines = f.read()\nanswer = lines.find('string')\n</code>\n</pre>\n", "senID": 1}, {"text": ["A simple find will be quite a bit quicker than regex if you can get away with it."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["If there is no way to tell where the string will be (first half, second half, etc) then there is really no optimized way to do the search other than the builtin \"find\" function.", "You could reduce the I/O time and memory consumption by not reading the file all in one shot, but at 4kb blocks (which is usually the size of an hard disk block).", "This will not make the search faster, unless the string is in the first part of the file, but in all case will reduce memory consumption which might be a good idea if the file is huge."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["The following function works for textfiles and binary files (returns only position in byte-count though), it does have the benefit to find strings even if they would overlap a line or buffer and would not be found when searching line- or buffer-wise."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "find strings even if they would overlap a line or buffer", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n def fnd(fname, s, start=0):\n    with open(fname, 'rb') as f:\n        fsize = os.path.getsize(fname)\n        bsize = 4096\n        buffer = None\n        if start &gt; 0:\n            f.seek(start)\n        overlap = len(s) - 1\n        while True:\n            if (f.tell() &gt;= overlap and f.tell() &lt; fsize):\n                f.seek(f.tell() - overlap)\n            buffer = f.read(bsize)\n            if buffer:\n                pos = buffer.find(s)\n                if pos &gt;= 0:\n                    return f.tell() - (len(buffer) - pos)\n            else:\n                return -1\n</code>\n</pre>\n", "senID": 1}, {"text": ["The idea behind this is:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"tag": "ul", "num": 3, "lis": [{"text": "seek to a start position in file", "tag": "none", "senID": 3}, {"text": "read from file to buffer (the search strings has to be smaller than the buffer size) but if not at the beginning, drop back the  - 1 bytes, to catch the string if started at the end of the last read buffer and continued on the next one.", "tag": "none", "senID": 4}, {"text": "return position or -1 if not found", "tag": "none", "senID": 5}]}, {"text": ["I used something like this to find signatures of files inside larger ISO9660 files, which was quite fast and did not use much memory, you can also use a larger buffer to speed things up."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}], [{"text": ["5000 lines isn't big (well, depends on how long the lines are...)"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Anyway: assuming the string will be a word and will be seperated by whitespace..."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n lines=open(file_path,'r').readlines()\nstr_wanted=\"whatever_youre_looking_for\"\n\n\n    for i in range(len(lines)):\n        l1=lines.split()\n        for p in range(len(l1)):\n            if l1[p]==str_wanted:\n                #found\n                # i is the file line, lines[i] is the full line, etc.\n</code>\n</pre>\n", "senID": 2}], [{"text": ["I've had a go at putting together a multiprocessing example of file text searching.", "This is my first effort at using the multiprocessing module; and I'm a python n00b.", "Comments quite welcome.", "I'll have to wait until at work to test on really big files.", "It should be faster on multi core systems than single core searching.", "Bleagh!", "How do I stop the processes once the text has been found and reliably report line number?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n import multiprocessing, os, time\nNUMBER_OF_PROCESSES = multiprocessing.cpu_count()\n\ndef FindText( host, file_name, text):\n    file_size = os.stat(file_name ).st_size \n    m1 = open(file_name, \"r\")\n\n    #work out file size to divide up to farm out line counting\n\n    chunk = (file_size / NUMBER_OF_PROCESSES ) + 1\n    lines = 0\n    line_found_at = -1\n\n    seekStart = chunk * (host)\n    seekEnd = chunk * (host+1)\n    if seekEnd &gt; file_size:\n        seekEnd = file_size\n\n    if host &gt; 0:\n        m1.seek( seekStart )\n        m1.readline()\n\n    line = m1.readline()\n\n    while len(line) &gt; 0:\n        lines += 1\n        if text in line:\n            #found the line\n            line_found_at = lines\n            break\n        if m1.tell() &gt; seekEnd or len(line) == 0:\n            break\n        line = m1.readline()\n    m1.close()\n    return host,lines,line_found_at\n\n# Function run by worker processes\ndef worker(input, output):\n    for host,file_name,text in iter(input.get, 'STOP'):\n        output.put(FindText( host,file_name,text ))\n\ndef main(file_name,text):\n    t_start = time.time()\n    # Create queues\n    task_queue = multiprocessing.Queue()\n    done_queue = multiprocessing.Queue()\n    #submit file to open and text to find\n    print 'Starting', NUMBER_OF_PROCESSES, 'searching workers'\n    for h in range( NUMBER_OF_PROCESSES ):\n        t = (h,file_name,text)\n        task_queue.put(t)\n\n    #Start worker processes\n    for _i in range(NUMBER_OF_PROCESSES):\n        multiprocessing.Process(target=worker, args=(task_queue, done_queue)).start()\n\n    # Get and print results\n\n    results = {}\n    for _i in range(NUMBER_OF_PROCESSES):\n        host,lines,line_found = done_queue.get()\n        results[host] = (lines,line_found)\n\n    # Tell child processes to stop\n    for _i in range(NUMBER_OF_PROCESSES):\n        task_queue.put('STOP')\n#        print \"Stopping Process #%s\" % i\n\n    total_lines = 0\n    for h in range(NUMBER_OF_PROCESSES):\n        if results[h][1] &gt; -1:\n            print text, 'Found at', total_lines + results[h][1], 'in', time.time() - t_start, 'seconds'\n            break\n        total_lines += results[h][0]\n\nif __name__ == \"__main__\":\n    main( file_name = 'testFile.txt', text = 'IPI1520' )\n</code>\n</pre>\n", "senID": 1}]]