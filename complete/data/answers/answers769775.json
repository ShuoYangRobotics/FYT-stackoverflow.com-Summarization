[[{"text": ["Define \"notably different\".", "Then have a look at \"edit distance\" measures."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "\"edit distance\" measures", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Edit%5Fdistance"}]}], [{"text": ["You could try a bit of code that counts words, and then sorts lines by those having the least common words. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If that doesn't do the trick, you can add in some smarts to filter out time stamps and numbers. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Your problem is similar to an earlier question on generating summaries of news stories."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "generating summaries of news stories", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/742711/shorten-a-text-and-only-keep-important-sentences"}]}], [{"text": ["I don't know a tool for you but if I were going to roll my own, I'd approach it like this:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "I", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["Presumably the log lines have a well defined structure, no?", "So"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"tag": "ul", "num": 5, "lis": [{"text": "parse the lines on that structure", "tag": "none", "senID": 2}, {"text": "write a number of very basic relevance filters (functions that just return a simple number from the parsed structure)", "tag": "none", "senID": 3}, {"text": "run the parsed lines through a set of filters, and cut on the basis of the total score", "tag": "none", "senID": 4}, {"text": "possibly sort the remaining lines into various bins by the results of more filters", "tag": "none", "senID": 5}, {"text": "generate reports, dump bins to files, or other output", "tag": "none", "senID": 6}]}, {"text": ["If you are familiar with the unix tool procmail, I'm suggesting a similar treatment customized for your data."], "childNum": 1, "tag": "p", "senID": 7, "childList": [{"text": "procmail", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"tag": "hr", "senID": 8}, {"text": ["As zacherates notes in the comments, your filters will typically ignore time stamps (and possibly IP address), and just concentrate on the content: for example really long http requests might represent an attack...or whatever applies to your domain."], "childNum": 1, "tag": "p", "senID": 9, "childList": [{"text": "really", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["Your binning filters might be as simple as a hash on a few selected fields, or you might try to do something with Charlie Martin's suggestion and used edit distance measures."], "childNum": 1, "tag": "p", "senID": 10, "childList": [{"text": "Charlie Martin's suggestion", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/769775/the-lines-that-stand-out-in-a-file-but-arent-exact-duplicates/769790#769790"}]}], [{"text": ["Perhaps you could do a basic calculation of \"words the same\"/\"all words\"?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["e.g.", "(including an offset to allow you to ignore the timestamp and the word 'INFO', if that's always the same):"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n def score(s1, s2, offset=26):\n    words1 = re.findall('\\w+', s1[offset:])\n    words2 = re.findall('\\w+', s2[offset:])\n    return float(len(set(words1) &amp; set(words2)))/max(len(set(words1)), len(set(words2)))\n</code>\n</pre>\n", "senID": 2}, {"text": ["Given:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n &gt;&gt;&gt; s1\n'2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234'\n&gt;&gt;&gt; s2\n'2009-04-20 00:04:02 INFO  com.foo.Bar - URL:/graph?id=asdfghjk'\n&gt;&gt;&gt; s3\n'2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses'\n&gt;&gt;&gt; s4\n'2009-04-20 00:06:00 INFO  com.baz.abc.Accessor - Cache /path/to/some/different/dir hits: 4352685 / 271315, 0.004423% misses'\n</code>\n</pre>\n", "senID": 4}, {"text": ["This yields:"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n &gt;&gt;&gt; score(s1,s2)\n0.8571428571428571\n&gt;&gt;&gt; score(s3,s4)\n0.75\n&gt;&gt;&gt; score(s1,s3)\n0.066666666666666666\n</code>\n</pre>\n", "senID": 6}, {"text": ["You've still got to decide which lines to compare.", "Also the use of set() may distort the scores slightly \u2013 the price of a simple algorithm :-)"], "childNum": 0, "tag": "p", "senID": 7, "childList": []}], [{"text": ["I wonder if you could just focus on the part that defines uniqueness for you.", "In this case, it seems that the part defining uniqueness is just the middle part:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n2009-04-20 00:03:57 INFO  com.foo.Bar - URL:/graph?id=1234\n                    ^---------------------^ \n\n2009-04-20 00:05:59 INFO  com.baz.abc.Accessor - Cache /path/to/some/dir hits: 3466 / 16534, 0.102818% misses\n                    ^--------------------------------^\n</pre>\n", "senID": 1}, {"text": ["I would then compare exactly this part, perhaps using a regular expression (just the parenthesized group; how to access sub-matches like this is language dependent):"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n /^.{20}(\\w+\\s+[\\w\\.-]+\\s+-\\s+\\w+)/\n</code>\n</pre>\n", "senID": 3}], [{"text": ["I think you want to break this into fields, sort by the \"severity level\" field and the next field (looks like \"class\").", "I'd use Haskell:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\nmodule Main where \nimport Data.List (nubBy, sortBy)\n\nsortAndNub s = nubBy fields2and3 \n     $ sortBy fields2and3comp\n     $ map words $ lines s\n\nfields2and3 a b =    fieldEq 2 a b \n                  && fieldEq 3 a b\nfieldEq f a b = a!!f == (b!!f)\nfields2and3comp a b = case compare (a!!2) (b!!2) of\n   LT -> LT\n   GT -> GT\n   EQ -> compare (a!!3) (b!!3)\nmain = interact $ unlines.(map unwords).sortAndNub\n</pre>\n", "senID": 1}]]