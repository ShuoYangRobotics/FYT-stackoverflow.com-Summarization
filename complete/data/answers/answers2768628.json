[[{"text": ["Iterating word-by-word through the contents of a file (the Wizard of Oz from Project Gutenberg, in my case), three different ways:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n from __future__ import with_statement\nimport time\nimport re\nfrom cStringIO import StringIO\n\ndef word_iter_std(filename):\n    start = time.time()\n    with open(filename) as f:\n        for line in f:\n            for word in line.split():\n                yield word\n    print 'iter_std took %0.6f seconds' % (time.time() - start)\n\ndef word_iter_re(filename):\n    start = time.time()\n    with open(filename) as f:\n        txt = f.read()\n    for word in re.finditer('\\w+', txt):\n        yield word\n    print 'iter_re took %0.6f seconds' % (time.time() - start)\n\ndef word_iter_stringio(filename):\n    start = time.time()\n    with open(filename) as f:\n        io = StringIO(f.read())\n    for line in io:\n        for word in line.split():\n            yield word\n    print 'iter_io took %0.6f seconds' % (time.time() - start)\n\nwoo = '/tmp/woo.txt'\n\nfor word in word_iter_std(woo): pass\nfor word in word_iter_re(woo): pass\nfor word in word_iter_stringio(woo): pass\n</code>\n</pre>\n", "senID": 1}, {"text": ["Resulting in:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n % python /tmp/junk.py\niter_std took 0.016321 seconds\niter_re took 0.028345 seconds\niter_io took 0.016230 seconds\n</code>\n</pre>\n", "senID": 3}], [{"text": ["This sounds like the sort of problem where a trie would really help.", "You should probably use some sort of compressed trie like a Patricia/radix trie.", "As long as you can fit the whole dictionary of words/phrases that you are looking for in the trie, this will greatly reduce the time complexity.", "How it will work is you take the beginning of a word and descend the trie until you find the longest match and increment the counter in that node.", "This might mean that you have to ascend the trie if a partial match doesn't pan out.", "Then you would proceed to the beginning of the next word and do it again.", "The advantage of the trie is that you are searching through the whole dictionary with each search through the trie (each look-up should take about O(m) where m is the average length of a word/phrase in your dictionary)."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "trie", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Trie"}, {"text": "Patricia/radix trie", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Radix_tree"}]}, {"text": ["If you can't fit the whole dictionary into one trie, then you could split the dictionary into a few tries (one for all words/phrases starting with a-l, one for m-z for instance) and do a sweep through the whole corpus for each trie. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["If the re module can't do it fast, you're going to be hard pressed doing it any faster.", "Either way you need to read the entire file.", "You might consider fixing your regular expression (can you provide one?).", "Maybe some background on what you are trying to accomplish too."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "re", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["You could try doing it the other way around...instead of processing the text corpus 2,000,000 times (once for each word), process it only once.", "For every single word in the corpus, increment a hash table or similar to store the count of that word.", "A simple example in pseudocode:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n word_counts = new hash&lt;string,int&gt;\nfor each word in corpus:\n  if exists(word_counts[word]):\n    word_counts[word]++\n  else:\n    word_counts[word] = 1\n</code>\n</pre>\n", "senID": 1}, {"text": ["You might be able to speed it up by initializing the word_counts ahead of time with the full list of words, this not needing that if statement...not sure."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["As xyld said, I do not think that you can beat the speed of the re module, although it would help if you posted your regexes and possibly the code as well.", "All I can add is try profiling before optimizing.", "You may be quite surprised when you see where most of the processing goes.", "I use hotshot to profile my code and am quite happy with it.", "You can find a good introduction to python profiling here http://onlamp.com/pub/a/python/2005/12/15/profiling.html."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://onlamp.com/pub/a/python/2005/12/15/profiling.html", "tag": "a", "pos": 4, "childList": [], "childNum": 0, "href": "http://onlamp.com/pub/a/python/2005/12/15/profiling.html"}]}], [{"text": ["If using re is not performant enough, you're probably using findall(), or finding the matches one by one manually.", "Using an iterator might make it faster:"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "re", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "findall()", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"code": "<pre>\n<code>\n &gt;&gt;&gt; for i in re.finditer(r'\\w+', 'Hello, this is a sentence.'):\n...     print i.group(0)\n...     \nHello\nthis\nis\na\nsentence\n</code>\n</pre>\n", "senID": 1}], [{"code": "<pre>\n<code>\n #!/usr/bin/env python\nimport re\n\ns = ''\nfor i in xrange(0, 100000):\n    s = s + 'Hello, this is a sentence. '\n    if i == 50000:\n        s = s + \" my phrase \"\n\ns = s + 'AARRGH'\n\nprint len(s)\n\nitr = re.compile(r'(my phrase)|(\\w+)').finditer(s)\nfor w in itr:\n    if w.group(0) == 'AARRGH':\n        print 'Found AARRGH'\n    elif w.group(0) == \"my phrase\":\n        print 'Found \"my phrase\"'\n</code>\n</pre>\n", "senID": 0}, {"text": ["Running this, we get"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n $ time python itrword.py\n2700017\nFound \"my phrase\"\nFound AARRGH\n\nreal    0m0.616s\nuser    0m0.573s\nsys     0m0.033s\n</code>\n</pre>\n", "senID": 2}, {"text": ["But, each \"phrase\" explicitly added to the regex will take its toll on performance -- the above is 50% slower than just using \"\\w+\", by my rough measurement."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}], [{"text": ["Have you considered looking at the Natural Language Toolkit.", "It includes many nice functions for working with a text corpus, also has a a cool FreqDist class that behaves dict-like (has keys) and list-like (slice).  "], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Natural Language Toolkit", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.nltk.org/"}]}]]