[[{"text": ["The major culprit here is as mentioned above the range() call.", "It will create a list with 15 million members, and that will eat up 200 MB of your memory, and with 15 processes, that's 3GB."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["But also don't read in the whole 15MB file into data(), read bit by bit from the response.", "Sticking those 15MB into a variable will use up 15MB of memory more than reading bit by bit from the response."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["You might want to consider simply just extracting data until you run out if indata, and comparing the count of data you extracted with what the first bytes said it should be.", "Then you need neither range() nor xrange().", "Seems more pythonic to me.", ":)"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Like others have said, you need at least the following two changes:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Even better, you could write the file as:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n items = GetData(url)\n    f = open(filename, 'w')\n    for item in items:\n        f.write(';'.join(item) + os.linesep)\n    f.close()\n</code>\n</pre>\n", "senID": 2}], [{"text": ["The last line should surely be f.close()?", "Those trailing parens are kinda important. "], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "f.close()", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["Consider using xrange() instead of range(), I believe that xrange is a generator whereas range() expands the whole list."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["I'd say either don't read the whole file into memory, or don't keep the whole unpacked structure in memory."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Currently you keep both in memory, at the same time, this is going to be quite big.", "So you've got at least two copies of your data in memory, plus some metadata."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Also the final line "], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n f.write(os.linesep.join(data))\n</code>\n</pre>\n", "senID": 4}, {"text": ["May actually mean you've temporarily got a third copy in memory (a big string with the entire output file)."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["So I'd say you're doing it in quite an inefficient way, keeping the entire input file, entire output file and a fair amount of intermediate data in memory at once."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["Using the generator to parse it is quite a nice idea.", "Consider writing each record out after you've generated it (it can then be discarded and the memory reused), or if that causes too many write requests, batch them into, say, 100 rows at once."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["Likewise, reading the response could be done in chunks.", "As they're fixed records this should be reasonably easy."], "childNum": 0, "tag": "p", "senID": 8, "childList": []}], [{"text": ["You can make this program more memory efficient by not reading all 15MB from the TCP connection, but instead processing each line as it is read.", "This will make the remote servers wait for you, of course, but that's okay. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Python is just not very memory efficient.", "It wasn't built for that."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["You could do more of your work in compiled C code if you convert this to a list comprehension:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n data = []\nitems = GetData(url)\nfor item in items:\n    data.append(';'.join(item))\n</code>\n</pre>\n", "senID": 1}, {"text": ["to:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n data = [';'.join(items) for items in GetData(url)]\n</code>\n</pre>\n", "senID": 3}, {"text": ["This is actually slightly different from your original code.", "In your version, GetData returns a 3-tuple, which comes back in items.", "You then iterate over this triplet, and append ';'.join(item) for each item in it.", "This means that you get 3 entries added to data for every triplet read from GetData, each one ';'.join'ed.", "If the items are just strings, then ';'.join will give you back a string with every other character a ';' - that is ';'.join(\"ABC\") will give back \"A;B;C\".", "I think what you actually wanted was to have each triplet saved back to the data list as the 3 values of the triplet, separated by semicolons.", "That is what my version generates."], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "wanted", "childNum": 0, "tag": "em", "pos": 5, "childList": []}]}, {"text": ["This may also help somewhat with your original memory problem, as you are no longer creating as many Python values.", "Remember that a variable in Python has much more overhead than one in a language like C.  Since each value is itself an object, and add the overhead of each name reference to that object, you can easily expand the theoretical storage requirement several-fold.", "In your case, reading 15Mb X 15 = 225Mb + the overhead of each item of each triple being stored as a string entry in your data list could quickly grow to your 2Gb observed size.", "At minimum, my version of your data list will have only 1/3 the entries in it, plus the separate item references are skipped, plus the iteration is done in compiled code."], "childNum": 1, "tag": "p", "senID": 5, "childList": [{"text": "much", "childNum": 0, "tag": "em", "pos": 1, "childList": []}]}], [{"text": ["There are 2 obvious places where you keep large data objects in memory (data variable in GetData() and data in MyThread.run() - these two will take about 500Mb) and probably there are other places in the skipped code.", "There are both easy to make memory efficient.", "Use response.read(4) instead of reading whole response at once and do it the same way in code behind UNPACK FIXED LENGTH OF BINARY DATA HERE.", "Change data.append(...) in MyThread.run() to "], "childNum": 8, "tag": "p", "senID": 0, "childList": [{"text": "data", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "GetData()", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "data", "childNum": 0, "tag": "code", "pos": 3, "childList": []}, {"text": "MyThread.run()", "childNum": 0, "tag": "code", "childList": []}, {"text": "response.read(4)", "childNum": 0, "tag": "code", "childList": []}, {"text": "UNPACK FIXED LENGTH OF BINARY DATA HERE", "childNum": 0, "tag": "code", "childList": []}, {"text": "data.append(...)", "childNum": 0, "tag": "code", "childList": []}, {"text": "MyThread.run()", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n if not first:\n    f.write(os.linesep)\nf.write(';'.join(item))\n</code>\n</pre>\n", "senID": 1}, {"text": ["These changes will save you a lot of memory."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Make sure you delete the threads after they are stopped.", "(using del)"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "del", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}]]