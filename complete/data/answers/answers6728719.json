[[{"code": "<pre>\n<code>\n import collections\ndef countWordDistances(li):\n    wordmap = collections.defaultdict(list)\n    for i, w in enumerate(li, 1):\n        wordmap[w].append(i)\n    for k, v in wordmap.iteritems():\n        wordmap[k] = sum(v)/float(len(v))\n\n    return wordmap\n</code>\n</pre>\n", "senID": 0}, {"text": ["This makes only one pass through the list, and keeps operations to a minimum.", "I timed this on a word list with 1.1M entries, 29k unique words, and it was almost twice as fast as Patrick's answer.", "On a list of 10k words, 2k unique, it was more than 300x faster than the OP's code."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["To make Python code go faster, there are two rules to keep in mind: use the best algorithm, and avoid Python.  "], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["On the algorithm front, iterating the list once instead of N+1 times (N= number of unique words) is the main thing that will speed this up.  "], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["On the \"avoid Python\" front, I mean: you want your code to be executing in C as much as possible.", "So using defaultdict is better than a dict where you explicitly check if the key is present.", "defaultdict does that check for you, but does it in C, in the Python implementation.", "enumerate is better than for i in range(len(li)), again because it's fewer Python steps.", "And enumerate(li, 1) makes the counting start at 1 instead of having to have a Python +1 somewhere in the loop."], "childNum": 5, "tag": "p", "senID": 4, "childList": [{"text": "defaultdict", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "defaultdict", "childNum": 0, "tag": "code", "pos": 2, "childList": []}, {"text": "enumerate", "childNum": 0, "tag": "code", "pos": 3, "childList": []}, {"text": "for i in range(len(li))", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "enumerate(li, 1)", "childNum": 0, "tag": "code", "pos": 4, "childList": []}]}, {"text": ["Edited: Third rule: use PyPy.", "My code goes twice as fast on PyPy as on 2.7."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}], [{"text": ["Based off @Ned Batchelder's solution, but without creating dummy lists:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n import collections\ndef countWordDistances(li):\n    wordmap = collections.defaultdict(lambda:[0.0, 0.0])\n    for i, w in enumerate(li, 1):\n        wordmap[w][0] += i\n        wordmap[w][1] += 1.0\n    for k, (t, n) in wordmap.iteritems():\n        wordmap[k] = t / n\n    return wordmap\n</code>\n</pre>\n", "senID": 1}], [{"text": ["I'm not sure if this will be faster than using a set, but it requires only one pass through the list:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n def countWordDistances(li):\n    wordmap = {}\n    for i in range(len(li)):\n        if li[i] in wordmap:\n            avg, num = wordmap[li[i]]\n            new_avg = avg*(num/(num+1.0)) + (1.0/(num+1.0))*i\n            wordmap[li[i]] = new_avg, num+1\n        else:\n            wordmap[li[i]] = (i, 1)\n\n    return wordmap\n</code>\n</pre>\n", "senID": 1}, {"text": ["This returns a modified version of wordmap, with the values associated with each key being a tuple of the average position and the number of occurences.", "You could obviously easily transform this to the form of the original output, but this would take some time."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["The code basically keeps a running average while iterating through the list, recalculating each time by taking a weighted average. "], "childNum": 0, "tag": "p", "senID": 3, "childList": []}], [{"text": ["Use a set:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n def countWordDistances(li):\n    '''\n    If li = ['that','sank','into','the','ocean']    \n    This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }\n    However, if there is a duplicate term, take the average of their positions\n    '''\n    wordmap = {}\n    unique_words = set(li)\n    for w in unique_words:\n        distances = [i+1 for i,x in enumerate(li) if x == w]\n        wordmap[w] = float(sum(distances)) / float(len(distances)) #take average\n    return wordmap\n</code>\n</pre>\n", "senID": 1}], [{"text": ["The first thing that springs to mind is to use a set to remove duplicate words:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n unique_words = set(li)\n</code>\n</pre>\n", "senID": 1}, {"text": ["In general, though, if you're worried about speed you need to profile the function to see where the bottleneck is, and then try to reduce that bottleneck."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Use a frozenset instead of a dict, since you're not doing anything with the values:"], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "frozenset", "tag": "a", "pos": 0, "childList": [{"text": "frozenset", "tag": "code"}], "childNum": 1, "href": "http://docs.python.org/library/stdtypes.html#set-types-set-frozenset"}, {"text": "frozenset", "childNum": 0, "tag": "code", "childList": []}, {"text": "dict", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n def removeDuplicatesFromList(seq):\n    return frozenset(seq)\n</code>\n</pre>\n", "senID": 1}], [{"text": ["Using list comprehension:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n def countWordDistances(l):\n    unique_words = set(l)\n    idx = [[i for i,x in enumerate(l) if x==item]\n            for item in unique_words]\n    return {item:1.*sum(idx[i])/len(idx[i]) + 1.\n            for i,item in enumerate(unique_words)}\n\nli = ['that','sank','into','the','ocean']\ncountWordDistances(li)\n# {'into': 3.0, 'ocean': 5.0, 'sank': 2.0, 'that': 1.0, 'the': 4.0}\n\nli2 = ['that','sank','into','the','ocean', 'that']\ncountWordDistances(li2)\n# {'into': 3.0, 'ocean': 5.0, 'sank': 2.0, 'that': 3.5, 'the': 4.0}\n</code>\n</pre>\n", "senID": 1}], [{"text": ["Oneliner -"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n from __future__ import division   # no need for this if using py3k\n\ndef countWordDistances(li):\n    '''\n    If li = ['that','sank','into','the','ocean']    \n    This function would return: { that:1, sank:2, into:3, the:4, ocean:5 }\n    However, if there is a duplicate term, take the average of their positions\n    '''\n    return {w:sum(dist)/len(dist) for w,dist in zip(set(li), ([i+1 for i,x in enumerate(li) if x==w] for w in set(li))) }\n</code>\n</pre>\n", "senID": 1}, {"text": ["What I am making in the last line is a dictionary comprehension, similar to a list comprehension."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}]]