[[{"text": ["I'd suggest looking at the presentation by David Beazley on using generators in Python.", "This technique allows you to handle a lot of data, and do complex processing, quickly and without blowing up your memory use.", "IMO, the trick isn't holding a huge amount of data in memory as efficiently as possible; the trick is avoiding loading a huge amount of data into memory at the same time."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "presentation by David Beazley", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.dabeaz.com/generators/"}]}], [{"text": ["Before you start tearing your hair out over the garbage collector, you might be able to avoid that 100mb hit of loading the entire file into memory by using a memory-mapped file object.", "See the mmap module."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "mmap", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/mmap.html"}]}], [{"text": ["Don't read the entire 100 meg file in at a time.", "Use streams to process a little bit at a time.", "Check out this blog post that talks about handling large csv and xml files.", "http://lethain.com/entry/2009/jan/22/handling-very-large-csv-and-xml-files-in-python/"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://lethain.com/entry/2009/jan/22/handling-very-large-csv-and-xml-files-in-python/", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://lethain.com/entry/2009/jan/22/handling-very-large-csv-and-xml-files-in-python/"}]}, {"text": ["Here is a sample of the code from the article."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n from __future__ import with_statement # for python 2.5\n\nwith open('data.in','r') as fin:\n    with open('data.out','w') as fout:\n        for line in fin:\n            fout.write(','.join(line.split(' ')))\n</code>\n</pre>\n", "senID": 2}], [{"text": ["So, from your comments I assume that your file looks something like this:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n item1,item2,item3,item4,item5,item6,item7,...,itemn\n</code>\n</pre>\n", "senID": 1}, {"text": ["which you all reduce to a single value by repeated application of some combination function.", "As a solution, only read a single value at a time:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n def read_values(f):\n    buf = []\n    while True:\n        c = f.read(1)\n        if c == \",\":\n            yield parse(\"\".join(buf))\n            buf = []\n        elif c == \"\":\n            yield parse(\"\".join(buf))\n            return\n        else:\n            buf.append(c)\n\nwith open(\"some_file\", \"r\") as f:\n     agg = initial\n     for v in read_values(f):\n         agg = combine(agg, v)\n</code>\n</pre>\n", "senID": 3}, {"text": ["This way, memory consumption stays constant, unless agg grows in time. "], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "agg", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["I hope I interpreted your problem correctly."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}], [{"text": ["First of all, don't touch the garbage collector.", "That's not the problem, nor the solution."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["It sounds like the real problem you're having is not with the file reading at all, but with the data structures that you're allocating as you process the files.", "Condering using del to remove structures that you no longer need during processing.", "Also, you might consider using marshal to dump some of the processed data to disk while you work through the next 100mb of input files."], "childNum": 2, "tag": "p", "senID": 1, "childList": [{"text": "del", "childNum": 0, "tag": "i", "pos": 1, "childList": []}, {"text": "marshal", "childNum": 0, "tag": "i", "pos": 2, "childList": []}]}, {"text": ["For file reading, you have basically two options: unix-style files as streams, or memory mapped files.", "For streams-based files, the default python file object is already buffered, so the simplest code is also probably the most efficient:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\nwith open(\"filename\", \"r\") as f:\n    for line in f:\n       # do something with a line of the files\n</pre>\n", "senID": 3}, {"text": ["Alternately, you can use f.read([size]) to read blocks of the file.", "However, usually you do this to gain CPU performance, by multithreading the processing part of your script, so that you can read and process at the same time.", "But it doesn't help with memory usage; in fact, it uses more memory."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["The other option is mmap, which looks like this:"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\nwith open(\"filename\", \"r+\") as f:\n    map = mmap.mmap(f.fileno(), 0)\n    line = map.readline()\n    while line != '':\n       # process a line\n       line = map.readline()\n</pre>\n", "senID": 6}, {"text": ["This sometimes outperforms streams, but it also won't improve memory usage."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}], [{"text": ["In your example code, data is being stored in the fc variable.", "If you don't keep a reference to fc around, your entire file contents will be removed from memory when the read method ends. "], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "fc", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "fc", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "read", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["If they are not, then you are keeping a reference somewhere.", "Maybe the reference is being created in read_100_mb_file, maybe in process.", "If there is no reference, CPython implementation will deallocate it almost immediatelly."], "childNum": 3, "tag": "p", "senID": 1, "childList": [{"text": "you are keeping a reference somewhere", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "read_100_mb_file", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "process", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"text": ["There are some tools to help you find where this reference is, guppy, dowser, pysizer..."], "childNum": 3, "tag": "p", "senID": 2, "childList": [{"text": "guppy", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://guppy-pe.sourceforge.net/"}, {"href": "http://www.aminus.net/wiki/Dowser", "text": "dowser", "childNum": 0, "tag": "a", "childList": []}, {"href": "http://pysizer.8325.org/", "text": "pysizer", "childNum": 0, "tag": "a", "childList": []}]}]]