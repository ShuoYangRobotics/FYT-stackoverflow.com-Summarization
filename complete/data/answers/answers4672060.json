[[{"text": ["There are a number of ways to do it, but, none will always work.", "Here are the two easiest:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 2, "lis": [{"text": "if it's a known finite set of websites: in your scraper convert each url from the normal url to the print url for a given site (cannot really be generalized across sites)", "tag": "none", "senID": 1}, {"text": "Use the arc90 readability algorithm (reference implementation  is in javascript) ", "tag": "none", "senID": 2}]}], [{"text": ["There's no way to do this that's guaranteed to work, but one strategy you might use is to try to find the element with the most visible text inside of it."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["It might be more useful to extract the RSS feeds (&lt;link type=\"application/rss+xml\" href=\"...\"/&gt;) on that page and parse the data in the feed to get the main content."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "&lt;link type=\"application/rss+xml\" href=\"...\"/&gt;", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["A while ago I wrote a simple Python script for just this task.", "It uses a heuristic to group text blocks together based on their depth in the DOM.", "The group with the most text is then assumed to be the main content.", "It's not perfect, but works generally well for news sites, where the article is generally the biggest grouping of text, even if broken up into multiple div/p tags."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "simple Python script", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.chrisspen.com/blog/how-to-extract-a-webpage%E2%80%99s-main-article-content-the-unicode-edition.html"}]}, {"text": ["You'd use the script like: python html2text.py &lt;url&gt;"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "python html2text.py &lt;url&gt;", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["I wouldn't try to scrape it from the web page - too many things could mess it up - but instead see which web sites publish RSS feeds.", "For example, the Guardian's RSS feed has most of the text from their leading articles:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["http://feeds.guardian.co.uk/theguardian/rss"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://feeds.guardian.co.uk/theguardian/rss", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://feeds.guardian.co.uk/theguardian/rss"}]}, {"text": ["I don't know if The Times (The London Times, not NY) has one because it's behind a paywall.", "Good luck with that..."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Another possibility of separating \"real\" content from noise is by measuring HTML density of the parts of a HTML page."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "measuring HTML density", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://ai-depot.com/articles/the-easy-way-to-extract-useful-text-from-arbitrary-html/"}]}, {"text": ["You will need a bit of experimentation with the thresholds to extract the \"real\" content, and I guess you could improve the algorithm by applying heuristics to specify the exact bounds of the HTML segment after having identified the interesting content."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Update: Just found out the URL above does not work right now; here is an alternative link to a cached version of archive.org."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "here is an alternative link", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://web.archive.org/web/20080620121103/http://ai-depot.com/articles/the-easy-way-to-extract-useful-text-from-arbitrary-html/"}]}]]