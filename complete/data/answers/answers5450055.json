[[{"text": ["If the data is a numpy array, you can try this:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n query = \"\"\"INSERT INTO `data` (frame, sensor_row, sensor_col, value) VALUES (%s, %s, %s, %s ) \"\"\"\nvalues = []\nrows, cols, frames = numpy.nonzero(data)\nfor row, col, frame in zip(rows, cols, frames):\n    values.append((frame, row, col, data[row,col,frame]))\n\ncur.executemany(query, values)\n</code>\n</pre>\n", "senID": 1}, {"text": ["or"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n query = \"\"\"INSERT INTO `data` (frame, sensor_row, sensor_col, value) VALUES (%s, %s, %s, %s ) \"\"\"\nrows, cols, frames = numpy.nonzero(data)\nvalues = [(row, col, frame, val) for row, col, frame, val in zip(rows, cols, frames, data[rows,cols,frames])]\ncur.executemany(query, values)\n</code>\n</pre>\n", "senID": 3}, {"text": ["Hope it helps"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["the fastest way to insert 4 million rows (16MB of data) would be to use load data infile - http://dev.mysql.com/doc/refman/5.0/en/load-data.html"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://dev.mysql.com/doc/refman/5.0/en/load-data.html", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://dev.mysql.com/doc/refman/5.0/en/load-data.html"}]}, {"text": ["so if possible generate a csv file then use load data infile.."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["hope this helps :)"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["EDIT"], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "EDIT", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["So I took one of your original data files rolloff.dat and wrote a quick and dirty program to convert it to the following csv format."], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "rolloff.dat", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Download frames.dat from here: http://rapidshare.com/files/454896698/frames.dat"], "childNum": 1, "tag": "p", "senID": 5, "childList": [{"text": "http://rapidshare.com/files/454896698/frames.dat", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://rapidshare.com/files/454896698/frames.dat"}]}, {"text": ["Frames.dat"], "childNum": 1, "tag": "p", "senID": 6, "childList": [{"text": "Frames.dat", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n patient_name, sample_date dd/mm/yyyy, frame_time (ms), frame 0..248, row 0..255, col 0..62, value\n\"Krulle (opnieuw) Krupp\",04/03/2010,0.00,0,5,39,0.4\n\"Krulle (opnieuw) Krupp\",04/03/2010,0.00,0,5,40,0.4\n...\n\"Krulle (opnieuw) Krupp\",04/03/2010,0.00,0,10,42,0.4\n\"Krulle (opnieuw) Krupp\",04/03/2010,0.00,0,10,43,0.4\n\"Krulle (opnieuw) Krupp\",04/03/2010,7.94,1,4,40,0.4\n\"Krulle (opnieuw) Krupp\",04/03/2010,7.94,1,5,39,0.4\n\"Krulle (opnieuw) Krupp\",04/03/2010,7.94,1,5,40,0.7\n\"Krulle (opnieuw) Krupp\",04/03/2010,7.94,1,6,44,0.7\n\"Krulle (opnieuw) Krupp\",04/03/2010,7.94,1,6,45,0.4\n...\n\"Krulle (opnieuw) Krupp\",04/03/2010,1968.25,248,241,10,0.4\n\"Krulle (opnieuw) Krupp\",04/03/2010,1968.25,248,241,11,0.4\n\"Krulle (opnieuw) Krupp\",04/03/2010,1968.25,248,241,12,1.1\n\"Krulle (opnieuw) Krupp\",04/03/2010,1968.25,248,241,13,1.4\n\"Krulle (opnieuw) Krupp\",04/03/2010,1968.25,248,241,14,0.4\n</code>\n</pre>\n", "senID": 7}, {"text": ["The file contains data only for frames that have values for each row and col - so zeros are excluded.", "24799 data rows were generated from your original file."], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"text": ["Next, I created a temporary loading (staging) table into which the frames.dat file is loaded.", "This is a temporary table which will allow you to manipulate/transform the data before loading into the proper production/reporting tables."], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"code": "<pre>\n<code>\n drop table if exists sample_temp;\ncreate table sample_temp\n(\npatient_name varchar(255) not null,\nsample_date date,\nframe_time decimal(6,2) not null default 0,\nframe_id tinyint unsigned not null,\nrow_id tinyint unsigned not null,\ncol_id tinyint unsigned not null,\nvalue decimal(4,1) not null default 0,\nprimary key (frame_id, row_id, col_id)\n)\nengine=innodb;\n</code>\n</pre>\n", "senID": 10}, {"text": ["All that remains is to load the data (note: i am using windows so you'll have to edit this script to make it linux compatible - check pathnames and change '\\r\\n' to '\\n')"], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"code": "<pre>\n<code>\n truncate table sample_temp;\n\nstart transaction;\n\nload data infile 'c:\\\\import\\\\frames.dat' \ninto table sample_temp\nfields terminated by ',' optionally enclosed by '\"'\nlines terminated by '\\r\\n'\nignore 1 lines\n(\npatient_name,\n@sample_date,\nframe_time,\nframe_id,\nrow_id,\ncol_id,\nvalue\n)\nset \nsample_date = str_to_date(@sample_date,'%d/%m/%Y');\n\ncommit;\n\nQuery OK, 24799 rows affected (1.87 sec)\nRecords: 24799  Deleted: 0  Skipped: 0  Warnings: 0\n</code>\n</pre>\n", "senID": 12}, {"text": ["The 24K rows were loaded in 1.87 seconds."], "childNum": 0, "tag": "p", "senID": 13, "childList": []}, {"text": ["Hope this helps :)"], "childNum": 0, "tag": "p", "senID": 14, "childList": []}], [{"text": ["I don't use Python or mySQL  but batch insert performance can often be sped up with transactions."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["Inserting multiple rows on each statement is one way of optimizing.", "However, why the need for the 3 loops?", "Maybe some sort of data transformation might be useful instead. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Another option is to disable indexes during the insertion, if you are certain that you won't have any duplicate data (assuming you actually have indexes on the table).", "Indexes must be updated for each statement, and also checked to prevent duplicates."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Invoke ALTER TABLE tablename DISABLE KEYS before starting your inserts, and when finished invoke ALTER TABLE tablename ENABLE KEYS and see if it helps"], "childNum": 2, "tag": "p", "senID": 2, "childList": [{"text": "ALTER TABLE tablename DISABLE KEYS", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "ALTER TABLE tablename ENABLE KEYS", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["From the manual:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["ALTER TABLE ...", "DISABLE KEYS tells MySQL to stop updating nonunique indexes.", "ALTER TABLE ...", "ENABLE KEYS then should be used to re-create missing indexes.", "MySQL does this with a special algorithm that is much faster than inserting keys one by one, so disabling keys before performing bulk insert operations should give a considerable speedup.", "Using ALTER TABLE ...", "DISABLE KEYS requires the INDEX privilege in addition to the privileges mentioned earlier."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["If i understand this correctly, executemany() executes an INSERT INTO query for each row you want to insert.", "This can be improved by creating a single INSERT query with all values, which should look like this:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n INSERT INTO data\n  (frame, sensor_row, sensor_col, value)\nVALUES\n (1, 1, 1, 1),\n (2, 2, 2, 2),\n (3, 3, 3, 3),\n ...\n</code>\n</pre>\n", "senID": 1}, {"text": ["Your python code should generate the row values in the brackets and create one query string out of it to finally execute the query once."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["You could employ list comprehenshions instead of for loops:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "for", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n values = [(frames, rows, cols, data[rows,cols,frames]) \\\n        for frames in range(nz) for rows in range(ny) \\\n        for cols in range(nx) if data[rows,cols,frames] &gt; 0.0]\n</code>\n</pre>\n", "senID": 1}, {"text": ["I'd estimate this could give you slight speed up such as 10-20%."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}]]