[[{"text": ["The only way to deal with this non-intrusively is to spawn each worker process such that its log goes to a different file descriptor (to disk or to pipe.", ")  Ideally, all log entries should be timestamped.", "Your controller process can then (if using disk files) coalesce the log files at the end of the run (sorting by timestamp) or, if using pipes (recommended approach), coalesce log entries on-the-fly from all pipes into a central log (e.g.", "periodically select from the pipes' fd's, perform merge-sort on the available log entries, flush to centralized log, repeat."], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "such that its log goes to a different file descriptor", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "select", "tag": "a", "pos": 3, "childList": [{"text": "select", "tag": "code"}], "childNum": 1, "href": "http://docs.python.org/library/select.html"}, {"text": "select", "childNum": 0, "tag": "code", "childList": []}]}], [{"text": ["I just now wrote a log handler of my own that just feeds everything to the parent process via a pipe.", "I've only been testing it for ten minutes but it seems to work pretty well (note this is hardcoded to RotatingFileHandler, which is my own use case)"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Updated.", "This now uses a queue for correct handling of concurrency, and also recovers from errors correctly.", "I've now been using this in production for several months and the current version below works without issue."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "Updated.", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n from logging.handlers import RotatingFileHandler\nimport multiprocessing, threading, logging, sys, traceback\n\nclass MultiProcessingLog(logging.Handler):\n    def __init__(self, name, mode, maxsize, rotate):\n        logging.Handler.__init__(self)\n\n        self._handler = RotatingFileHandler(name, mode, maxsize, rotate)\n        self.queue = multiprocessing.Queue(-1)\n\n        t = threading.Thread(target=self.receive)\n        t.daemon = True\n        t.start()\n\n    def setFormatter(self, fmt):\n        logging.Handler.setFormatter(self, fmt)\n        self._handler.setFormatter(fmt)\n\n    def receive(self):\n        while True:\n            try:\n                record = self.queue.get()\n                self._handler.emit(record)\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except EOFError:\n                break\n            except:\n                traceback.print_exc(file=sys.stderr)\n\n    def send(self, s):\n        self.queue.put_nowait(s)\n\n    def _format_record(self, record):\n        # ensure that exc_info and args\n        # have been stringified.  Removes any chance of\n        # unpickleable things inside and possibly reduces\n        # message size sent over the pipe\n        if record.args:\n            record.msg = record.msg % record.args\n            record.args = None\n        if record.exc_info:\n            dummy = self.format(record)\n            record.exc_info = None\n\n        return record\n\n    def emit(self, record):\n        try:\n            s = self._format_record(record)\n            self.send(s)\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\n    def close(self):\n        self._handler.close()\n        logging.Handler.close(self)\n</code>\n</pre>\n", "senID": 2}], [{"text": ["Yet another alternative might be the various non-file-based logging handlers in the logging package: "], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "logging package", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/logging.html"}]}, {"tag": "ul", "num": 3, "lis": [{"text": "SocketHandler", "tag": "none", "senID": 1}, {"text": "DatagramHandler", "tag": "none", "senID": 2}, {"text": "SyslogHandler", "tag": "none", "senID": 3}]}, {"text": ["(and others)"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["This way, you could easily have a logging daemon somewhere that you could write to safely and would handle the results correctly.", "Eg a simple socket server that just unpickles the message and emits it to its own rotating file handler."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["The syslog handler would take care of this for you too.", "Of course, you could use your own instance of syslog not the system one."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}], [{"text": ["just publish somewhere your instance of the logger.", "that way, the other modules and clients can use your API to get the logger without having to import multiprocessing."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "import multiprocessing", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}], [{"text": ["I also like zzzeek's answer but Andre is correct that a queue is required to prevent garbling.", "I had some luck with the pipe, but did see garbling which is somewhat expected.", "Implementing it turned out to be harder than I thought, particularly due to running on Windows, where there are some additional restrictions about global variables and stuff (see: http://stackoverflow.com/questions/765129/hows-python-multiprocessing-implemented-on-windows)"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://stackoverflow.com/questions/765129/hows-python-multiprocessing-implemented-on-windows", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/765129/hows-python-multiprocessing-implemented-on-windows"}]}, {"text": ["But, I finally got it working.", "This example probably isn't perfect, so comments and suggestions are welcome.", "It also does not support setting the formatter or anything other than the root logger.", "Basically, you have to reinit the logger in each of the pool processes with the queue and set up the other attributes on the logger."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Again, any suggestions on how to make the code better are welcome.", "I certainly don't know all the Python tricks yet :-)"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n import multiprocessing, logging, sys, re, os, StringIO, threading, time, Queue\n\nclass MultiProcessingLogHandler(logging.Handler):\n    def __init__(self, handler, queue, child=False):\n        logging.Handler.__init__(self)\n\n        self._handler = handler\n        self.queue = queue\n\n        # we only want one of the loggers to be pulling from the queue.\n        # If there is a way to do this without needing to be passed this\n        # information, that would be great!\n        if child == False:\n            self.shutdown = False\n            self.polltime = 1\n            t = threading.Thread(target=self.receive)\n            t.daemon = True\n            t.start()\n\n    def setFormatter(self, fmt):\n        logging.Handler.setFormatter(self, fmt)\n        self._handler.setFormatter(fmt)\n\n    def receive(self):\n        #print \"receive on\"\n        while (self.shutdown == False) or (self.queue.empty() == False):\n            # so we block for a short period of time so that we can\n            # check for the shutdown cases.\n            try:\n                record = self.queue.get(True, self.polltime)\n                self._handler.emit(record)\n            except Queue.Empty, e:\n                pass\n\n    def send(self, s):\n        # send just puts it in the queue for the server to retrieve\n        self.queue.put(s)\n\n    def _format_record(self, record):\n        ei = record.exc_info\n        if ei:\n            dummy = self.format(record) # just to get traceback text into record.exc_text\n            record.exc_info = None  # to avoid Unpickleable error\n\n        return record\n\n    def emit(self, record):\n        try:\n            s = self._format_record(record)\n            self.send(s)\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\n    def close(self):\n        time.sleep(self.polltime+1) # give some time for messages to enter the queue.\n        self.shutdown = True\n        time.sleep(self.polltime+1) # give some time for the server to time out and see the shutdown\n\n    def __del__(self):\n        self.close() # hopefully this aids in orderly shutdown when things are going poorly.\n\ndef f(x):\n    # just a logging command...\n    logging.critical('function number: ' + str(x))\n    # to make some calls take longer than others, so the output is \"jumbled\" as real MP programs are.\n    time.sleep(x % 3)\n\ndef initPool(queue, level):\n    \"\"\"\n    This causes the logging module to be initialized with the necessary info\n    in pool threads to work correctly.\n    \"\"\"\n    logging.getLogger('').addHandler(MultiProcessingLogHandler(logging.StreamHandler(), queue, child=True))\n    logging.getLogger('').setLevel(level)\n\nif __name__ == '__main__':\n    stream = StringIO.StringIO()\n    logQueue = multiprocessing.Queue(100)\n    handler= MultiProcessingLogHandler(logging.StreamHandler(stream), logQueue)\n    logging.getLogger('').addHandler(handler)\n    logging.getLogg\n</code>\n</pre>", "senID": 3}], [{"text": ["I liked zzzeek's answer.", "I would just substitute the Pipe for a Queue since if multiple threads/processes use the same pipe end to generate log messages they will get garbled."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["A variant of the others that keeps the logging and queue thread separate."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n \"\"\"sample code for logging in subprocesses using multiprocessing\n\n* Little handler magic - The main process uses loggers and handlers as normal.\n* Only a simple handler is needed in the subprocess that feeds the queue.\n* Original logger name from subprocess is preserved when logged in main\n  process.\n* As in the other implementations, a thread reads the queue and calls the\n  handlers. Except in this implementation, the thread is defined outside of a\n  handler, which makes the logger definitions simpler.\n* Works with multiple handlers.  If the logger in the main process defines\n  multiple handlers, they will all be fed records generated by the\n  subprocesses loggers.\n\ntested with Python 2.5 and 2.6 on Linux and Windows\n\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport traceback\nimport multiprocessing, threading, logging, sys\n\nDEFAULT_LEVEL = logging.DEBUG\n\nformatter = logging.Formatter(\"%(levelname)s: %(asctime)s - %(name)s - %(process)s - %(message)s\")\n\nclass SubProcessLogHandler(logging.Handler):\n    \"\"\"handler used by subprocesses\n\n    It simply puts items on a Queue for the main process to log.\n\n    \"\"\"\n\n    def __init__(self, queue):\n        logging.Handler.__init__(self)\n        self.queue = queue\n\n    def emit(self, record):\n        self.queue.put(record)\n\nclass LogQueueReader(threading.Thread):\n    \"\"\"thread to write subprocesses log records to main process log\n\n    This thread reads the records written by subprocesses and writes them to\n    the handlers defined in the main process's handlers.\n\n    \"\"\"\n\n    def __init__(self, queue):\n        threading.Thread.__init__(self)\n        self.queue = queue\n        self.daemon = True\n\n    def run(self):\n        \"\"\"read from the queue and write to the log handlers\n\n        The logging documentation says logging is thread safe, so there\n        shouldn't be contention between normal logging (from the main\n        process) and this thread.\n\n        Note that we're using the name of the original logger.\n\n        \"\"\"\n        # Thanks Mike for the error checking code.\n        while True:\n            try:\n                record = self.queue.get()\n                # get the logger for this record\n                logger = logging.getLogger(record.name)\n                logger.callHandlers(record)\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except EOFError:\n                break\n            except:\n                traceback.print_exc(file=sys.stderr)\n\nclass LoggingProcess(multiprocessing.Process):\n\n    def __init__(self, queue):\n        multiprocessing.Process.__init__(self)\n        self.queue = queue\n\n    def _setupLogger(self):\n        # create the logger to use.\n        logger = logging.getLogger('test.subprocess')\n        # The only handler desired is the SubProcessLogHandler.  If any others\n        # exist, remove them. In this case, on Unix and Linux the StreamHandler\n        # will be inherited.\n\n        for handler in logger.handlers:\n            # just a check for my sanity\n            assert not isinstance(handler, SubProcessLogHandler)\n            logger.removeHandler(handler)\n        # add the handler\n        handler = SubProcessLogHandler(self.queue)\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n        # On Windows, the level will not be inherited.  Also, we could just\n        # set the level to log everything here and filter it in the main\n        # process handlers.  For now, just set it from the global default.\n        logger.setLevel(DEFAULT_LEVEL)\n        self.logger = logger\n\n    def run(self):\n        self._setupLogger()\n        logger = self.logger\n        # and here goes the logging\n        p = multiprocessing.current_process()\n        logger.info('hello from process %s with pid %s' % (p.name, p.pid))\n\n\nif __name__ == '__main__':\n    # queue used by the subprocess loggers\n    queue = multiprocessing.Queue()\n    # Just a normal logger\n    logger = logging.getLogger('test')\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    logger.setLevel(DEFAULT_LEVEL)\n    logger.info('hello from the main process')\n    # This thread will read from the subprocesses and write to the main log's\n    # handlers.\n    log_queue_reader = LogQueueReader(queue)\n    log_queue_reader.start()\n    # create the processes.\n    for i in range(10):\n        p = LoggingProcess(queue)\n        p.start()\n    # The way I read the multiprocessing warning about Queue, joining a\n    # process before it has finished feeding the Queue can cause a deadlock.\n    # Also, Queue.empty() is not realiable, so just make sure all processes\n    # are finished.\n    # active_children joins subprocesses when they're finished.\n    while multiprocessing.active_children():\n        time.sleep(.1)\n</code>\n</pre>\n", "senID": 1}], [{"text": ["One of the alternatives is to write the mutliprocessing logging to a known file and register an atexit handler to join on those processes read it back on stderr; however, you won't get a real-time flow to the output messages on stderr that way."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "atexit", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["I have a solution that's similar to ironhacker's except that I use logging.exception in some of my code and found that I needed to format the exception before passing it back over the Queue since tracebacks aren't pickle'able:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n class QueueHandler(logging.Handler):\n    def __init__(self, queue):\n        logging.Handler.__init__(self)\n        self.queue = queue\n    def emit(self, record):\n        if record.exc_info:\n            # can't pass exc_info across processes so just format now\n            record.exc_text = self.formatException(record.exc_info)\n            record.exc_info = None\n        self.queue.put(record)\n    def formatException(self, ei):\n        sio = cStringIO.StringIO()\n        traceback.print_exception(ei[0], ei[1], ei[2], None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1] == \"\\n\":\n            s = s[:-1]\n        return s\n</code>\n</pre>\n", "senID": 1}]]