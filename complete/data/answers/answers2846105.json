[[{"text": ["oh you need to ignore the robots.txt"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n br = mechanize.Browser()\nbr.set_handle_robots(False)\n</code>\n</pre>\n", "senID": 1}], [{"text": ["You can try lying about your user agent (e.g., by trying to make believe you're a human being and not a robot) if you want to get in possible legal trouble with Barnes &amp; Noble.", "Why not instead get in touch with their business development department and convince them to authorize you specifically?", "They're no doubt just trying to avoid getting their site scraped by some classes of robots such as price comparison engines, and if you can convince them that you're not one, sign a contract, etc, they may well be willing to make an exception for you."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["A \"technical\" workaround that just breaks their policies as encoded in robots.txt is a high-legal-risk approach that I would never recommend.", "BTW, how does their robots.txt read?"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "does", "childNum": 0, "tag": "em", "pos": 1, "childList": []}]}], [{"text": ["Mechanize automatically follows robots.txt, but it can be disabled assuming you have permission, or you have through the ethics through .."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Set a flag in your browser: "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n browser.set_handle_equiv(False)\n</code>\n</pre>\n", "senID": 2}, {"text": ["This ignores robots.txt."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["Also, make sure you throttle your requests, so you don't put too much load on their site.", "(Note, this also makes it less likely that they will detect and ban you)."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["Set your User-Agent header to match some real IE/FF User-Agent."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "User-Agent", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Here's my IE8 useragent string:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; AskTB5.6)\n</code>\n</pre>\n", "senID": 2}], [{"text": ["The error you're receiving is not related to the user agent.", "mechanize by default checks robots.txt directives automatically when you use it to navigate to a site.", "Use the .set_handle_robots(false) method of mechanize.browser to disable this behavior."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["As it seems, you have to do less work to bypass robots.txt, at least says this article.", "So you might have to remove some code to ignore the filter."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "robots.txt", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "at least says this article", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://www.diovo.com/2009/03/how-can-a-crawler-bypass-robotstxt/"}]}], [{"text": ["Without debating the ethics of this you could modify the headers to look like the googlebot for example, or is the googlebot blocked as well?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}]]