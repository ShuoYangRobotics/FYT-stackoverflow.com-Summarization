[[{"text": ["You might find some useful results at the bottom of this link"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["http://www.scipy.org/PerformancePython"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://www.scipy.org/PerformancePython", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.scipy.org/PerformancePython"}]}, {"text": ["From the introduction,"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["It also compares MATLAB and seems to show similar speeds to when using Python and NumPy."], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "NumPy", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/NumPy"}]}, {"text": ["Of course this is only a specific example, your application might be allow better or worse performance.", "There is no harm in running the same test on both and comparing."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["You can also compile NumPy with optimized libraries such as ATLAS which provides some BLAS/LAPACK routines.", "These should be of comparable speed to MATLAB. "], "childNum": 3, "tag": "p", "senID": 5, "childList": [{"text": "ATLAS", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Automatically_Tuned_Linear_Algebra_Software"}, {"text": "BLAS", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms"}, {"text": "LAPACK", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/LAPACK"}]}, {"text": ["I'm not sure if the NumPy downloads are already built against it, but I think ATLAS will tune libraries to your system if you compile NumPy,"], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["http://www.scipy.org/Installing_SciPy/Windows"], "childNum": 1, "tag": "p", "senID": 7, "childList": [{"text": "http://www.scipy.org/Installing_SciPy/Windows", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.scipy.org/Installing_SciPy/Windows"}]}, {"text": ["The link has more details on what is required under the Windows platform."], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"text": ["EDIT:"], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"text": ["If you want to find out what performs better, C or C++, it might be worth asking a new question.", "Although from the link above C++ has best performance.", "Other solutions are quite close too i.e.", "Pyrex, Python/Fortran (using f2py) and inline C++."], "childNum": 0, "tag": "p", "senID": 10, "childList": []}, {"text": ["The only matrix algebra under C++ I have ever done was using MTL  and implementing an Extended Kalman Filter.", "I guess, though, in essence it depends on the libraries you are using LAPACK/BLAS and how well optimised it is. "], "childNum": 1, "tag": "p", "senID": 11, "childList": [{"text": "MTL", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.osl.iu.edu/research/mtl/"}]}, {"text": ["This link has a list of object-oriented numerical packages for many languages."], "childNum": 0, "tag": "p", "senID": 12, "childList": []}, {"text": ["http://www.oonumerics.org/oon/"], "childNum": 1, "tag": "p", "senID": 13, "childList": [{"text": "http://www.oonumerics.org/oon/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.oonumerics.org/oon/"}]}], [{"text": ["NumPy and MATLAB both use an underlying BLAS implementation for standard linear algebra operations.", "For some time both used ATLAS, but nowadays MATLAB apparently also comes with other implementations like Intel's Math Kernel Library (MKL).", "Which one is faster by how much depends on the system and how the BLAS implementation was compiled.", "You can also compile NumPy with MKL and Enthought is working on MKL support for their Python distribution (see their roadmap).", "Here is also a recent interesting blog post about this."], "childNum": 7, "tag": "p", "senID": 0, "childList": [{"text": "NumPy", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/NumPy"}, {"text": "BLAS", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms"}, {"text": "ATLAS", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Automatically_Tuned_Linear_Algebra_Software"}, {"text": "Math Kernel Library", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Math_Kernel_Library"}, {"text": "Enthought", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Enthought"}, {"text": "roadmap", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://www.enthought.com/products/roadmap.php"}, {"text": "interesting blog post", "tag": "a", "pos": 4, "childList": [], "childNum": 0, "href": "http://dpinte.wordpress.com/2010/01/15/numpy-performance-improvement-with-the-mkl/"}]}, {"text": ["On the other hand, if you need more specialized operations or data structures then both Python and MATLAB offer you various ways for optimization (like Cython, PyCUDA,...)."], "childNum": 2, "tag": "p", "senID": 1, "childList": [{"text": "Cython", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Cython"}, {"href": "http://mathema.tician.de/software/pycuda", "text": "PyCUDA", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["Edit: I corrected this answer to take into account different BLAS implementations.", "I hope it is now a fair representation of the current situation."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["The only valid test is to benchmark it.", "It really depends on what your platform is, and how well the Biot-Savart Law maps to Matlab or NumPy/SciPy built-in operations."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["As for making Python faster, Google's working on Unladen Swallow, a JIT compiler for Python.", "There are probably other projects like this as well."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["As per your edit 2, I recommend very strongly that you use Fortran because you can leverage the available linear algebra subroutines (Lapack and Blas) and it is way simpler than C/C++ for matrix computations."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If you prefer to go with a C/C++ approach, I would use C, because you presumably need raw performance on a presumably simple interface (matrix computations tend to have simple interfaces and complex algorithms)."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["If, however, you decide to go with C++, you can use the TNT (the Template Numerical Toolkit, the C++ implementation of Lapack)."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Good luck."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}], [{"text": ["If you're just using Python (with NumPy), it may be slower, depending on which pieces you use, whether or not you have optimized linear algebra libraries installed, and how well you know how to take advantage of NumPy. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["To make it faster, there are a few things you can do.", "There is a tool called Cython that allows you to add type declarations to Python code and translate it into a Python extension module in C. How much benefit this gets you depends a bit on how diligent you are with your type declarations - if you don't add any at all, you won't see much of any benefit.", "Cython also has support for NumPy types, though these are a bit more complicated than other types."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "Cython", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://cython.org/"}]}, {"text": ["If you have a good graphics card and are willing to learn a bit about GPU computing, PyCUDA can also help.", "(If you don't have an nvidia graphics card, I hear there is a PyOpenCL in the works as well).", "I don't know your problem domain, but if it can be mapped into a CUDA problem then it should be able to handle your 10^9 elements nicely."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "PyCUDA", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://mathema.tician.de/software/pycuda"}]}], [{"text": ["And here is an updated \"comparison\" between MATLAB and NumPy/MKL based on some linear algebra functions:"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "NumPy", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/NumPy"}, {"href": "http://en.wikipedia.org/wiki/Math_Kernel_Library", "text": "MKL", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["http://dpinte.wordpress.com/2010/03/16/numpymkl-vs-matlab-performance/"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://dpinte.wordpress.com/2010/03/16/numpymkl-vs-matlab-performance/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://dpinte.wordpress.com/2010/03/16/numpymkl-vs-matlab-performance/"}]}, {"text": ["The dot product is not that slow ;-)"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["I don't know if it's faster than MATLAB, but to make Python faster you could use Psyco.", "It speeds up your Python code without any changes to your source code."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Psyco", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Psyco"}]}, {"text": ["http://psyco.sourceforge.net/introduction.html"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://psyco.sourceforge.net/introduction.html", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://psyco.sourceforge.net/introduction.html"}]}], [{"text": ["I would also like to point out that Python (+NumPy) can easily interface with Fortran via the F2Py module, which basically nets you native Fortran speeds on the pieces of code you offload into it."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}]]