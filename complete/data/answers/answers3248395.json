[[{"text": ["Aha!", "So your real problem is how to test many conditions per line and if one of them is satisfied, to output that line.", "Easiest will be using regular expression, me thinks:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n import re\nkeywords = ['S0414', 'GT213', 'AT3423', 'PR342'] # etc - you probably get those from some source\npattern = re.compile('|'.join(keywords))\n\nfor line in inf:\n    if pattern.search(ln):\n        outf.write(line)\n</code>\n</pre>\n", "senID": 1}], [{"text": ["Testing many conditions per line is generally slow when using a naive algorithm.", "There are various superior algorithms (e.g.", "using Tries) which can do much better.", "I suggest you give the Aho\u2013Corasick string matching algorithm a shot.", "See here for a python implementation.", "It should be considerably faster than the naive approach of using a nested loop and testing every string individually."], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "Tries", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Trie"}, {"text": "Aho\u2013Corasick string matching algorithm", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Aho%2dCorasick_string_matching_algorithm"}, {"text": "here", "tag": "a", "pos": 4, "childList": [], "childNum": 0, "href": "http://hkn.eecs.berkeley.edu/~dyoo/python/ahocorasick/"}]}], [{"text": ["According to Python's documentation of file objects, iteration you're doing should not be especially slow, and search for substrings should also be fine speed-wise."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "file objects", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/stdtypes.html#file-objects"}]}, {"text": ["I don't see any reason why your code should be slow, so if you need it to go faster you might have to rewrite it in C and use mmap() for fast access to the source file."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "mmap()", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["You could try reading in big blocks, and avoiding the overhead of line-splitting except for the specific lines of interest.", "E.g., assuming none of your lines is longer than a megabyte:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n BLOCKSIZE = 1024 * 1024\n\ndef byblock_fullines(f):\n    tail = ''\n    while True:\n        block = f.read(BLOCKSIZE)\n        if not block: break\n        linend = block.rindex('\\n')\n        newtail = block[linend + 1:]\n        block = tail + block[:linend + 1]\n        tail = newtail\n        yield block\n    if tail: yield tail + '\\n'\n</code>\n</pre>\n", "senID": 1}, {"text": ["this takes an open file argument and yields blocks of about 1MB guaranteed to end with a newline.", "To identify (iterator-wise) all occurrences of a needle string within a haystack string:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n def haystack_in_needle(haystack, needle):\n    start = 0\n    while True:\n        where = haystack.find(needle, start)\n        if where == -1: return\n        yield where\n        start = where + 1\n</code>\n</pre>\n", "senID": 3}, {"text": ["To identify all relevant lines from within such a block:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"code": "<pre>\n<code>\n def wantlines_inblock(s, block):\n    last_yielded = None\n    for where in haystack_in_needle(block, s):\n        prevend = block.rfind('\\n', where)  # could be -1, that's OK\n        if prevend == last_yielded: continue  # no double-yields\n        linend = block.find('\\n', where)\n        if linend == -1: linend = len(block)\n        yield block[prevend + 1: linend]\n        last_yielded = prevend\n</code>\n</pre>\n", "senID": 5}, {"text": ["How this all fits together:"], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"code": "<pre>\n<code>\n def main():\n    with open('bigfile.txt') as f:\n        with open('smallfile.txt', 'w') as g:\n            for block in byblock_fulllines(f):\n                for line in wantlines_inblock('S0414', block)\n                    f.write(line)\n</code>\n</pre>\n", "senID": 7}, {"text": ["In 2.7 you could fold both with statements into one, just to reduce nesting a bit."], "childNum": 1, "tag": "p", "senID": 8, "childList": [{"text": "with", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Note: this code is untested so there might be (hopefully small;-) errors such as off-by-one's.", "Performance needs tuning of the block size and must be calibrated by measurement on your specific machine and data.", "Your mileage may vary.", "Void where prohibited by law."], "childNum": 0, "tag": "p", "senID": 9, "childList": []}], [{"text": ["If the line begins with S0414, then you could use the .startswith method:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": ".startswith", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n if line.startswith('S0414'): small_file3.write(line)\n</code>\n</pre>\n", "senID": 1}, {"text": ["You could also strip left whitespace, if there is any:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n line.lstrip().startswith('S0414')\n</code>\n</pre>\n", "senID": 3}, {"text": ["If 'S0414' always appears after a certain point, for example, it is always at least 10 characters in and never in the last 5 characters, you could do:"], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "'S0414'", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n 'S0414' in line[10:-5]\n</code>\n</pre>\n", "senID": 5}, {"text": ["Otherwise, you will have to search through each line, like you are. "], "childNum": 0, "tag": "p", "senID": 6, "childList": []}], [{"text": ["1.", "Try to read whole file"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "1. Try to read whole file", "childNum": 0, "tag": "strong", "pos": -1, "childList": []}]}, {"text": ["One speed up you can do is read whole file in memory if that is possible, else read in chunks.", "You said 'several hudred thousand lines' lets say 1 million lines with each line 100 char i.e.", "around 100 MB, if you have that much free memory (I assume you have) just do this"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n big_file = open('C:\\\\gbigfile.txt', 'r')\nbig_file_lines = big_file.read_lines()\nbig_file.close()\nsmall_file3 = open('C:\\\\small_file3.txt', 'w')\nfor line in big_file_lines:\n   if 'S0414' in line:\n      small_file3.write(line)\nsmall_file3.close()\n</code>\n</pre>\n", "senID": 2}, {"text": ["Time this with orginal version and see if it makes difference, I think it will."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["But if your file is really big in GBs, then you can read it in chunks e.g.", "100 MB chunks, split it into lines and search but don't forget to join lines at each 100MB interval (I can elaborate more if this is the case)"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["file.readlines returns a list containing all the lines of data in the file.", "If given an optional parameter sizehint, it reads that many bytes from the file and enough more to complete a line, and returns the lines from that.", "This is often used to allow efficient reading of a large file by lines, but without having to load the entire file in memory.", "Only complete lines will be returned."], "childNum": 2, "tag": "p", "senID": 5, "childList": [{"text": "file.readlines", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/tutorial/inputoutput.html#methods-of-file-objects"}, {"text": "This is often used to allow efficient reading of a large file by lines, but without having to load the entire file in memory. Only complete lines will be returned.", "childNum": 0, "tag": "strong", "pos": -1, "childList": []}]}, {"text": ["Also see following link for speed difference between line by line vs entire file reading.", "http://handyfloss.wordpress.com/2008/02/15/python-speed-vs-memory-tradeoff-reading-files/"], "childNum": 1, "tag": "p", "senID": 6, "childList": [{"text": "http://handyfloss.wordpress.com/2008/02/15/python-speed-vs-memory-tradeoff-reading-files/", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://handyfloss.wordpress.com/2008/02/15/python-speed-vs-memory-tradeoff-reading-files/"}]}, {"text": ["2.", "Try to write whole file"], "childNum": 1, "tag": "p", "senID": 7, "childList": [{"text": "2. Try to write whole file", "childNum": 0, "tag": "strong", "pos": -1, "childList": []}]}, {"text": ["You can also store line and write them at once at end, though I am not sure if it will help much"], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"code": "<pre>\n<code>\n big_file = open('C:\\\\gbigfile.txt', 'r')\nbig_file_lines = big_file.read_lines()\nsmall_file_lines = []\nfor line in big_file_lines:\n   if 'S0414' in line:\n      small_file_lines.append(line)\nsmall_file3 = open('C:\\\\small_file3.txt', 'w')\nsmall_file3.write(\"\".join(small_file_lines))\nsmall_file3.close()\n</code>\n</pre>\n", "senID": 9}, {"text": ["3.", "Try filter"], "childNum": 1, "tag": "p", "senID": 10, "childList": [{"text": "3. Try filter", "childNum": 0, "tag": "strong", "pos": -1, "childList": []}]}, {"text": ["You can also try to use filter, instead of loop see if it makes difference"], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"code": "<pre>\n<code>\n small_file_lines= filter(lambda line:line.find('S0414') &gt;= 0, big_file_lines)\n</code>\n</pre>\n", "senID": 12}], [{"text": ["What are the criteria that define the 30000 lines you want to extract?", "The more information you give, the more likely you are to get a useful answer."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If you want all the lines containing a certain string, or more generally containing any of a given set of strings, or an occurrence of a regular expression, use grep.", "It's likely to be significantly faster for large data sets."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "grep", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["This reminds me of a problem described by Tim Bray, who attempted to extract data from web server log files using multi-core machines.", "The results are described in The Wide Finder Project and Wide Finder 2.", "So, if serial optimizations don't go fast enough for you, this may be a place to start.", "There are examples of this sort of problem contributed in many languages, including python.", "Key quote from that last link:"], "childNum": 4, "tag": "p", "senID": 0, "childList": [{"text": "Tim Bray", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.tbray.org/ongoing/misc/Tim"}, {"text": "The Wide Finder Project", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://www.tbray.org/ongoing/When/200x/2007/09/20/Wide-Finder"}, {"text": "Wide Finder 2", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://www.tbray.org/ongoing/When/200x/2008/05/01/Wide-Finder-2"}, {"text": "including python", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://effbot.org/zone/wide-finder.htm"}]}, {"text": ["Having said this, 30,000 lines isn't that many so you may want to at least start by investigating your disk read/write performance.", "Does it help if you write the output to something other than the disk that you are reading the input from or read the whole file in one go before processing?"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "that", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}], [{"text": ["The best bet to speed it up would be if the specific string S0414 always appears at the same character position, so instead of having to make several failed comparisons per line (you said they start with different names) it could just do one and done."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "S0414", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["e.g.", "if you're file has lines like"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n GLY S0414 GCT\nASP S0435 AGG\nLEU S0432 CCT\n</code>\n</pre>\n", "senID": 2}, {"text": ["do an if line[4:9] == 'S0414': small.write(line)."], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "if line[4:9] == 'S0414': small.write(line)", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["This method assumes the special values appear in the same position on the line in gbigfile"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "gbigfile", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n def mydict(iterable):\n    d = {}\n    for k, v in iterable:\n        if k in d:\n            d[k].append(v)\n        else:\n            d[k] = [v]\n    return d\n\nwith open(\"C:\\\\to_find.txt\", \"r\") as t:\n    tofind = mydict([(x[0], x) for x in t.readlines()])\n\nwith open(\"C:\\\\gbigfile.txt\", \"r\") as bigfile:\n    with open(\"C:\\\\outfile.txt\", \"w\") as outfile:\n        for line in bigfile:\n            seq = line[4:9]\n            if seq in tofind[seq[0]]:\n                outfile.write(line)\n</code>\n</pre>\n", "senID": 1}, {"text": ["Depending on what the distribution of the starting letter in those targets you can cut your comparisons down by a significant amount.", "If you don't know where the values will appear you're talking about a LONG operation because you'll have to compare hundreds of thousands - let's say 300,000 -- 30,000 times.", "That's 9 million comparisons which is going to take a long time."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "long", "childNum": 0, "tag": "em", "pos": 2, "childList": []}]}]]