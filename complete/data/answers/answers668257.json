[[{"text": ["One option would be to post the work onto a queue of some sort (you could use something Enterprisey like ActiveMQ with pyactivemq or STOMP as a connector or you could use something lightweight like Kestrel which is written in Scala and speaks the same protocl as memcache so you can just use the python memcache client to talk to it)."], "childNum": 4, "tag": "p", "senID": 0, "childList": [{"text": "ActiveMQ", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://activemq.apache.org/"}, {"href": "http://code.google.com/p/pyactivemq/", "text": "pyactivemq", "childNum": 0, "tag": "a", "childList": []}, {"href": "http://stomp.codehaus.org/", "text": "STOMP", "childNum": 0, "tag": "a", "childList": []}, {"href": "http://github.com/robey/kestrel/tree/master", "text": "Kestrel", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["Once you have the queueing mechanism set up, you can create as many or as few worker tasks that are subscribed to the queue and do the actual downloading work as you want.", "You can even have them live on other machines so they don't interfere with the speed of serving yourwebsite at all.", "When the workers are done, they post the results back to the database or another queue where the webserver can pick them up."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["If you don't want to have to manage external worker processes then you could make the workers threads in the same python process that is running the webserver, but then obviously it will have greater potential to impact your web page serving performance."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["You might be able to use urllib to download the files and the Queue module to manage a number of worker threads.", "e.g:"], "childNum": 4, "tag": "p", "senID": 0, "childList": [{"text": "urllib", "tag": "a", "pos": 0, "childList": [{"text": "urllib", "tag": "code"}], "childNum": 1, "href": "http://docs.python.org/library/urllib.html"}, {"text": "urllib", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "Queue", "tag": "a", "pos": -1, "childList": [{"text": "Queue", "tag": "code"}], "childNum": 1, "href": "http://docs.python.org/library/queue.html"}, {"text": "Queue", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"code": "<pre>\n<code>\n import urllib\nfrom threading import Thread\nfrom Queue import Queue\n\nNUM_WORKERS = 20\n\nclass Dnld:\n    def __init__(self):\n        self.Q = Queue()\n        for i in xrange(NUM_WORKERS):\n            t = Thread(target=self.worker)\n            t.setDaemon(True)\n            t.start()\n\n    def worker(self):\n        while 1:\n            url, Q = self.Q.get()\n            try:\n                f = urllib.urlopen(url)\n                Q.put(('ok', url, f.read()))\n                f.close()\n            except Exception, e:\n                Q.put(('error', url, e))\n                try: f.close() # clean up\n                except: pass\n\n    def download_urls(self, L):\n        Q = Queue() # Create a second queue so the worker \n                    # threads can send the data back again\n        for url in L:\n            # Add the URLs in `L` to be downloaded asynchronously\n            self.Q.put((url, Q))\n\n        rtn = []\n        for i in xrange(len(L)):\n            # Get the data as it arrives, raising \n            # any exceptions if they occur\n            status, url, data = Q.get()\n            if status == 'ok':\n                rtn.append((url, data))\n            else:\n                raise data\n        return rtn\n\ninst = Dnld()\nfor url, data in inst.download_urls(['http://www.google.com']*2):\n    print url, data\n</code>\n</pre>\n", "senID": 1}], [{"text": ["Here is an interesting piece of code.", "I didn't use it myself, but it looks nice ;)"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["https://github.com/facebook/tornado/blob/master/tornado/httpclient.py"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "https://github.com/facebook/tornado/blob/master/tornado/httpclient.py", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "https://github.com/facebook/tornado/blob/master/tornado/httpclient.py"}]}, {"text": ["Low level AsyncHTTPClient:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["\"An non-blocking HTTP client backed with pycurl.", "Example usage:\""], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n import ioloop\n\ndef handle_request(response):\n    if response.error:\n        print \"Error:\", response.error\n    else:\n        print response.body\n    ioloop.IOLoop.instance().stop()\n\nhttp_client = httpclient.AsyncHTTPClient()\nhttp_client.fetch(\"http://www.google.com/\", handle_request)\nioloop.IOLoop.instance().start()\n</code>\n</pre>\n", "senID": 4}, {"text": ["\"\nfetch() can take a string URL or an HTTPRequest instance, which offers more options, like executing POST/PUT/DELETE requests."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["The keyword argument max_clients to the AsyncHTTPClient constructor determines the maximum number of simultaneous fetch() operations that can execute in parallel on each IOLoop."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["There is also new implementation in progress:\nhttps://github.com/facebook/tornado/blob/master/tornado/simple_httpclient.py\n\"Non-blocking HTTP client with no external dependencies.", "...", "This class is still in development and not yet recommended for production use."], "childNum": 1, "tag": "p", "senID": 7, "childList": [{"text": "https://github.com/facebook/tornado/blob/master/tornado/simple_httpclient.py", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "https://github.com/facebook/tornado/blob/master/tornado/simple_httpclient.py"}]}], [{"text": ["I'd just build a service in twisted that did that concurrent fetch and analysis and access that from web.py as a simple http request."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["Use the async http client which uses asynchat and asyncore.", "http://sourceforge.net/projects/asynchttp/files/asynchttp-production/asynchttp.py-1.0/asynchttp.py/download"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://sourceforge.net/projects/asynchttp/files/asynchttp-production/asynchttp.py-1.0/asynchttp.py/download", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://sourceforge.net/projects/asynchttp/files/asynchttp-production/asynchttp.py-1.0/asynchttp.py/download"}]}], [{"text": ["I'm not sure I'm understanding your question, so I'll give multiple partial answers to start with."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 3, "lis": [{"text": "If your concern is that web.py is having to download data from somewhere and analyze the results before responding, and you fear the request may time out before the results are ready, you could use ajax to split the work up.  Return immediately with a container page (to hold the results) and a bit of javascript to poll the sever for the results until the client has them all.  Thus the client never waits for the server, though the user still has to wait for the results.", "tag": "none", "senID": 1}, {"text": "If your concern is tying up the server waiting for the client to get the results, I doubt if that will actually be a problem.  Your networking layers should not require you to wait-on-write", "tag": "none", "senID": 2}, {"text": "If you are worrying about the server waiting while the client downloads static content from elsewhere, either ajax or clever use of redirects should solve your problem", "tag": "none", "senID": 3}]}], [{"text": ["I don't know if this will exactly work, but it looks like it might: EvServer: Python Asynchronous WSGI Server has a web.py interface and can do comet style push to the browser client."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "EvServer: Python Asynchronous WSGI Server", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://code.google.com/p/evserver/"}, {"href": "http://code.google.com/p/evserver/source/browse/trunk/evserver/examples/framework%5Fwebpy.py", "text": "web.py interface", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["If that isn't right, maybe you can use the Concurrence HTTP client for async download of the pages and figure out how to serve them to browser via ajax or comet."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "Concurrence HTTP client", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://opensource.hyves.org/concurrence/http.html"}]}], [{"text": ["Along the lines of MarkusQ's answer, MochiKit is a nice JavaScript library, with robust async methods inspired by Twisted."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "MochiKit", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://mochikit.com/"}]}], [{"text": ["Actually you can integrate twisted with web.py.", "I'm not really sure  how  as I've only done it with django (used twisted with it)."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["Nowadays there are excellent Python libs you might want to use - urllib3 (uses thread pools) and requests (uses thread pools through urllib3 or non blocking IO through gevent)"], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "urllib3", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://urllib3.readthedocs.org/"}, {"href": "http://docs.python-requests.org/", "text": "requests", "childNum": 0, "tag": "a", "childList": []}, {"href": "http://www.gevent.org/", "text": "gevent", "childNum": 0, "tag": "a", "childList": []}]}]]