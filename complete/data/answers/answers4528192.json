[[{"text": ["If you intern() each log entry then you'll use only one string for each similar log entry regardless of the number of times it shows up, thereby lowering memory usage a lot."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "intern()", "tag": "a", "pos": 0, "childList": [{"text": "intern()", "tag": "code"}], "childNum": 1, "href": "http://docs.python.org/library/functions.html#intern"}, {"text": "intern()", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n &gt;&gt;&gt; a = 'foo'\n&gt;&gt;&gt; b = 'foo'\n&gt;&gt;&gt; a is b\nTrue\n&gt;&gt;&gt; b = 'f' + ('oo',)[0]\n&gt;&gt;&gt; a is b\nFalse\n&gt;&gt;&gt; a = intern('foo')\n&gt;&gt;&gt; b = intern('f' + ('oo',)[0])\n&gt;&gt;&gt; a is b\nTrue\n</code>\n</pre>\n", "senID": 1}], [{"text": ["You could also process the log lines in reverse -- then use a set to keep track of which users you've seen:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "reverse", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n s = set()\n\n# note, this piece is inefficient in that I'm reading all the lines\n# into memory in order to reverse them...  There are recipes out there\n# for reading a file in reverse.\nlines = open('log').readlines()\nlines.reverse()\n\nfor line in lines:\n    line = line.strip()\n    user, op = line.split(':')\n    if not user in s:\n         print line\n         s.add(user)\n</code>\n</pre>\n", "senID": 1}], [{"text": ["The various dbm modules (dbm in Python 3, or anydbm, gdbm, dbhash, etc.", "in Python 2) let you create simple databases of key to value mappings.", "They are stored on the disk so there is no huge memory impact.", "And you can store them as logs if you wish to."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["This sounds like the perfect kind of problem for a Map/Reduce solution.", "See:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 2, "lis": [{"text": ["http://en.wikipedia.org/wiki/MapReduce"], "childNum": 0, "tag": "a", "senID": 1, "childList": []}, {"text": ["Hadoop"], "childNum": 0, "tag": "a", "senID": 2, "childList": []}]}, {"text": ["for example."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}], [{"text": ["Its pretty to easy to mock up the data structure to see how much memory it would take."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Something like this where you could change gen_string to generate data that would approximate the messages.  "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n import random\nfrom commands import getstatusoutput as gso\n\ndef gen_string():\n     return str(random.random())\n\n d = {}\n for z in range(10**6):\n     d[gen_string()] = gen_string()\n\nprint gso('ps -eo %mem,cmd |grep test.py')[1]\n</code>\n</pre>\n", "senID": 2}, {"text": ["On a one gig netbook:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n 0.4 vim test.py\n  0.1 /bin/bash -c time python test.py\n 11.7 /usr/bin/python2.6 test.py\n  0.1 sh -c { ps -eo %mem,cmd |grep test.py; } 2&gt;&amp;1\n  0.0 grep test.py\n\n   real    0m26.325s\n   user    0m25.945s\n   sys     0m0.377s\n</code>\n</pre>\n", "senID": 4}, {"text": ["...", "So its using about 10% of 1 gig for 100,000 records"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["But it would also depend on how much data redundancy you have ..."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}], [{"text": ["Thanks to @Ignacio for intern() -"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n def procLog(logName, userDict):\n    inf = open(logName, 'r')\n    for ln in inf.readlines():\n        name,act = ln.split(':')\n        userDict[name] = intern(act)\n    inf.close()\n    return userDict\n\ndef doLogs(logNameList):\n    userDict = {}\n    for logName in logNameList:\n        userDict = procLog(logName, userDict)\n    return userDict\n\ndef writeOrderedLog(logName, userDict):\n    keylist = userDict.keys()\n    keylist.sort()\n    outf = open(logName,'w')\n    for k in keylist:\n        outf.write(k + ':' + userDict[k])\n    outf.close()\n\ndef main():\n    mylogs = ['log20101214', 'log20101215', 'log20101216']\n    d = doLogs(mylogs)\n    writeOrderedLog('cumulativeLog', d)\n</code>\n</pre>\n", "senID": 1}, {"text": ["the question, then, is how much memory this will consume."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n def makeUserName():\n    ch = random.choice\n    syl = ['ba','ma','ta','pre','re','cu','pro','do','tru','ho','cre','su','si','du','so','tri','be','hy','cy','ny','quo','po']\n    # 22**5 is about 5.1 million potential names\n    return ch(syl).title() + ch(syl) + ch(syl) + ch(syl) + ch(syl)\n\nch = random.choice\nstates = ['joined', 'added pic', 'added article', 'added comment', 'voted', 'logged out']\nd = {}\nt = []\nfor i in xrange(1000):\n    for j in xrange(8000):\n        d[makeUserName()] = ch(states)\n    t.append( (len(d), sys.getsizeof(d)) )\n</code>\n</pre>\n", "senID": 3}, {"text": ["which results in"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"src": "http://i.stack.imgur.com/M4s68.png", "tag": "img", "senID": 5}, {"text": ["(horizontal axis = number of user names, vertical axis = memory usage in bytes) which is... slightly weird.", "It looks like a dictionary preallocates quite a lot of memory, then doubles it every time it gets too full?"], "childNum": 1, "tag": "p", "senID": 6, "childList": [{"text": "(horizontal axis = number of user names, vertical axis = memory usage in bytes)", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["Anyway, 4 million users takes just under 100MB of RAM - but it actually reallocates around 3 million users, 50MB, so if the doubling holds, you will need about 800MB of RAM to process 24 to 48 million users."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}]]