[[{"text": ["I would generate a Python source file every time the data changes, and have that file primarily consisting of a dictionary.", "This assumes that lookup is by symbol, and that the data readily fit into memory."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n data = {\n  \"AAPL\":       (\"Jan-1-1985\",      \"Dec-27-2010\"),\n...\n}\n</code>\n</pre>\n", "senID": 1}, {"text": ["To bulk-update the end date, use pprint.pprint, overwriting the entire file."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Edit: To illustrate how such a file can be written, here is a script that fills it out with random data"], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "Edit", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n import random, string, pprint\n\ndef randsym():\n    res =[]\n    for i in range(4):\n        res.append(random.choice(string.uppercase))\n    return ''.join(res)\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\ndays = range(1,29)\nyears = range(1980,2010)\ndef randdate():\n    return \"%s-%s-%s\" % (random.choice(months),\n                         random.choice(days),\n                         random.choice(years))\n\ndata = {}\nfor i in range(15000):\n    data[randsym()] = (randdate(), \"Dec-27-2010\")\n\nwith open(\"data.py\", \"w\") as f:\n    f.write(\"data=\")\n    f.write(pprint.pformat(data))\n</code>\n</pre>\n", "senID": 4}, {"text": ["To access the data, do from data import data."], "childNum": 1, "tag": "p", "senID": 5, "childList": [{"text": "from data import data", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["Since your data is highly structured, XML is not useful.", "CSV and JSON are reasonably fast and easy to edit for your purpose.", "However, if you value consistency (i.e.", "the data must not ever ever be wrong because it's updated while being read), you'll need to use file locking to ensure that.", "Unless you ever need only a subset of data and your application does not run on more than one machine in parallel, I don't see a rationale for a database."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "file locking", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/489861/locking-a-file-in-python"}]}], [{"text": ["Getting random lines by symbol and its not a lot of data?", "You need some kind of indexing.", "Store it in a python dict read from the source (a csv file?", ") when you start the web app, and restart the web app when the data changes."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Getting random lines by line number and its not a lot of data?", "Store it in a python list of tuples read from the source (a csv file?", ") when you start the web app, and restart the web app when the data changes."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["This assumes the web app is read-only, and updates to the data are done outside the app manually.", "Kick the server to notice the change."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Even being almost static, you may want to sort, search and filter - so it is not only about storing.", "Almost any read-many-write-once solution will make you happy, including:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 3, "lis": [{"text": "SQLite", "tag": "none", "senID": 1}, {"text": "MySQL", "tag": "none", "senID": 2}, {"text": "Key/value databases", "tag": "none", "senID": 3}]}, {"text": ["Indexes and other performance enhancements depends on your dataset properties, like cardinality, number of records, etc.", "Are you going to split load among multiple machines in the future?", "I would go with a database even for small datasets, just to be more future-proof, unless its an ad hoc application."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["I would avoid XML since it will require more parsing and its advantages don't benefit tables.", "Also, I would avoid dictionaries if you need more than a one-to-one mapping (ie.", "AAPL comes up twice or more).", "If the sets of data are relatively small, I would suggest CSV since it is ridiculously easy to use as a list:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n import csv\n\nmyList = []\nmyReader = csv.reader(open(\"your_file.csv\", \"rb\"))\nfor row in reader:\n    myList.append(row)\n...do stuff...\nmyWriter = csv.writer(open(\"your_file.csv\", \"wb\"))\nmyWriter.writerows(myList)\n</code>\n</pre>\n", "senID": 1}, {"text": ["If you need pure speed, efficiency, and scalability, there is no parallel to SQL, whatever form you choose.", "Between various forms of SQL (MySQL, MSSQL, Postgre, etc.", ") the differences are relatively small compared with the difference between SQL in general and CSV or XML."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["I said \"relatively small\" for CSV because there is no hard and fast rule that I can give you.", "It depends on a whole host of factors, but anything above a few MB will probably be noticably faster through SQL on many systems."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}], [{"text": ["My take on the problem:\nSQL  It scales, most of the work is handled for you.", "If you understand SQL it is probably (98%) the way to go."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "the", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["CSV Files: These get ugly on a HDD once you are dealing with more than a few (12 ish) accesses per second.", "HOWEVER- if the data is of reasonable size consider using a ramdrive, you can separate the data into files, and access them at blazing speeds.", "Lots of small files, no problem.", "But you will need to make sure that any data that needs to be saved is saved on real magnetic storage, or an SSD.", "CSV files on an SSD you might be looking at 1000 accesses/second if the data is small enough.", "With some good naming of files and small enough dataset this can be a viable option."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "blazing", "childNum": 0, "tag": "em", "pos": 1, "childList": []}]}, {"text": ["Plenty of ifs here but blazing speeds are a trade-off for crazy scalability, and having data consistency handled for you."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "ifs", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}]]