[[{"text": ["Most languages would probably be a reasonable fit, the critical components are "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Today most languages have libraries with good support for the above, of course you will need some way to persist the results that might be a database of some sorts. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["The more important thing rather than the language is understanding all concepts you need to deal with.", "Here are some Python examples that might help get you started."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["http://www.example-code.com/python/pythonspider.asp"], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "http://www.example-code.com/python/pythonspider.asp", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.example-code.com/python/pythonspider.asp"}]}], [{"text": ["Any language you can easily use with a good network library and support for parsing the formats you want to crawl.", "Those are really the only qualifications."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["You could consider using a combination of python and PyGtkMozEmbed or PyWebKitGtk plus javascript to create your spider."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["The spidering could be done in javascript after the page and all other scripts have loaded."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["You'd have one of the few web spiders that supports javascript, and might pick up some hidden stuff the others don't see :)"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["C++ - if you know what you're doing.", "You will not need a web server and a web application, because a web crawler is just a client, after all. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["why write your own when you can copy http://code.activestate.com/recipes/576551-simple-web-crawler/"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://code.activestate.com/recipes/576551-simple-web-crawler/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://code.activestate.com/recipes/576551-simple-web-crawler/"}]}, {"text": ["you might need to fix a few things here and there, like use htmlentities instead of replacing &amp; with &amp;"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["C is GoD of all when it comes to writing multicore/threaded crawlers but then it has its own complication.", "After C, some go for JAVA (due to wide exploration and usage) while other go to Python.", "If you have nice architecture, I can assure you these three language would really not limit your efficiency. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["This python code is a C Curl implementation and can crawl around 10,000 pages in 300 secs on a nice server"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n #! /usr/bin/env python\n# -*- coding: iso-8859-1 -*-\n# vi:ts=4:et\n# $Id: retriever-multi.py,v 1.29 2005/07/28 11:04:13 mfx Exp $\n\n#\n# Usage: python retriever-multi.py &lt;file with URLs to fetch&gt; [&lt;# of\n#          concurrent connections&gt;]\n#\n\nimport sys\nimport pycurl\n\n# We should ignore SIGPIPE when using pycurl.NOSIGNAL - see\n# the libcurl tutorial for more info.\ntry:\n    import signal\n    from signal import SIGPIPE, SIG_IGN\n    signal.signal(signal.SIGPIPE, signal.SIG_IGN)\nexcept ImportError:\n    pass\n\n\n# Get args\nnum_conn = 10\ntry:\n    if sys.argv[1] == \"-\":\n        urls = sys.stdin.readlines()\n    else:\n        urls = open(sys.argv[1]).readlines()\n    if len(sys.argv) &gt;= 3:\n        num_conn = int(sys.argv[2])\nexcept:\n    print \"Usage: %s &lt;file with URLs to fetch&gt; [&lt;# of concurrent connections&gt;]\" % sys.argv[0]\n    raise SystemExit\n\n\n# Make a queue with (url, filename) tuples\nqueue = []\nfor url in urls:\n    url = url.strip()\n    if not url or url[0] == \"#\":\n        continue\n    filename = \"doc_%03d.dat\" % (len(queue) + 1)\n    queue.append((url, filename))\n\n\n# Check args\nassert queue, \"no URLs given\"\nnum_urls = len(queue)\nnum_conn = min(num_conn, num_urls)\nassert 1 &lt;= num_conn &lt;= 10000, \"invalid number of concurrent connections\"\nprint \"PycURL %s (compiled against 0x%x)\" % (pycurl.version, pycurl.COMPILE_LIBCURL_VERSION_NUM)\nprint \"----- Getting\", num_urls, \"URLs using\", num_conn, \"connections -----\"\n\n\n# Pre-allocate a list of curl objects\nm = pycurl.CurlMulti()\nm.handles = []\nfor i in range(num_conn):\n    c = pycurl.Curl()\n    c.fp = None\n    c.setopt(pycurl.FOLLOWLOCATION, 1)\n    c.setopt(pycurl.MAXREDIRS, 5)\n    c.setopt(pycurl.CONNECTTIMEOUT, 30)\n    c.setopt(pycurl.TIMEOUT, 300)\n    c.setopt(pycurl.NOSIGNAL, 1)\n    m.handles.append(c)\n\n\n# Main loop\nfreelist = m.handles[:]\nnum_processed = 0\nwhile num_processed &lt; num_urls:\n    # If there is an url to process and a free curl object, add to multi stack\n    while queue and freelist:\n        url, filename = queue.pop(0)\n        c = freelist.pop()\n        c.fp = open(filename, \"wb\")\n        c.setopt(pycurl.URL, url)\n        c.setopt(pycurl.WRITEDATA, c.fp)\n        m.add_handle(c)\n        # store some info\n        c.filename = filename\n        c.url = url\n    # Run the internal curl state machine for the multi stack\n    while 1:\n        ret, num_handles = m.perform()\n        if ret != pycurl.E_CALL_MULTI_PERFORM:\n            break\n    # Check for curl objects which have terminated, and add them to the freelist\n    while 1:\n        num_q, ok_list, err_list = m.info_read()\n        for c in ok_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print \"Success:\", c.filename, c.url, c.getinfo(pycurl.EFFECTIVE_URL)\n            freelist.append(c)\n        for c, errno, errmsg in err_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print \"Failed: \", c.filename, c.url, errno, errmsg\n            freelist.append(c)\n        num_processed = num_processed + len(ok_list) + len(err_list)\n        if num_q == 0:\n            break\n    # Currently no more I/O is pending, could do something in the meantime\n    # (display a progress bar, etc.).\n    # We just call select() to sleep until some more data is available.\n    m.select(1.0)\n\n\n# Cleanup\nfor c in m.handles:\n    if c.fp is not None:\n        c.fp.close()\n        c.fp = None\n    c.close()\nm.close()\n</code>\n</pre>\n", "senID": 2}], [{"text": ["C# and C++ are probably the best two languages for this, it's just a matter of which you know better and which is faster (C# is probably easier)."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["I wouldn't recommend Python, Javascript, or PHP.", "They will usually be slower in text processing compared to a C-family language.", "If you're looking to crawl any significant chunk of the web, you'll need all the speed you can get."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["I've used C# and the HtmlAgilityPack to do so before, it works relatively well and is pretty easy to pick up.", "The ability to use a lot of the same commands to work with HTML as you would XML makes it nice (I had experience working with XML in C#)."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["You might want to test the speed of available C# HTML parsing libraries vs C++ parsing libraries.", "I know in my app, I was running through 60-70 fairly messy pages a second and pulling a good bit of data out of each (but that was a site with a pretty constant layout)."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["Edit: I notice you mentioned accessing a database.", "Both C++ and C# have libraries to work with most common database systems, from SQLite (which would be great for a quick crawler on a few sites) to midrange engines like MySQL and MSSQL up to the bigger DB engines (I've never used Oracle or DB2 from either language, but it's possible)."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}]]