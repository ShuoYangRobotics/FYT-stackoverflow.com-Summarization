[[{"text": ["Okay, you appear to be pretty confused about several things.", "Let's start at the beginning: you mentioned a \"multidimensional function\", but then go on to discuss the usual one-variable Gaussian curve.", "This is not a multidimensional function: when you integrate it, you only integrate one variable (x).", "The distinction is important to make, because there is a monster called a \"multivariate Gaussian distribution\" which is a true multidimensional function and, if integrated, requires integrating over two or more variables (which uses the expensive Monte Carlo technique I mentioned before).", "But you seem to just be talking about the regular one-variable Gaussian, which is much easier to work with, integrate, and all that."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "not", "childNum": 0, "tag": "em", "pos": 2, "childList": []}, {"text": "is", "childNum": 0, "tag": "em", "pos": 3, "childList": []}]}, {"text": ["The one-variable Gaussian distribution has two parameters, sigma and mu, and is a function of a single variable we'll denote x.", "You also appear to be carrying around a normalization parameter n (which is useful in a couple of applications).", "Normalization parameters are usually not included in calculations, since you can just tack them back on at the end (remember, integration is a linear operator: int(n*f(x), x) = n*int(f(x), x) ).", "But we can carry it around if you like; the notation I like for a normal distribution is then "], "childNum": 6, "tag": "p", "senID": 1, "childList": [{"text": "sigma", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "mu", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "x", "childNum": 0, "tag": "code", "pos": 2, "childList": []}, {"text": "n", "childNum": 0, "tag": "code", "pos": 3, "childList": []}, {"text": "not", "childNum": 0, "tag": "em", "childList": []}, {"text": "int(n*f(x), x) = n*int(f(x), x)", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["N(x | mu, sigma, n) := (n/(sigma*sqrt(2*pi))) * exp((-(x-mu)^2)/(2*sigma^2))"], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "N(x | mu, sigma, n) := (n/(sigma*sqrt(2*pi))) * exp((-(x-mu)^2)/(2*sigma^2))", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["(read that as \"the normal distribution of x given sigma, mu, and n is given by...\") So far, so good; this matches the function you've got.", "Notice that the only true variable here is x: the other three parameters are fixed for any particular Gaussian."], "childNum": 7, "tag": "p", "senID": 3, "childList": [{"text": "x", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "sigma", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "mu", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "n", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "true variable", "childNum": 0, "tag": "em", "childList": []}, {"text": "x", "childNum": 0, "tag": "code", "childList": []}, {"text": "fixed", "childNum": 0, "tag": "em", "childList": []}]}, {"text": ["Now for a mathematical fact: it is provably true that all Gaussian curves have the same shape, they're just shifted around a little bit.", "So we can work with N(x|0,1,1), called the \"standard normal distribution\", and just translate our results back to the general Gaussian curve.", "So if you have the integral of N(x|0,1,1), you can trivially calculate the integral of any Gaussian.", "This integral appears so frequently that it has a special name: the error function erf.", "Because of some old conventions, it's not exactly erf; there are a couple additive and multiplicative factors also being carried around. "], "childNum": 6, "tag": "p", "senID": 4, "childList": [{"text": "N(x|0,1,1)", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "N(x|0,1,1)", "childNum": 0, "tag": "code", "pos": 2, "childList": []}, {"text": "error function", "childNum": 0, "tag": "em", "pos": 3, "childList": []}, {"text": "erf", "childNum": 0, "tag": "code", "pos": 4, "childList": []}, {"text": "exactly", "childNum": 0, "tag": "em", "childList": []}, {"text": "erf", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["If Phi(z) = integral(N(x|0,1,1), -inf, z); that is, Phi(z) is the integral of the standard normal distribution from minus infinity up to z, then it's true by the definition of the error function that"], "childNum": 3, "tag": "p", "senID": 5, "childList": [{"text": "Phi(z) = integral(N(x|0,1,1), -inf, z)", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "Phi(z)", "childNum": 0, "tag": "code", "childList": []}, {"text": "z", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["Phi(z) = 0.5 + 0.5 * erf(z / sqrt(2))."], "childNum": 1, "tag": "p", "senID": 6, "childList": [{"text": "Phi(z) = 0.5 + 0.5 * erf(z / sqrt(2))", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Likewise, if Phi(z | mu, sigma, n) = integral( N(x|sigma, mu, n), -inf, z); that is, Phi(z | mu, sigma, n) is the integral of the normal distribution given parameters mu, sigma, and n from minus infinity up to z, then it's true by the definition of the error function that"], "childNum": 6, "tag": "p", "senID": 7, "childList": [{"text": "Phi(z | mu, sigma, n) = integral( N(x|sigma, mu, n), -inf, z)", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "Phi(z | mu, sigma, n)", "childNum": 0, "tag": "code", "childList": []}, {"text": "mu", "childNum": 0, "tag": "code", "childList": []}, {"text": "sigma", "childNum": 0, "tag": "code", "childList": []}, {"text": "n", "childNum": 0, "tag": "code", "childList": []}, {"text": "z", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["Phi(z | mu, sigma, n) = (n/2) * (1 + erf((x - mu) / (sigma * sqrt(2))))."], "childNum": 1, "tag": "p", "senID": 8, "childList": [{"text": "Phi(z | mu, sigma, n) = (n/2) * (1 + erf((x - mu) / (sigma * sqrt(2))))", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Take a look at the Wikipedia article on the normal CDF if you want more detail or a proof of this fact."], "childNum": 1, "tag": "p", "senID": 9, "childList": [{"text": "the Wikipedia article on the normal CDF", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function"}]}, {"text": ["Okay, that should be enough background explanation.", "Back to your (edited) post.", "You say \"The erf(z) in scipy.special would require me to define exactly what t is initially\".", "I have no idea what you mean by this; where does t (time?", ") enter into this at all?", "Hopefully the explanation above has demystified the error function a bit and it's clearer now as to why the error function is the right function for the job."], "childNum": 1, "tag": "p", "senID": 10, "childList": [{"text": "t", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Your Python code is OK, but I would prefer a closure over a lambda:"], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"code": "<pre>\n<code>\n def make_gauss(N, sigma, mu):\n    k = N / (sigma * math.sqrt(2*math.pi))\n    s = -1.0 / (2 * sigma * sigma)\n    def f(x):\n        return k * math.exp(s * (x - mu)*(x - mu))\nreturn f\n</code>\n</pre>\n", "senID": 12}, {"text": ["Using a closure enables precomputation of constants k and s, so the returned function will need to do less work each time it's called (which can be important if you're integrating it, which means it'll be called many times).", "Also, I have avoided any use of the exponentiation operator **, which is slower than just writing the squaring out, and hoisted the divide out of the inner loop and replaced it with a multiply.", "I haven't looked at all at their implementation in Python, but from my last time tuning an inner loop for pure speed using raw x87 assembly, I seem to remember that adds, subtracts, or multiplies take about 4 CPU cycles each, divides about 36, and exponentiation about 200.", "That was a couple years ago, so take those numbers with a grain of salt; still, it illustrates their relative complexity.", "As well, calculating exp(x) the brute-force way is a very bad idea; there are tricks you can take when writing a good implementation of exp(x) that make it significantly faster and more accurate than a general a**b style exponentiation."], "childNum": 6, "tag": "p", "senID": 13, "childList": [{"text": "k", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "s", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "**", "childNum": 0, "tag": "code", "pos": 4, "childList": []}, {"text": "exp(x)", "childNum": 0, "tag": "code", "childList": []}, {"text": "exp(x)", "childNum": 0, "tag": "code", "childList": []}, {"text": "a**b", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["I've never used the numpy version of the constants pi and e; I've always stuck with the plain old math module's versions.", "I don't know why you might prefer either one."], "childNum": 0, "tag": "p", "senID": 14, "childList": []}, {"text": ["I'm not sure what you're going for with the quad() call.", "quad(gen_gauss, -inf, inf, (10,2,0)) ought to integrate a renormalized Gaussian from minus infinity to plus infinity, and should always spit out 10 (your normalization factor), since the Gaussian integrates to 1 over the real line.", "Any answer far from 10 (I wouldn't expect exactly 10 since quad() is only an approximation, after all) means something is screwed up somewhere... hard to say what's screwed up without knowing the actual return value and possibly the inner workings of quad()."], "childNum": 5, "tag": "p", "senID": 15, "childList": [{"text": "quad()", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "quad(gen_gauss, -inf, inf, (10,2,0))", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "exactly", "childNum": 0, "tag": "em", "pos": 2, "childList": []}, {"text": "quad()", "childNum": 0, "tag": "code", "childList": []}, {"text": "quad()", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["Hopefully that has demystified some of the confusion, and explained why the error function  is the right answer to your problem, as well as how to do it all yourself if you're curious.", "If any of my explanation wasn't clear, I suggest taking a quick look at Wikipedia first; if you still have questions, don't hesitate to ask. "], "childNum": 0, "tag": "p", "senID": 16, "childList": []}], [{"text": ["scipy ships with the \"error function\", aka Gaussian integral:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n import scipy.special\nhelp(scipy.special.erf)\n</code>\n</pre>\n", "senID": 1}], [{"text": ["I assume you're handling multivariate Gaussians; if so, SciPy already has the function you're looking for: it's called MVNDIST (\"MultiVariate Normal DISTribution).", "The SciPy documentation is, as ever, terrible, so I can't even find where the function is buried, but it's in there somewhere.", "The documentation is easily the worst part of SciPy, and has frustrated me to no end in the past."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "it's in there somewhere", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://www.google.com/search?q=site%3Ascipy.org+MVNDST+&ie=utf-8&oe=utf-8&aq=t"}]}, {"text": ["Single-variable Gaussians just use the good old error function, of which many implementations are available."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["As for attacking the problem in general, yes, as James Thompson mentions, you just want to write your own gaussian distribution function and feed it to quad().", "If you can avoid the generalized integration, though, it's a good idea to do so -- specialized integration techniques for a particular function (like MVNDIST uses) are going to be much faster than a standard Monte Carlo multidimensional integration, which can be extremely slow for high accuracy."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Why not just always do your integration from -infinity to +infinity, so that you always know the answer?", "(joking!"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["My guess is that the only reason that there's not already a canned Gaussian function in SciPy is that it's a trivial function to write.", "Your suggestion about writing your own function and passing it to quad to integrate sounds excellent.", "It uses the accepted SciPy tool for doing this, it's minimal code effort for you, and it's very readable for other people even if they've never seen SciPy. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["What exactly do you mean by a fixed-width integrator?", "Do you mean using a different algorithm than whatever QUADPACK is using?"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Edit: For completeness, here's something like what I'd try for a Gaussian with the mean of 0 and standard deviation of 1 from 0 to +infinity:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n from scipy.integrate import quad\nfrom math import pi, exp\nmean = 0\nsd   = 1\nquad(lambda x: 1 / ( sd * ( 2 * pi ) ** 0.5 ) * exp( x ** 2 / (-2 * sd ** 2) ), 0, inf )\n</code>\n</pre>\n", "senID": 4}, {"text": ["That's a little ugly because the Gaussian function is a little long, but still pretty trivial to write. "], "childNum": 0, "tag": "p", "senID": 5, "childList": []}], [{"text": ["The gaussian distribution is also called a normal distribution.", "The cdf function in the scipy norm module does what you want."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["from scipy.stats import norm"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["print norm.cdf(0.0)"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["0.5"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm"], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm"}]}], [{"text": ["use the attachment in http://projects.scipy.org/scipy/ticket/846"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://projects.scipy.org/scipy/ticket/846", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://projects.scipy.org/scipy/ticket/846"}]}], [{"text": ["Is there a python function to get the expectation of a function over a multivariate normal distribution? "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["That is, is there a python integration routine (over infinite limits) that explicitly and efficiently makes use of the fact that the distribution is multivariate normal when integrating over a user-specified function?"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}]]