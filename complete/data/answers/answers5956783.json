[[{"text": ["Operating with Python objects in a heavy loop like that, whether they are float, np.float32, is always slow.", "NumPy is fast for operations on vectors and matrices, because all of the operations are performed on big chunks of data by parts of the library written in C, and not by the Python interpreter.", "Code run in the interpreter and/or using Python objects is always slow, and using non-native types makes it even slower.", "That's to be expected."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "float", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "np.float32", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"text": ["If your app is slow and you need to optimize it, you should try either converting your code to a vector solution that uses NumPy directly, and is fast, or you could use tools such as Cython to create a fast implementation of the loop in C."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["CPython floats are allocated in chunks"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "CPython floats are allocated in chunks", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["The key problem with comparing numpy scalar allocations to the float type is that CPython always allocates the memory for float and int objects in blocks of size N."], "childNum": 3, "tag": "p", "senID": 1, "childList": [{"text": "float", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "float", "childNum": 0, "tag": "code", "childList": []}, {"text": "int", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["Internally, CPython maintains a linked list of blocks each large enough to hold N float objects.", "When you call float(1) CPython checks if there is space available in the current block; if not it allocates a new block.", "Once it has space in the current block it simply initializes that space and returns a pointer to it. "], "childNum": 2, "tag": "p", "senID": 2, "childList": [{"text": "float", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "float(1)", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}, {"text": ["On my machine each block can hold 41 float objects, so there is some overhead for the first float(1) call but the next 40 run much faster as the memory is allocated and ready."], "childNum": 2, "tag": "p", "senID": 3, "childList": [{"text": "float", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "float(1)", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["Slow numpy.float32 vs. numpy.float64"], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "Slow numpy.float32 vs. numpy.float64", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["It appears that numpy has 2 paths it can take when creating a scalar type: fast and slow.", "This depends on whether the scalar type has a Python base class to which it can defer for argument conversion."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["For some reason numpy.float32 is hard-coded to take the slower path (defined by the_WORK0macro), while numpy.float64 gets a chance to take the faster path (defined by the_WORK1macro).", "Note that scalartypes.c.src is a template which generates scalartypes.c at build time."], "childNum": 8, "tag": "p", "senID": 6, "childList": [{"text": "numpy.float32", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "(defined by the_WORK0macro)", "tag": "a", "pos": -1, "childList": [{"text": "_WORK0", "tag": "code"}], "childNum": 1, "href": "https://github.com/numpy/numpy/blob/v1.5.1/numpy/core/src/multiarray/scalartypes.c.src#L2101"}, {"text": "_WORK0", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "numpy.float64", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "(defined by the_WORK1macro)", "tag": "a", "pos": -1, "childList": [{"text": "_WORK1", "tag": "code"}], "childNum": 1, "href": "https://github.com/numpy/numpy/blob/v1.5.1/numpy/core/src/multiarray/scalartypes.c.src#L2101"}, {"text": "_WORK1", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "scalartypes.c.src", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "scalartypes.c", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["You can visualize this in Cachegrind.", "I've included screen captures showing how many more calls are made to construct a float32 vs. float64:"], "childNum": 2, "tag": "p", "senID": 7, "childList": [{"text": "float32", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "float64", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["float64takes the fast path"], "childNum": 2, "tag": "p", "senID": 8, "childList": [{"text": "float64takes the fast path", "childNum": 1, "tag": "strong", "pos": 0, "childList": [{"text": "float64", "tag": "code"}]}, {"text": "float64", "childNum": 0, "tag": "code", "childList": []}]}, {"src": "http://i.stack.imgur.com/AWDLT.png", "tag": "img", "senID": 9}, {"text": ["float32takes the slow path"], "childNum": 2, "tag": "p", "senID": 10, "childList": [{"text": "float32takes the slow path", "childNum": 1, "tag": "strong", "pos": 0, "childList": [{"text": "float32", "tag": "code"}]}, {"text": "float32", "childNum": 0, "tag": "code", "childList": []}]}, {"src": "http://i.stack.imgur.com/WgmjQ.png", "tag": "img", "senID": 11}, {"text": ["Updated - Which type takes the slow/fast path may depend on whether the OS is 32-bit vs 64-bit.", "On my test system, Ubuntu Lucid 64-bit, the float64 type is 10 times faster than float32."], "childNum": 3, "tag": "p", "senID": 12, "childList": [{"text": "Updated", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "float64", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "float32", "childNum": 0, "tag": "code", "childList": []}]}], [{"text": ["Perhaps, that is why you should use Numpy directly instead of using loops."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n s1 = np.ones(10000000, dtype=np.float)\ns2 = np.ones(10000000, dtype=np.float32)\ns3 = np.ones(10000000, dtype=np.float64)\n\nnp.sum(s1) &lt;-- 17.3 ms\nnp.sum(s2) &lt;-- 15.8 ms\nnp.sum(s3) &lt;-- 17.3 ms\n</code>\n</pre>\n", "senID": 1}], [{"text": ["If you're after fast scalar arithmetic, you should be looking at libraries like gmpy rather than numpy (as others have noted, the latter is optimised more for vector operations rather than scalar ones)."], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "gmpy", "tag": "a", "pos": 0, "childList": [{"text": "gmpy", "tag": "code"}], "childNum": 1, "href": "https://code.google.com/p/gmpy/"}, {"text": "gmpy", "childNum": 0, "tag": "code", "childList": []}, {"text": "numpy", "childNum": 0, "tag": "code", "childList": []}]}], [{"text": ["I can confirm the results also.", "I tried to see what it would look like using all numpy types, and the difference persists.", "So then, my tests were:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n def testStandard(length=100000):\n    s = 1.0\n    addend = 8.0\n    modulo = 2399232.0\n    startTime = datetime.now()\n    for i in xrange(length):\n        s = (s + addend) * s % modulo\n    return datetime.now() - startTime\n\ndef testNumpy(length=100000):\n    s = np.float64(1.0)\n    addend = np.float64(8.0)\n    modulo = np.float64(2399232.0)\n    startTime = datetime.now()\n    for i in xrange(length):\n        s = (s + addend) * s % modulo\n    return datetime.now() - startTime\n</code>\n</pre>\n", "senID": 1}, {"text": ["So at this point, the numpy types are all interacting with each other, but the 10x difference persists (2 sec vs 0.2 sec)."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["If I had to guess, I would say that there are two possible reasons for why the default float types are much faster.", "The first possibility is that python performs significant optimizations under the hood for dealing with certain numeric operations or looping in general (e.g.", "loop unrolling).", "The second possibility is that the numpy types involves an extra layer of abstraction (i.e.", "having to read from an address).", "To look into the effects of each, I did a few extra checks."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["One difference could be the result of python having to take extra steps to resolve the float64 types.", "Unlike compiled languages that generate efficient tables, python 2.6 (and maybe 3) has a significant cost for resolving things that you'd generally think of as free.", "Even a simple X.a resolution has to resolve the dot operator EVERY time it is called.", "(Which is why if you have a loop that calls instance.function() you're better off having a variable \"function = instance.function\" declared outside the loop)."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["From my understanding, when you use python standard operators, these are fairly similar to using the ones from \"import operator.", "\"  If you substitute add, mul, and mod in for your +, *, and %, you see a static performance hit of about 0.5 sec versus the standard operators (to both cases).", "This means that by wrapping the operators, the standard python float operations get 3x slower.", "If you do one further, using operator.add and those variants adds on 0.7 sec approximately (over 1m trials, starting with 2 sec and 0.2 sec respectively).", "That's verging on the 5x slowness.", "So basically, if each of these issues happens twice, you're basically at the 10x slower point."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["So let's assume we're the python interpreter for a moment.", "Case 1, we do an operation on native types, let's say a+b.", "Under the hood, we can check the types of a and b and dispatch our addition to python's optimized code.", "Case 2, we have an operation of two other types (also a+b).", "Under the hood, we check if they're native types (they're not).", "We move on to the 'else' case.", "The else case sends us to something like a.add(b).", "a.add can then do a dispatch to numpy's optimized code.", "So at this point we have had additional overhead of an extra branch, one '.", "' get slots property, and a function call.", "And we've only got into the addition operation.", "We then have to use the result to create a new float64 (or alter an existing float64).", "Meanwhile, the python native code probably cheats by treating its types specially to avoid this sort of overhead.  "], "childNum": 2, "tag": "p", "senID": 6, "childList": [{"text": "add", "childNum": 0, "tag": "strong", "pos": 2, "childList": []}, {"text": "add", "childNum": 0, "tag": "strong", "pos": 6, "childList": []}]}, {"text": ["Based on the above examination of the costliness of python function calls and scoping overhead, it would be pretty easy for numpy to incur a 9x penalty just getting to and from its c math functions.", "I can entirely imagine this process taking many times longer than a simple math operation call.", "For each operation, the numpy library will have to wade through layers of python to get to its C implementation."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["So in my opinion, the reason for this is probably captured in this effect:"], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"code": "<pre>\n<code>\n length = 10000000\nclass A():\n    X = 10\nstartTime = datetime.now()\nfor i in xrange(length):\n    x = A.X\nprint \"Long Way\", datetime.now() - startTime\nstartTime = datetime.now()\ny = A.X\nfor i in xrange(length):\n    x = y\nprint \"Short Way\", datetime.now() - startTime\n</code>\n</pre>\n", "senID": 9}, {"text": ["This simple case shows a difference of 0.2 sec vs 0.14 sec (short way faster, obviously).", "I think what you're seeing is mainly just a bunch of those issues adding up."], "childNum": 0, "tag": "p", "senID": 10, "childList": []}, {"text": ["To avoid this, I can think of a a couple possible solutions that mainly echo what has been said.", "The first solution is to try to keep your evaluations inside NumPy as much as possible, as Selinap said.", "A large amount of the losses are probably due to the interfacing.", "I would look into ways to dispatch your job into numpy or some other numeric library optimized in C (gmpy has been mentioned).", "The goal should be to push as much into C at the same time as possible, then get the result(s) back.", "You want to put in big jobs, not lots of small jobs."], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"text": ["The second solution, of course, would be to do more of your intermediate and small operations in python if you can.", "Clearly, using the native objects are going to be faster.", "They're going to be the first options on all the branch statements and will always have the shortest path to C code.", "Unless you have a specific need for fixed precision calculation or other issues with the default operators, I don't see why one wouldn't use the straight python functions for many things."], "childNum": 0, "tag": "p", "senID": 12, "childList": []}], [{"text": ["Really strange...I confirm the results in Ubuntu 11.04 32bit, python 2.7.1, numpy 1.5.1 (official packages):"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n import numpy as np\ndef testfloat():\n    s = 0\n    for i in range(10000000):  \n        s+= float(1)\ndef testfloat32():\n    s = 0\n    for i in range(10000000):  \n        s+= np.float32(1)\ndef testfloat64():\n    s = 0\n    for i in range(10000000):  \n        s+= np.float64(1)\n\n%time testfloat()\nCPU times: user 4.66 s, sys: 0.06 s, total: 4.73 s\nWall time: 4.74 s\n\n%time testfloat64()\nCPU times: user 11.43 s, sys: 0.07 s, total: 11.50 s\nWall time: 11.57 s\n\n\n%time testfloat32()\nCPU times: user 47.99 s, sys: 0.09 s, total: 48.08 s\nWall time: 48.23 s\n</code>\n</pre>\n", "senID": 1}, {"text": ["I don't see why float32 should be 5 times slower that float64."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}]]