[[{"text": ["To write a lazy function, just use yield:"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "yield", "tag": "a", "pos": 0, "childList": [{"text": "yield", "tag": "code"}], "childNum": 1, "href": "http://docs.python.org/tutorial/classes.html#generators"}, {"text": "yield", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n def read_in_chunks(file_object, chunk_size=1024):\n    \"\"\"Lazy function (generator) to read a file piece by piece.\n    Default chunk size: 1k.\"\"\"\n    while True:\n        data = file_object.read(chunk_size)\n        if not data:\n            break\n        yield data\n\n\nf = open('really_big_file.dat')\nfor piece in read_in_chunks(f):\n    process_data(piece)\n</code>\n</pre>\n", "senID": 1}, {"tag": "hr", "senID": 2}, {"text": ["Another option would be to use iter and a helper function:"], "childNum": 2, "tag": "p", "senID": 3, "childList": [{"text": "iter", "tag": "a", "pos": 0, "childList": [{"text": "iter", "tag": "code"}], "childNum": 1, "href": "http://docs.python.org/library/functions.html#iter"}, {"text": "iter", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n f = open('really_big_file.dat')\ndef read1k():\n    return f.read(1024)\n\nfor piece in iter(read1k, ''):\n    process_data(piece)\n</code>\n</pre>\n", "senID": 4}, {"tag": "hr", "senID": 5}, {"text": ["If the file is line-based, the file object is already a lazy generator of lines:"], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"code": "<pre>\n<code>\n for line in open('really_big_file.dat'):\n    process_data(line)\n</code>\n</pre>\n", "senID": 7}], [{"text": ["You can use the mmap module to map the contents of the file into memory and access it with indices and slices.", "Here an example from the documentation:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "mmap module", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/mmap.html"}]}, {"code": "<pre>\n<code>\n import mmap\nwith open(\"hello.txt\", \"r+\") as f:\n    # memory-map the file, size 0 means whole file\n    map = mmap.mmap(f.fileno(), 0)\n    # read content via standard file methods\n    print map.readline()  # prints \"Hello Python!\"\n    # read content via slice notation\n    print map[:5]  # prints \"Hello\"\n    # update content using slice notation;\n    # note that new content must have same size\n    map[6:] = \" world!\\n\"\n    # ... and read again using standard file methods\n    map.seek(0)\n    print map.readline()  # prints \"Hello  world!\"\n    # close the map\n    map.close()\n</code>\n</pre>\n", "senID": 1}], [{"text": ["file.readlines() takes in an optional size argument which approximates the number of lines read in the lines returned."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n bigfile = open('bigfilename','r')\ntmp_lines = bigfile.readlines(BUF_SIZE)\nwhile tmp_lines:\n    process([line for line in tmp_lines])\n    tmp_lines = bigfile.readlines(BUF_SIZE)\n</code>\n</pre>\n", "senID": 1}], [{"text": ["Take a look at this post on Neopythonic: \"Sorting a million 32-bit integers in 2MB of RAM using Python\""], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "this post on Neopythonic", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://neopythonic.blogspot.com/2008/10/sorting-million-32-bit-integers-in-2mb.html"}]}], [{"text": ["i am not allowed to comment due to my low reputation, but SilentGhosts solution should be much easier with file.readlines([sizehint])"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["python file methods"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "python file methods", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/stdtypes.html#file-objects"}]}, {"text": ["edit: SilentGhost is right, but this should be better than:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n s = \"\" \nfor i in xrange(100): \n   s += file.next()\n</code>\n</pre>\n", "senID": 3}], [{"text": ["I'm in a somewhat similar situation.", "It's not clear whether you know chunk size in bytes; I usually don't, but the number of records (lines) that is required is known:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n def get_line():\n     with open('4gb_file') as file:\n         for i in file:\n             yield i\n\nlines_required = 100\ngen = get_line()\nchunk = [i for i, j in zip(gen, range(lines_required))]\n</code>\n</pre>\n", "senID": 1}, {"text": ["Update: Thanks nosklo.", "Here's what I meant.", "It almost works, except that it loses a line 'between' chunks."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "Update", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n chunk = [next(gen) for i in range(lines_required)]\n</code>\n</pre>\n", "senID": 3}, {"text": ["Does the trick w/o losing any lines, but it doesn't look very nice."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}]]