[[{"tag": "ul", "num": 4, "lis": [{"text": ["Mechanize"], "childNum": 0, "tag": "a", "senID": 0, "childList": []}, {"text": ["Twill"], "childNum": 0, "tag": "a", "senID": 1, "childList": []}, {"text": ["BeautifulSoup"], "childNum": 0, "tag": "a", "senID": 2, "childList": []}, {"text": ["Scrapy"], "childNum": 0, "tag": "a", "senID": 3, "childList": []}]}], [{"text": ["Use Scrapy."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Scrapy", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://scrapy.org/"}]}, {"text": ["It is a twisted-based web crawler framework.", "Still under heavy development but it works already.", "Has many goodies:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"tag": "ul", "num": 7, "lis": [{"text": "Built-in support for parsing HTML, XML, CSV, and Javascript", "tag": "none", "senID": 2}, {"text": "A media pipeline for scraping items with images (or any other media) and download the image files as well", "tag": "none", "senID": 3}, {"text": "Support for extending Scrapy by plugging your own functionality using middlewares, extensions, and pipelines", "tag": "none", "senID": 4}, {"text": "Wide range of built-in middlewares and extensions for handling of compression, cache, cookies, authentication, user-agent spoofing, robots.txt handling, statistics, crawl depth restriction, etc", "tag": "none", "senID": 5}, {"text": "Interactive scraping shell console, very useful for developing and debugging", "tag": "none", "senID": 6}, {"text": "Web management console for monitoring and controlling your bot", "tag": "none", "senID": 7}, {"text": "Telnet console for low-level access to the Scrapy process", "tag": "none", "senID": 8}]}, {"text": ["Example code to extract information about all torrent files added today in the mininova torrent site, by using a XPath selector on the HTML returned:"], "childNum": 1, "tag": "p", "senID": 9, "childList": [{"text": "mininova", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.mininova.org/"}]}, {"code": "<pre>\n<code>\n class Torrent(ScrapedItem):\n    pass\n\nclass MininovaSpider(CrawlSpider):\n    domain_name = 'mininova.org'\n    start_urls = ['http://www.mininova.org/today']\n    rules = [Rule(RegexLinkExtractor(allow=['/tor/\\d+']), 'parse_torrent')]\n\n    def parse_torrent(self, response):\n        x = HtmlXPathSelector(response)\n        torrent = Torrent()\n\n        torrent.url = response.url\n        torrent.name = x.x(\"//h1/text()\").extract()\n        torrent.description = x.x(\"//div[@id='description']\").extract()\n        torrent.size = x.x(\"//div[@id='info-left']/p[2]/text()[2]\").extract()\n        return [torrent]\n</code>\n</pre>\n", "senID": 10}], [{"text": ["Check the HarvestMan, a multi-threaded web-crawler written in Python, also give a look to the spider.py module."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "HarvestMan", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://bulba.sdsu.edu/docwiki/HarvestMan"}, {"href": "http://pypi.python.org/pypi/spider.py/0.5", "text": "spider.py", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["And here you can find code samples to build a simple web-crawler."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "here", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.example-code.com/python/pythonspider.asp"}]}], [{"text": ["I hacked the above script to include a login page as I needed it to access a drupal site.", "Not pretty but may help someone out there."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n #!/usr/bin/python\n\nimport httplib2\nimport urllib\nimport urllib2\nfrom cookielib import CookieJar\nimport sys\nimport re\nfrom HTMLParser import HTMLParser\n\nclass miniHTMLParser( HTMLParser ):\n\n  viewedQueue = []\n  instQueue = []\n  headers = {}\n  opener = \"\"\n\n  def get_next_link( self ):\n    if self.instQueue == []:\n      return ''\n    else:\n      return self.instQueue.pop(0)\n\n\n  def gethtmlfile( self, site, page ):\n    try:\n        url = 'http://'+site+''+page\n        response = self.opener.open(url)\n        return response.read()\n    except Exception, err:\n        print \" Error retrieving: \"+page\n        sys.stderr.write('ERROR: %s\\n' % str(err))\n    return \"\" \n\n    return resppage\n\n  def loginSite( self, site_url ):\n    try:\n    cj = CookieJar()\n    self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n\n    url = 'http://'+site_url \n        params = {'name': 'customer_admin', 'pass': 'customer_admin123', 'opt': 'Log in', 'form_build_id': 'form-3560fb42948a06b01d063de48aa216ab', 'form_id':'user_login_block'}\n    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'\n    self.headers = { 'User-Agent' : user_agent }\n\n    data = urllib.urlencode(params)\n    response = self.opener.open(url, data)\n    print \"Logged in\"\n    return response.read() \n\n    except Exception, err:\n    print \" Error logging in\"\n    sys.stderr.write('ERROR: %s\\n' % str(err))\n\n    return 1\n\n  def handle_starttag( self, tag, attrs ):\n    if tag == 'a':\n      newstr = str(attrs[0][1])\n      print newstr\n      if re.search('http', newstr) == None:\n        if re.search('mailto', newstr) == None:\n          if re.search('#', newstr) == None:\n            if (newstr in self.viewedQueue) == False:\n              print \"  adding\", newstr\n              self.instQueue.append( newstr )\n              self.viewedQueue.append( newstr )\n          else:\n            print \"  ignoring\", newstr\n        else:\n          print \"  ignoring\", newstr\n      else:\n        print \"  ignoring\", newstr\n\n\ndef main():\n\n  if len(sys.argv)!=3:\n    print \"usage is ./minispider.py site link\"\n    sys.exit(2)\n\n  mySpider = miniHTMLParser()\n\n  site = sys.argv[1]\n  link = sys.argv[2]\n\n  url_login_link = site+\"/node?destination=node\"\n  print \"\\nLogging in\", url_login_link\n  x = mySpider.loginSite( url_login_link )\n\n  while link != '':\n\n    print \"\\nChecking link \", link\n\n    # Get the file from the site and link\n    retfile = mySpider.gethtmlfile( site, link )\n\n    # Feed the file into the HTML parser\n    mySpider.feed(retfile)\n\n    # Search the retfile here\n\n    # Get the next link in level traversal order\n    link = mySpider.get_next_link()\n\n  mySpider.close()\n\n  print \"\\ndone\\n\"\n\nif __name__ == \"__main__\":\n  main()\n</code>\n</pre>\n", "senID": 1}], [{"text": ["I've used Ruya and found it pretty good."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Ruya", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://ruya.sourceforge.net/"}]}], [{"text": ["Another simple spider \nUses BeautifulSoup and urllib2.", "Nothing too sophisticated, just reads all a href's builds a list and goes though it."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "simple spider", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.grenadepod.com/2009/12/13/python-web-crawler/"}]}], [{"text": ["Trust me nothing is better than curl.. .", "the following code can crawl 10,000 urls in parallel in less than 300 secs on Amazon EC2"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["CAUTION: Don't hit the same domain at such a high speed.. ."], "childNum": 2, "tag": "p", "senID": 1, "childList": [{"text": "CAUTION:", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "Don't hit the same domain at such a high speed.. .", "childNum": 0, "tag": "em", "childList": []}]}, {"code": "<pre>\n<code>\n #! /usr/bin/env python\n# -*- coding: iso-8859-1 -*-\n# vi:ts=4:et\n# $Id: retriever-multi.py,v 1.29 2005/07/28 11:04:13 mfx Exp $\n\n#\n# Usage: python retriever-multi.py &lt;file with URLs to fetch&gt; [&lt;# of\n#          concurrent connections&gt;]\n#\n\nimport sys\nimport pycurl\n\n# We should ignore SIGPIPE when using pycurl.NOSIGNAL - see\n# the libcurl tutorial for more info.\ntry:\n    import signal\n    from signal import SIGPIPE, SIG_IGN\n    signal.signal(signal.SIGPIPE, signal.SIG_IGN)\nexcept ImportError:\n    pass\n\n\n# Get args\nnum_conn = 10\ntry:\n    if sys.argv[1] == \"-\":\n        urls = sys.stdin.readlines()\n    else:\n        urls = open(sys.argv[1]).readlines()\n    if len(sys.argv) &gt;= 3:\n        num_conn = int(sys.argv[2])\nexcept:\n    print \"Usage: %s &lt;file with URLs to fetch&gt; [&lt;# of concurrent connections&gt;]\" % sys.argv[0]\n    raise SystemExit\n\n\n# Make a queue with (url, filename) tuples\nqueue = []\nfor url in urls:\n    url = url.strip()\n    if not url or url[0] == \"#\":\n        continue\n    filename = \"doc_%03d.dat\" % (len(queue) + 1)\n    queue.append((url, filename))\n\n\n# Check args\nassert queue, \"no URLs given\"\nnum_urls = len(queue)\nnum_conn = min(num_conn, num_urls)\nassert 1 &lt;= num_conn &lt;= 10000, \"invalid number of concurrent connections\"\nprint \"PycURL %s (compiled against 0x%x)\" % (pycurl.version, pycurl.COMPILE_LIBCURL_VERSION_NUM)\nprint \"----- Getting\", num_urls, \"URLs using\", num_conn, \"connections -----\"\n\n\n# Pre-allocate a list of curl objects\nm = pycurl.CurlMulti()\nm.handles = []\nfor i in range(num_conn):\n    c = pycurl.Curl()\n    c.fp = None\n    c.setopt(pycurl.FOLLOWLOCATION, 1)\n    c.setopt(pycurl.MAXREDIRS, 5)\n    c.setopt(pycurl.CONNECTTIMEOUT, 30)\n    c.setopt(pycurl.TIMEOUT, 300)\n    c.setopt(pycurl.NOSIGNAL, 1)\n    m.handles.append(c)\n\n\n# Main loop\nfreelist = m.handles[:]\nnum_processed = 0\nwhile num_processed &lt; num_urls:\n    # If there is an url to process and a free curl object, add to multi stack\n    while queue and freelist:\n        url, filename = queue.pop(0)\n        c = freelist.pop()\n        c.fp = open(filename, \"wb\")\n        c.setopt(pycurl.URL, url)\n        c.setopt(pycurl.WRITEDATA, c.fp)\n        m.add_handle(c)\n        # store some info\n        c.filename = filename\n        c.url = url\n    # Run the internal curl state machine for the multi stack\n    while 1:\n        ret, num_handles = m.perform()\n        if ret != pycurl.E_CALL_MULTI_PERFORM:\n            break\n    # Check for curl objects which have terminated, and add them to the freelist\n    while 1:\n        num_q, ok_list, err_list = m.info_read()\n        for c in ok_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print \"Success:\", c.filename, c.url, c.getinfo(pycurl.EFFECTIVE_URL)\n            freelist.append(c)\n        for c, errno, errmsg in err_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print \"Failed: \", c.filename, c.url, errno, errmsg\n            freelist.append(c)\n        num_processed = num_processed + len(ok_list) + len(err_list)\n        if num_q == 0:\n            break\n    # Currently no more I/O is pending, could do something in the meantime\n    # (display a progress bar, etc.).\n    # We just call select() to sleep until some more data is available.\n    m.select(1.0)\n\n\n# Cleanup\nfor c in m.handles:\n    if c.fp is not None:\n        c.fp.close()\n        c.fp = None\n    c.close()\nm.close()\n</code>\n</pre>\n", "senID": 2}], [{"text": ["pyspider.py"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "pyspider.py", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://bauerdata.bauerhost.dk/python-program-eksempler/pyspider"}]}]]