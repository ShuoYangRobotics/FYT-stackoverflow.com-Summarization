[[{"text": ["Don't worry about GIL.", "In your case it doesn't matter."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Easiest way to do what you want is to create thread pool, using threading module and one of thread pool implementations from ASPN.", "Each thread from that pool can use httplib to download your web pages."], "childNum": 3, "tag": "p", "senID": 1, "childList": [{"text": "threading", "childNum": 0, "tag": "em", "pos": 0, "childList": []}, {"text": "ASPN", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://code.activestate.com/recipes/langs/python/tags/pool/"}, {"text": "httplib", "childNum": 0, "tag": "em", "pos": 1, "childList": []}]}, {"text": ["Another option is to use PyCURL module -- it supports parallel downlaods natively, so you don't have to implement it yourself."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "PyCURL", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://pycurl.cvs.sourceforge.net/"}]}], [{"text": ["GIL prevents you from effectively doing processor load balancing with threads.", "As this is not processor loads balancing but preventing one IO wait from stopping the whole download, the GIL is not relevant here.", "*)"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["So all you need to do is create several processes that download at the same time.", "You can do that with the threading module or the multiprocessing module."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["*) Well... unless you have Gigabit connections and your problem is really that your processor gets overloaded before your net does.", "But that's obviously not the case here."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["I recently solved this same problem.", "One thing to consider is that some people don't take kindly to having their servers bogged down and will block an IP address that does so.", "The standard courtesy that I've heard is about 3 seconds between page requests, but this is flexible."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If you are downloading from multiple websites you can group your URLs by domain and create one thread per.", "Then in your thread you can do something like this:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n for url in urls:\n    timer = time.time()\n    # ... get your content ...\n    # perhaps put content in a queue to be written back to \n    # your database if it doesn't allow concurrent writes.\n    while time.time() - timer &lt; 3.0:\n        time.sleep(0.5)\n</code>\n</pre>\n", "senID": 2}, {"text": ["Sometimes just getting your response will take the full 3 seconds and you don't have to worry about it."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["Granted, this won't help you at all if you're only downloading from one site, but it may keep you from getting blocked."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["My machine handles about 200 threads before the overhead of managing them slowed the process down.", "I ended up at something like 40-50 pages per second."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}], [{"text": ["urllib &amp; threading (or multiprocessing) packages has all you need to do the \"spider\" you need."], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "urllib", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/urllib.html"}, {"href": "http://docs.python.org/library/threading.html#module-threading", "text": "threading", "childNum": 0, "tag": "a", "childList": []}, {"href": "http://docs.python.org/library/multiprocessing.html#module-multiprocessing", "text": "multiprocessing", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["What you have to do is get urls from DB, and for each url start a thread or process that \ngrabs the url. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["just as example (misses Data Base urls retrieving):"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n #!/usr/bin/env python\nimport Queue\nimport threading\nimport urllib2\nimport time\n\nhosts = [\"http://yahoo.com\", \"http://google.com\", \"http://amazon.com\",\n    \"http://ibm.com\", \"http://apple.com\"]\n\nqueue = Queue.Queue()\n\nclass ThreadUrl(threading.Thread):\n    \"\"\"Threaded Url Grab\"\"\"\n    def __init__(self, queue):\n        threading.Thread.__init__(self)\n        self.queue = queue\n\n    def run(self):\n        while True:\n            #grabs host from queue\n            host = self.queue.get()\n\n            #grabs urls of hosts and prints first 1024 bytes of page\n            url = urllib2.urlopen(host)\n            print url.read(1024)\n\n            #signals to queue job is done\n            self.queue.task_done()\n\nstart = time.time()\ndef main():\n\n    #spawn a pool of threads, and pass them queue instance\n    for i in range(5):\n        t = ThreadUrl(queue)\n        t.setDaemon(True)\n        t.start()\n\n    #populate queue with data\n    for host in hosts:\n        queue.put(host)\n\n    #wait on the queue until everything has been processed\n    queue.join()\n\nmain()\nprint \"Elapsed Time: %s\" % (time.time() - start)\n</code>\n</pre>\n", "senID": 3}], [{"text": ["You can have a look at the multiprocessing package which avoid the GIL lock problem."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "multiprocessing package", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/multiprocessing.html"}]}], [{"text": ["Downloading is IO, which may be done asynchronously using non-blocking sockets or twisted.", "Both of these solutions will be much more efficient than threading or multiprocessing."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["Nowadays there are excellent Python libs that do this for you - urllib3 and requests"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "urllib3", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://urllib3.readthedocs.org/"}, {"href": "http://docs.python-requests.org/", "text": "requests", "childNum": 0, "tag": "a", "childList": []}]}]]