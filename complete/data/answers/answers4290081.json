[[{"text": ["This is a complicated question, and there are no perfect answers.", "I'll try to give you an overview of the major concepts, and point you in the direction of some useful reading on the topic. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Assume that you a one dimensional set of data, and you have a finite set of probability distribution functions that you think the data may have been generated from.", "You can consider each distribution independently, and try to find parameters that are reasonable given your data.", "There are two methods for setting parameters for a probability distribution function given data:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["In my experience, Maximum Likelihood has been preferred in recent years, although this may not be the case in every field. "], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Here's a concrete example of how to estimate parameters in R. Consider a set of random points generated from a Gaussian distribution with mean of 0 and standard deviation of 1:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n x = rnorm( n = 100, mean = 0, sd = 1 )\n</code>\n</pre>\n", "senID": 4}, {"text": ["Assume that you know the data were generated using a Gaussian process, but you've forgotten (or never knew!", ") the parameters for the Gaussian.", "You'd like to use the data to give you reasonable estimates of the mean and standard deviation.", "In R, there is a standard library that makes this very straightforward:"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n library(MASS)\nparams = fitdistr( x, \"normal\" )\nprint( params )\n</code>\n</pre>\n", "senID": 6}, {"text": ["This gave me the following output:"], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"code": "<pre>\n<code>\n mean           sd     \n  -0.17922360    1.01636446 \n ( 0.10163645) ( 0.07186782)\n</code>\n</pre>\n", "senID": 8}, {"text": ["Those are fairly close to the right answer, and the numbers in parentheses are confidence intervals around the parameters.", "Remember that every time you generate a new set of points, you'll get a new answer for the estimates."], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"text": ["Mathematically, this is using maximum likelihood to estimate both the mean and standard deviation of the Gaussian.", "Likelihood means (in this case) \"probability of data given values of the parameters.", "\" Maximum likelihood means \"the values of the parameters that maximize the probability of generating my input data.", "\" Maximum likelihood estimation is the algorithm for finding the values of the parameters which maximize the probability of generating the input data, and for some distributions it can involve numerical optimization algorithms.", "In R, most of the work is done by fitdistr, which in certain cases will call optim."], "childNum": 3, "tag": "p", "senID": 10, "childList": [{"text": "numerical optimization", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Optimization_%28mathematics%29"}, {"text": "fitdistr", "tag": "a", "pos": 4, "childList": [], "childNum": 0, "href": "http://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html"}, {"href": "http://sekhon.berkeley.edu/stats/html/optim.html", "text": "optim", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["You can extract the log-likelihood from your parameters like this:"], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"code": "<pre>\n<code>\n print( params$loglik )\n[1] -139.5772\n</code>\n</pre>\n", "senID": 12}, {"text": ["It's more common to work with the log-likelihood rather than likelihood to avoid rounding errors.", "Estimating the joint probability of your data involves multiplying probabilities, which are all less than 1.", "Even for a small set of data, the joint probability approaches 0 very quickly, and adding the log-probabilities of your data is equivalent to multiplying the probabilities.", "The likelihood is maximized as the log-likelihood approaches 0, and thus more negative numbers are worse fits to your data. "], "childNum": 0, "tag": "p", "senID": 13, "childList": []}, {"text": ["With computational tools like this, it's easy to estimate parameters for any distribution.", "Consider this example:"], "childNum": 0, "tag": "p", "senID": 14, "childList": []}, {"code": "<pre>\n<code>\n x = x[ x &gt;= 0 ]\n\ndistributions = c(\"normal\",\"exponential\")\n\nfor ( dist in distributions ) {\n    print( paste( \"fitting parameters for \", dist ) )\n    params = fitdistr( x, dist )\n    print( params )\n    print( summary( params ) )\n    print( params$loglik )\n}\n</code>\n</pre>\n", "senID": 15}, {"text": ["The exponential distribution doesn't generate negative numbers, so I removed them in the first line.", "The output (which is stochastic) looked like this:"], "childNum": 0, "tag": "p", "senID": 16, "childList": []}, {"code": "<pre>\n<code>\n [1] \"fitting parameters for  normal\"\n      mean          sd    \n  0.72021836   0.54079027 \n (0.07647929) (0.05407903)\n         Length Class  Mode   \nestimate 2      -none- numeric\nsd       2      -none- numeric\nn        1      -none- numeric\nloglik   1      -none- numeric\n[1] -40.21074\n[1] \"fitting parameters for  exponential\"\n     rate  \n  1.388468 \n (0.196359)\n         Length Class  Mode   \nestimate 1      -none- numeric\nsd       1      -none- numeric\nn        1      -none- numeric\nloglik   1      -none- numeric\n[1] -33.58996\n</code>\n</pre>\n", "senID": 17}, {"text": ["The exponential distribution is actually slightly more likely to have generated this data than the normal distribution, likely because the exponential distribution doesn't have to assign any probability density to negative numbers."], "childNum": 0, "tag": "p", "senID": 18, "childList": []}, {"text": ["All of these estimation problems get worse when you try to fit your data to more distributions.", "Distributions with more parameters are more flexible, so they'll fit your data better than distributions with less parameters.", "Also, some distributions are special cases of other distributions (for example, the Exponential is a special case of the Gamma).", "Because of this, it's very common to use prior knowledge to constrain your choice models to a subset of all possible models."], "childNum": 2, "tag": "p", "senID": 19, "childList": [{"text": "Exponential", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Exponential_distribution"}, {"text": "Gamma", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Gamma_distribution"}]}, {"text": ["One trick to get around some problems in parameter estimation is to generate a lot of data, and leave some of the data out for cross-validation.", "To cross-validate your fit of parameters to data, leave some of the data out of your estimation procedure, and then measure each model's likelihood on the left-out data. "], "childNum": 1, "tag": "p", "senID": 20, "childList": [{"text": "cross-validation", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Cross-validation"}]}], [{"text": ["take a look at fitdistrplus (http://cran.r-project.org/web/packages/fitdistrplus/index.html).", "try the function descdist, which provides a plot of skew vs. kurtosis of the data and also shows some common distributions.", "fitdist allows you to fit any distributions you can define in terms of density and cdf.", "you can then use gofstat which computes the KS and AD stats which measure distance of the fit from the data."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["You are essentially wanting to compare your real world data to a set of theoretical distributions.", "There is the function qqnorm() in base R, which will do this for the normal distribution, but I prefer the probplot function in e1071 which allows you to test other distributions.", "Here is a code snippet that will plot your real data against each one of the theoretical distributions that we paste into the list.", "We use plyr to go through the list, but there are several other ways to go through the list as well."], "childNum": 4, "tag": "p", "senID": 0, "childList": [{"text": "qqnorm()", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "probplot", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "e1071", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "plyr", "childNum": 0, "tag": "code", "pos": 3, "childList": []}]}, {"code": "<pre>\n<code>\n library(\"plyr\") \nlibrary(\"e1071\")\n\nrealData &lt;- rnorm(1000) #Real data is normally distributed\n\ndistToTest &lt;- list(qnorm = \"qnorm\", lognormal = \"qlnorm\", qexp =  \"qexp\")\n\n#function to test real data against list of distributions above. Output is a jpeg for each distribution.\ntestDist &lt;- function(x, data){\n    jpeg(paste(x, \".jpeg\", sep = \"\"))\n    probplot(data, qdist = x)\n    dev.off()\n    }\n\nl_ply(distToTest, function(x) testDist(x, realData))\n</code>\n</pre>\n", "senID": 1}], [{"text": ["This is probably a bit more general than you need, but might give you something to go on."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["One way to estimate a probability density function from random data is to use an Edgeworth or Butterworth expansion.", "These approximations use density function properties known as cumulants (the unbiased estimators for which are the k-statistics) and express the density function as a perturbation from a Gaussian distribution."], "childNum": 4, "tag": "p", "senID": 1, "childList": [{"text": "cumulants", "tag": "a", "pos": 1, "childList": [{"text": "cumulants", "tag": "em"}], "childNum": 1, "href": "http://mathworld.wolfram.com/Cumulant.html"}, {"text": "cumulants", "childNum": 0, "tag": "em", "childList": []}, {"href": "http://mathworld.wolfram.com/k-Statistic.html", "text": "k-statistics", "childNum": 1, "tag": "a", "childList": [{"text": "k-statistics", "tag": "em"}]}, {"text": "k-statistics", "childNum": 0, "tag": "em", "childList": []}]}, {"text": ["These both have some rather dire weaknesses such as producing divergent density functions, or even density functions that are negative over some regions.", "However, some people find them useful for highly clustered data, or as starting points for further estimation, or for piecewise estimated density functions, or as part of a heuristic."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["M. G. Kendall and A. Stuart, The advanced theory of statistics, vol.", "1,\nCharles Griffin, 1963, was the most complete reference I found for this, with a whopping whole page dedicated to the topic; most other texts had a sentence on it at most or listed the expansion in terms of the moments instead of the cumulants which is a bit useless.", "Good luck finding a copy, though, I had to send my university librarian on a trip to the archives for it... but this was years ago, so maybe the internet will be more helpful today."], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "M. G. Kendall and A. Stuart, The advanced theory of statistics, vol. 1,\nCharles Griffin, 1963,", "childNum": 0, "tag": "em", "pos": -1, "childList": []}]}, {"text": ["The most general form of your question is the topic of a field known as non-parametric density estimation, where given:"], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "non-parametric density estimation", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"tag": "ul", "num": 2, "lis": [{"text": "data from a random process with an unknown distribution, and", "tag": "none", "senID": 5}, {"text": "constraints on the underlying process", "tag": "none", "senID": 6}]}, {"text": ["...you produce a density function that is the most likely to have produced the data.", "(More realistically, you create a method for computing an approximation to this function at any given point, which you can use for further work, eg.", "comparing the density functions from two sets of random data to see whether they could have come from the same process)."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["Personally, though, I have had little luck in using non-parametric density estimation for anything useful, but if you have a steady supply of sanity you should look into it."], "childNum": 0, "tag": "p", "senID": 8, "childList": []}], [{"text": ["I'm not a scientist, but if you were doing it with a pencil an paper, the obvious way would be to make a graph, then compare the graph to one of a known standard-distribution."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Going further with that thought, \"comparing\" is looking if the curves of a standard-distribution and yours are similar."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Trigonometry, tangents... would be my last thought."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["I'm not an expert, just another humble Web Developer =)"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}], [{"text": ["For what it's worth, it seems like you might want to look at the Poisson distribution."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}]]