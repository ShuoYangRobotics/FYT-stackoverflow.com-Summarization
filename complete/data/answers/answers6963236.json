[[{"text": ["Probably Certainly the second, I don't see any difference in doing a search in a big string or many in small strings.", "You may skip some chars thanks to the shorter lines, but the split operation has its costs too (searching for \\n, creating n different strings, creating the list) and the loop is done in python."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "Probably", "childNum": 0, "tag": "del", "pos": 0, "childList": []}, {"text": "\\n", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}, {"text": ["The string __contain__ method is implemented in C and so noticeably faster."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "__contain__", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Also consider that the second method aborts as soon as the first match is found, but the first one splits all the string before even starting to search inside it."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "splits", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["This is rapidly proven with a simple benchmark:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n import timeit\n\nprepare = \"\"\"\nwith open('bible.txt') as fh:\n    text = fh.read()\n\"\"\"\n\npresplit_prepare = \"\"\"\nwith open('bible.txt') as fh:\n    text = fh.read()\nlines = text.split('\\\\n')\n\"\"\"\n\nlongsearch = \"\"\"\n'hello' in text\n\"\"\"\n\nsplitsearch = \"\"\"\nfor line in text.split('\\\\n'):\n    if 'hello' in line:\n        break\n\"\"\"\n\npresplitsearch = \"\"\"\nfor line in lines:\n    if 'hello' in line:\n        break\n\"\"\"\n\n\nbenchmark = timeit.Timer(longsearch, prepare)\nprint \"IN on big string takes:\", benchmark.timeit(1000), \"seconds\"\n\nbenchmark = timeit.Timer(splitsearch, prepare)\nprint \"IN on splitted string takes:\", benchmark.timeit(1000), \"seconds\"\n\nbenchmark = timeit.Timer(presplitsearch, presplit_prepare)\nprint \"IN on pre-splitted string takes:\", benchmark.timeit(1000), \"seconds\"\n</code>\n</pre>\n", "senID": 4}, {"text": ["The result is:"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n IN on big string takes: 4.27126097679 seconds\nIN on splitted string takes: 35.9622690678 seconds\nIN on pre-splitted string takes: 11.815297842 seconds\n</code>\n</pre>\n", "senID": 6}, {"text": ["The bible.txt file actually is the bible, I found it here: http://patriot.net/~bmcgin/kjvpage.html (text version)"], "childNum": 2, "tag": "p", "senID": 7, "childList": [{"text": "is", "childNum": 0, "tag": "em", "pos": 0, "childList": []}, {"href": "http://patriot.net/~bmcgin/kjvpage.html", "text": "http://patriot.net/~bmcgin/kjvpage.html", "childNum": 0, "tag": "a", "childList": []}]}], [{"text": ["If you are only matching once to see if the substring is in the string at all, then both methods are about the same, and you get more overhead for splitting it into separate line by line searches; so the large string search is a bit faster."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If you have to do multiple matches, then I would tokenize the string and stuff them into a dictionary or set and store it in memory."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n s = 'SOME REALLY LONG STRING\ntokens = set(s.split())\nreturn substring in tokens\n</code>\n</pre>\n", "senID": 2}], [{"text": ["The second one is a lot faster, here are some measurement data:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n def get_mother_of_all_strings():\n    return \"abcdefg\\nhijklmnopqr\\nstuvwxyz\\naatargetbb\"\n\nfirst: 2.00\nsecond: 0.26\n</code>\n</pre>\n", "senID": 1}], [{"text": ["for loop in python is slow, and split a large string is also slow.", "so search the large string is much faster."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["The second way is faster, the slitting adds one more O(n) iteration of searching and delimiting, the alocating memory for each sublist, then close to O(n^2) to iter each sublist and search a string in them.", "while just O(n) to search the bigger string."], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "close to", "childNum": 0, "tag": "em", "pos": 0, "childList": []}, {"text": "O(n^2)", "childNum": 0, "tag": "strong", "pos": -1, "childList": []}, {"text": "O(n)", "childNum": 0, "tag": "strong", "pos": 1, "childList": []}]}], [{"code": "<pre>\n<code>\n import timeit\n\na = #a really long string with multiple lines and target near the end\n\ntimeit.timeit(stmt='[\"target\" in x for x in a.split(\"\\\\n\")]', setup='a = \"\"\"%s\"\"\"'%a)\n23.499058284211792\ntimeit.timeit(stmt='\"target\" in a', setup='a = \"\"\"%s\"\"\"'%a)\n5.2557157624293325\n</code>\n</pre>\n", "senID": 0}, {"text": ["So the large string is MUCH faster to search through than a split list of smaller ones."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}]]