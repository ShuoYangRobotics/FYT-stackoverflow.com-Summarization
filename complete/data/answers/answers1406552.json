[[{"text": ["As a practical matter, I'd probably use print statements to indicate failure in that case.", "A more correct solution is to use warnings:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["http://docs.python.org/library/warnings.html"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://docs.python.org/library/warnings.html", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/warnings.html"}]}, {"text": ["You could, however, use the logging facility to generate a more detailed record of your test results (i.e.", "set your \"B\" class failures to write warnings to the logs)."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["http://docs.python.org/library/logging.html"], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "http://docs.python.org/library/logging.html", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/logging.html"}]}, {"text": ["Edit:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["The way we handle this in Django is that we have some tests we expect to fail, and we have others that we skip based on the environment.", "Since we can generally predict whether a test SHOULD fail or pass (i.e.", "if we can't import a certain module,  the system doesn't have it, and so the test won't work), we can skip failing tests intelligently.", "This means that we still run every test that will pass, and have no tests that \"might\" pass.", "Unit tests are most useful when they do things predictably, and being able to detect whether or not a test SHOULD pass before we run it makes this possible."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}], [{"text": ["Im not totally sure how unittest works, but most unit testing frameworks have something akin to categories.", "I suppose you could just categorize such tests, mark them to be ignored, and then run them only when your interested in them.", "But I know from experience that ignored tests very quickly become...just that ignored tests that nobody ever runs and are therefore a waste of time and energy to write them. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["My advice is for your app to do, or do not, there is no try."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Asserts in unit tests are binary: they will work or they will fail, there's no mid-term."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Given that, to create those \"non-critical\" tests you should not use assertions when you don't want the tests to fail.", "You should do this carefully so you don't compromise the \"usefulness\" of the test."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["My advice to your OCR example is that you use something to record the success rate in your tests code and then create one assertion like: \"assert success_rate > 8.5\", and that should give the effect you desire."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["From unittest documentation which you link:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "unittest", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/unittest.html"}]}, {"text": ["In your case, you can create separate TestSuite instances for the criticial and non-critical tests.", "You could control which suite is passed to the test runner with a command line argument.", "Test suites can also contain other test suites so you can create big hierarchies if you want."], "childNum": 2, "tag": "p", "senID": 1, "childList": [{"text": "TestSuite", "tag": "a", "pos": 0, "childList": [{"text": "TestSuite", "tag": "code"}], "childNum": 1, "href": "http://docs.python.org/library/unittest.html#unittest.TestSuite"}, {"text": "TestSuite", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}], [{"text": ["There are some test systems that allow warnings rather than failures, but test_unit is not one of them (I don't know which ones do, offhand) unless you want to extend it (which is possible)."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["You can make the tests so that they log warnings rather than fail."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Another way to handle this is to separate out the tests and only run them to get the pass/fail reports and not have any build dependencies (this depends on your build setup)."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Take a look at Nose : http://somethingaboutorange.com/mrl/projects/nose/0.11.1/"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://somethingaboutorange.com/mrl/projects/nose/0.11.1/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://somethingaboutorange.com/mrl/projects/nose/0.11.1/"}]}, {"text": ["There are plenty of command line options for selecting tests to run, and you can keep your existing unittest tests."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Another possibility is to create a \"B\" branch (you ARE using some sort of version control, right?", ") and have your unit tests for \"B\" in there.", "That way, you keep your release version's unit tests clean (Look, all dots!", "), but still have tests for B.", "If you're using a modern version control system like git or mercurial (I'm partial to mercurial), branching/cloning and merging are trivial operations, so that's what I'd recommend."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["However, I think you're using tests for something they're not meant to do.", "The real question is \"How important to you is it that 'B' works?", "\" Because your test suite should only have tests in it that you care whether they pass or fail.", "Tests that, if they fail, it means the code is broken.", "That's why I suggested only testing \"B\" in the \"B\" branch, since that would be the branch where you are developing the \"B\" feature. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["You could test using logger or print commands, if you like.", "But if you don't care enough that it's broken to have it flagged in your unit tests, I'd seriously question whether you care enough to test it at all.", "Besides, that adds needless complexity (extra variables to set debug level, multiple testing vectors that are completely independent of each other yet operate within the same space, causing potential collisions and errors, etc, etc).", "Unless you're developing a \"Hello, World!", "\" app, I suspect your problem set is complicated enough without adding additional, unnecessary complications."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Thank you for the great answers.", "No only one answer was really complete, so I'm writing here a combination of all answers that helped me.", "If you like this answer, please vote up the people who were responsible for this."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "a combination of all answers that helped me", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://meta.stackoverflow.com/questions/13413/how-do-i-combine-two-answers-to-create-the-best-answer-on-stackoverflow/13414#13414"}]}, {"text": ["Conclusions"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "Conclusions", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Unit tests (or at least unit tests in unittest module) are binary.", "As Guilherme Chapiewski says: they will work or they will fail, there's no mid-term."], "childNum": 3, "tag": "p", "senID": 2, "childList": [{"text": "unittest", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "Guilherme Chapiewski says", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/1406552/non-critical-unittest-failures/1406610#1406610"}, {"text": "they will work or they will fail, there's no mid-term.", "childNum": 0, "tag": "em", "childList": []}]}, {"text": ["Thus, my conclusion is that unit tests are not exactly the right tool for this job.", "It seems that unit tests are more concerned about \"keep everything working, no failure is expected\", and thus I can't (or it's not easy) to have non-binary tests."], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "\"keep everything working, no failure is expected\"", "childNum": 0, "tag": "em", "pos": 1, "childList": []}]}, {"text": ["So, unit tests don't seem the right tool if I'm trying to improve an algorithm or an implementation, because unit tests can't tell me how better is one version when compared to the other (supposing both of them are correctly implemented, then both will pass all unit tests)."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["My final solution"], "childNum": 1, "tag": "p", "senID": 5, "childList": [{"text": "My final solution", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["My final solution is based on ryber's idea and code shown in wcoenen answer.", "I'm basically extending the default TextTestRunner and making it less verbose.", "Then, my main code call two test suits: the critical one using the standard TextTestRunner, and the non-critical one, with my own less-verbose version."], "childNum": 4, "tag": "p", "senID": 6, "childList": [{"text": "ryber's idea", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/1406552/non-critical-unittest-failures/1406576#1406576"}, {"text": "wcoenen answer", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/1406552/non-critical-unittest-failures/1407485#1407485"}, {"text": "TextTestRunner", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "TextTestRunner", "childNum": 0, "tag": "code", "pos": 2, "childList": []}]}, {"code": "<pre>\n<code>\n class _TerseTextTestResult(unittest._TextTestResult):\n    def printErrorList(self, flavour, errors):\n        for test, err in errors:\n            #self.stream.writeln(self.separator1)\n            self.stream.writeln(\"%s: %s\" % (flavour,self.getDescription(test)))\n            #self.stream.writeln(self.separator2)\n            #self.stream.writeln(\"%s\" % err)\n\n\nclass TerseTextTestRunner(unittest.TextTestRunner):\n    def _makeResult(self):\n        return _TerseTextTestResult(self.stream, self.descriptions, self.verbosity)\n\n\nif __name__ == '__main__':\n    sys.stderr.write(\"Running non-critical tests:\\n\")\n    non_critical_suite = unittest.TestLoader().loadTestsFromTestCase(TestSomethingNonCritical)\n    TerseTextTestRunner(verbosity=1).run(non_critical_suite)\n\n    sys.stderr.write(\"\\n\")\n\n    sys.stderr.write(\"Running CRITICAL tests:\\n\")\n    suite = unittest.TestLoader().loadTestsFromTestCase(TestEverythingImportant)\n    unittest.TextTestRunner(verbosity=1).run(suite)\n</code>\n</pre>\n", "senID": 7}, {"text": ["Possible improvements"], "childNum": 1, "tag": "p", "senID": 8, "childList": [{"text": "Possible improvements", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["It should still be useful to know if there is any testing framework with non-binary tests, like Kathy Van Stone suggested.", "Probably I won't use it this simple personal project, but it might be useful on future projects."], "childNum": 1, "tag": "p", "senID": 9, "childList": [{"text": "Kathy Van Stone suggested", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/1406552/non-critical-unittest-failures/1406832#1406832"}]}], [{"text": ["Python 2.7 (and 3.1) added support for skipping some test methods or test cases, as well as marking some tests as expected failure."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "expected failure", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["http://docs.python.org/library/unittest.html#skipping-tests-and-expected-failures"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://docs.python.org/library/unittest.html#skipping-tests-and-expected-failures", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/unittest.html#skipping-tests-and-expected-failures"}]}, {"text": ["Tests marked as expected failure won't be counted as failure on a TestResult."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["You could write your test so that they count success rate.", "With OCR you could throw at code 1000 images and require that 95% is successful. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If your program must work with type A then if this fails the test fails.", "If it's not required to work with B, what is the value of doing such a test ?"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}]]