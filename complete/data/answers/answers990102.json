[[{"text": ["Another solution is:\nhttp://docs.python.org/library/multiprocessing.html"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://docs.python.org/library/multiprocessing.html", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/multiprocessing.html"}]}, {"text": ["Note 1: This is not a limitation of the Python language, but of CPython implementation."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "not", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Note 2: With regard to affinity, your OS shouldn't have a problem doing that itself."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["I've found the following rule of thumb sufficient over the years: If the workers are dependent on some shared state, I use one multiprocessing process per core (CPU bound), and per core a fix pool of worker threads (I/O bound).", "The OS will take care of assigining the different Python processes to the cores."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["I have never heard of anyone using taskset for a performance gain with Python.", "Doesn't mean it can't happen in your case, but definitely publish your results so others can critique your benchmarking methods and provide validation."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Personally though, I would decouple your I/O threads from the CPU bound threads using a message queue.", "That way your front end is now completely network I/O bound (some with HTTP interface, some with message queue interface) and ideal for your threading situation.", "Then the CPU intense processes can either use multiprocessing or just be individual processes waiting for work to arrive on the message queue."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["In the longer term you might also want to consider replacing your threaded I/O front-end with Twisted or some thing like eventlets because, even if they won't help performance they should improve scalability.", "Your back-end is now already scalable because you can run your message queue over any number of machines+cpus as needed."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "eventlets", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://wiki.secondlife.com/wiki/Eventlet"}]}], [{"text": ["The Python GIL is per Python interpreter.", "That means the only to avoid problems with it while doing multiprocessing is simply starting multiple interpreters (i.e.", "using seperate processes instead of threads for concurrency) and then using some other IPC primitive for communication between the processes (such as sockets).", "That being said, the GIL is not a problem when using threads with blocking I/O calls."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["The main problem of the GIL as mentioned earlier is that you can't execute 2 different python code threads at the same time.", "A thread blocking on a blocking I/O call is blocked and hence not executin python code.", "This means it is not blocking the GIL.", "If you have two CPU intensive tasks in seperate python threads, that's where the GIL kills multi-processing in Python (only the CPython implementation, as pointed out earlier).", "Because the GIL stops CPU #1 from executing a python thread while CPU #0 is busy executing the other python thread."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Until such time as the GIL is removed from Python, co-routines may be used in place of threads.", "I have it on good authority that this strategy has been implemented by two successful start-ups, using greenlets in at least one case."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["An interesting solution is the experiment reported by Ryan Kelly on his blog: http://www.rfk.id.au/blog/entry/a-gil-adventure-threading2/"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://www.rfk.id.au/blog/entry/a-gil-adventure-threading2/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.rfk.id.au/blog/entry/a-gil-adventure-threading2/"}]}], [{"text": ["This is a pretty old question but since everytime I search about information related to python and performance on multi-core systems this post is always on the result list, I would not let this past before me an do not share my thoughts."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["You can use the multiprocessing module that rather than create threads for each task, it creates another process of cpython compier interpreting your code.", "It would make your application to take advantage of multicore systems.", "The only problem that I see on this approach is that you will have a considerable overhead by creating an entire new process stack on memory.", "(http://en.wikipedia.org/wiki/Thread_(computing)#How_threads_differ_from_processes)"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://en.wikipedia.org/wiki/Thread_(computing)#How_threads_differ_from_processes", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Thread_%28computing%29#How_threads_differ_from_processes"}]}, {"text": ["Python Multiprocessing module:\nhttp://docs.python.org/dev/library/multiprocessing.html"], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "http://docs.python.org/dev/library/multiprocessing.html", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/dev/library/multiprocessing.html"}]}, {"text": ["\"The reason I am not using the multiprocessing module is that (in this case) part of the program is heavily network I/O bound (HTTP requests) so having a pool of worker threads is a GREAT way to squeeze performance out of a box...\""], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "\"The reason I am not using the multiprocessing module is that (in this case) part of the program is heavily network I/O bound (HTTP requests) so having a pool of worker threads is a GREAT way to squeeze performance out of a box...\"", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["About this, I guess that you can have also a pool of process too: http://docs.python.org/dev/library/multiprocessing.html#using-a-pool-of-workers"], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "http://docs.python.org/dev/library/multiprocessing.html#using-a-pool-of-workers", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/dev/library/multiprocessing.html#using-a-pool-of-workers"}]}, {"text": ["Att,\nLeo"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}]]