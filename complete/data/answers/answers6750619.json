[[{"text": ["There are lots of options but it will be hard to find one which fits all your needs."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["In your case, try this approach:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Use another thread to collect the results (i.e.", "another queue).", "When the number of result objects == number of puts in the first queue, then you're finished."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Make sure that all communication goes via the queue or the \"config object\".", "Avoid accessing data structures which are shared between threads.", "This should save you 99% of the problems."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}], [{"text": ["You can try pycurl, though the interface is not easy at first, but once you look at examples, its not hard to understand.", "I have used it to fetch 1000s of web pages in parallel on meagre linux box. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["The only problem is that it provides a basic infrastructure (basically just a python layer above the excellent curl library).", "You will have to write few lines to achieve the features as you want. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["I don't think such a complete library exists, so you'll probably have to write your own.", "I suggest taking a look at gevent for this task.", "They even provide a concurrent_download.py example script.", "Then you can use urllib2 for most of the other requirements, such as handling HTTP status codes, and displaying download progress."], "childNum": 5, "tag": "p", "senID": 0, "childList": [{"text": "gevent", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://www.gevent.org/"}, {"text": "concurrent_download.py", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "https://code.google.com/p/gevent/source/browse/examples/concurrent_download.py"}, {"text": "urllib2", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/urllib2.html"}, {"href": "http://www.voidspace.org.uk/python/articles/urllib2.shtml#httperror", "text": "handling HTTP status codes", "childNum": 0, "tag": "a", "childList": []}, {"href": "http://stackoverflow.com/questions/2028517/python-urllib2-progress-hook", "text": "displaying download progress", "childNum": 0, "tag": "a", "childList": []}]}], [{"text": ["I would suggest Twisted, although it is not a ready made solution, but provides the main building blocks to get every feature you listed in an easy way and it does not use threads."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If you are interested, take a look at the following links:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"tag": "ul", "num": 2, "lis": [{"text": ["http://twistedmatrix.com/documents/current/api/twisted.web.client.html#getPage"], "childNum": 0, "tag": "a", "senID": 2, "childList": []}, {"text": ["http://twistedmatrix.com/documents/current/api/twisted.web.client.html#downloadPage"], "childNum": 0, "tag": "a", "senID": 3, "childList": []}]}, {"text": ["As per your requirements:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["Try using aria2 through simple python subprocess module.", "It provide all requirements from your list, except 7, out of the box, and 7 is easy to write.", "aria2c has a nice xml-rpc or json-rpc interface to interact with it from your scripts."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "aria2", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://aria2.sourceforge.net/"}, {"text": "subprocess", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/subprocess.html"}]}], [{"text": ["Threading isn't \"half-assed\" unless you're a bad programmer.", "The best general approach to this problem is the producer / consumer model.", "You have one dedicated URL producer, and N dedicated download threads (or even processes if you use the multiprocessing model)."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "N", "childNum": 0, "tag": "em", "pos": 2, "childList": []}]}, {"text": ["As for all of your requirements, ALL of them CAN be done with the normal python threaded model (yes, even catching Ctrl+C -- I've done it)."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Does urlgrabber fit your requirements?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["http://urlgrabber.baseurl.org/"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://urlgrabber.baseurl.org/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://urlgrabber.baseurl.org/"}]}, {"text": ["If it doesn't, you could consider volunteering to help finish it.", "Contact the authors, Michael Stenner and Ryan Tomayko."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Update: Googling for \"parallel wget\" yields these, among others:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["http://puf.sourceforge.net/"], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "http://puf.sourceforge.net/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://puf.sourceforge.net/"}]}, {"text": ["http://www.commandlinefu.com/commands/view/3269/parallel-file-downloading-with-wget"], "childNum": 1, "tag": "p", "senID": 5, "childList": [{"text": "http://www.commandlinefu.com/commands/view/3269/parallel-file-downloading-with-wget", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.commandlinefu.com/commands/view/3269/parallel-file-downloading-with-wget"}]}, {"text": ["It seems like you have a number of options to choose from."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}], [{"text": ["I used the standard libs for that, urllib.urlretrieve to be precise.", "downloaded podcasts this way, via a simple thread pool, each using its own retrieve.", "I did about 10 simultanous connections, more should not be a problem.", "Continue a interrupted download, maybe not.", "Ctrl-C could be handled, I guess.", "Worked on Windows, installed a handler for progress bars.", "All in all 2 screens of code, 2 screens for generating the URLs to retrieve."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "urllib.urlretrieve", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["This seems pretty flexible:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["http://keramida.wordpress.com/2010/01/19/parallel-downloads-with-python-and-gnu-wget/"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://keramida.wordpress.com/2010/01/19/parallel-downloads-with-python-and-gnu-wget/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://keramida.wordpress.com/2010/01/19/parallel-downloads-with-python-and-gnu-wget/"}]}], [{"text": ["Nowadays there are excellent Python libs you might want to use - urllib3 and requests"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "urllib3", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://urllib3.readthedocs.org/"}, {"href": "http://docs.python-requests.org/", "text": "requests", "childNum": 0, "tag": "a", "childList": []}]}]]