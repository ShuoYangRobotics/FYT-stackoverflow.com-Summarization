[[{"text": ["This is why your OS has multiprocessing pipelines."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n collapse.py sometweetfile | filter.py | user_id.py | user_split.py -d some_directory\n</code>\n</pre>\n", "senID": 1}, {"text": ["collapse.py"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n import sys\nwith open(\"source\",\"r\") as theFile:\n    tweet = {}\n    for line in theFile:\n        rec_type, content = line.split('\\t')\n        if rec_type in tweet:\n            t, u, w = tweet.get('T',''), tweet.get('U',''), tweet.get('W','')\n            result=  \"{0}\\t{1}\\t{2}\".format( t, u, w )\n            sys.stdout.write( result )\n            tweet= {}\n        tweet[rec_type]= content\n    t, u, w = tweet.get('T',''), tweet.get('U',''), tweet.get('W','')\n    result=  \"{0}\\t{1}\\t{2}\".format( t, u, w )\n    sys.stdout.write( result )\n</code>\n</pre>\n", "senID": 3}, {"text": ["filter.py"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"code": "<pre>\n<code>\n import sys\nfor tweet in sys.stdin:\n    t, u, w = tweet.split('\\t')\n    if 'No Post Title' in t:\n        continue\n    sys.stdout.write( tweet )\n</code>\n</pre>\n", "senID": 5}, {"text": ["user_id.py"], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"code": "<pre>\n<code>\n import sys\nimport urllib\nfor tweet in sys.stdin:\n    t, u, w = tweet.split('\\t')\n    path=urlparse(w).path.strip('/')\n    result= \"{0}\\t{1}\\t{2}\\t{3}\".format( t, u, w, path )\n    sys.stdout.write( result )\n</code>\n</pre>\n", "senID": 7}, {"text": ["user_split.py"], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"code": "<pre>\n<code>\n users = {}\nfor tweet in sys.stdin:\n    t, u, w, user = tweet.split('\\t')\n    if user not in users:\n        # May run afoul of open file limits...\n        users[user]= open(some_directory+user,\"w\")\n    users[user].write( tweet )\n    users[user].flush( tweet )\nfor u in users:\n    users[u].close()\n</code>\n</pre>\n", "senID": 9}, {"text": ["Wow, you say.", "What a lot of code."], "childNum": 0, "tag": "p", "senID": 10, "childList": []}, {"text": ["Yes.", "But.", "It spreads out among ALL the processing cores you own and it all runs concurrently.", "Also, when you connect stdout to stdin through a pipe, it's really only a shared buffer: there's no physical I/O occurring."], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"text": ["It's amazingly fast to do things this way.", "That's why the *Nix operating systems work that way.", "This is what you need to do for real speed."], "childNum": 1, "tag": "p", "senID": 12, "childList": [{"text": "*Nix", "childNum": 0, "tag": "strong", "pos": 1, "childList": []}]}, {"text": ["The LRU algorithm, FWIW."], "childNum": 0, "tag": "p", "senID": 13, "childList": []}, {"code": "<pre>\n<code>\n if user not in users:\n        # Only keep a limited number of files open\n        if len(users) &gt; 64: # or whatever your OS limit is.\n            lru, aFile, u = min( users.values() )\n            aFile.close()\n            users.pop(u)\n        users[user]= [ tolu, open(some_directory+user,\"w\"), user ]\n    tolu += 1\n    users[user][1].write( tweet )\n    users[user][1].flush() # may not be necessary\n    users[user][0]= tolu\n</code>\n</pre>\n", "senID": 14}], [{"text": ["You spend most of the time in I/O.", "Solutions: "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 3, "lis": [{"text": "make larger I/O operations, i.e. read into a buffer of let's say 512K and do not write information till you have a buffer of 256K at least.", "tag": "none", "senID": 1}, {"text": "avoid doing file open and close as much as possible", "tag": "none", "senID": 2}, {"text": "use several threads to read from the file, i.e. split the file to chunks and give each thread it's own chunk to work on", "tag": "none", "senID": 3}]}], [{"text": ["For such a mass of information, I would use a database (MySQL, PostgreSQL, SQLite, etc.).", "They are optimized for the kind of things you are doing."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Thus, instead of appending to a file, you could simply add a line to a table (either the junk or the \"good\" table), with the URL and the associated data (the same URL can be on multiple lines).", "This would definitely speed up the writing part."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["With the current approach, time is lost because the input file is read from one location on your hard drive while you write on many different locations: the head of the hard drive moves back and forth physically, which is slow.", "Also, creating new files takes time.", "If you could mostly read from the input file and let a database handle the data caching and disk writing optimization, the processing would undoubtedly be faster."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Not sure if this is going to be faster, just an idea.", "Your file looks like a csv with tabs as delimiters.", "Have you tried createing a CSV reader?"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "csv", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "CSV", "childNum": 0, "tag": "code", "pos": 2, "childList": []}]}, {"code": "<pre>\n<code>\n import csv\nreader = csv.reader(open('bigfile'), 'excel-tab')\nfor line in reader:\n    process_line()\n</code>\n</pre>\n", "senID": 1}, {"text": ["EDIT: Calling csv.field_size_limit(new_limit) is pointless here."], "childNum": 2, "tag": "p", "senID": 2, "childList": [{"text": "EDIT:", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "csv.field_size_limit(new_limit)", "childNum": 0, "tag": "code", "childList": []}]}], [{"text": ["You could try creating a dict with the format {url: [lines...]}, and only writing each file at the end.", "I suspect that repeatedly opening and closing files is a lot of overhead.", "How many lines are you writing per file on average?", "If basically each line is getting its own file then there's not much you can do, except change that requirement :)"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "{url: [lines...]}", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["On my system at least, almost all of the running time would be spent closing files.", "Sequential reading and writing is fast, so you can very well make several passes over the data.", "Here's what I'd do:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If you can write to 200 files in parallel, after two passes over all the data you'd have 40000 files containing 150 users on average, so after the third pass you'd probably be nearly done."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Here's some code that assumes the file has been preprocessed according to S.Lott's answer (collapse, filter, user_id).", "Note that it will delete the input file along with other intermediate files."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n todo = ['source']\ncounter = 0\n\nwhile todo:\n    infilename = todo.pop()\n    infile = open(infilename)\n    users = {}\n    files = []\n    filenames = []\n    for tweet in infile:\n        t, u, w, user = tweet.split('\\t')\n        if user not in users:\n            users[user] = len(users) % MAX_FILES\n            if len(files) &lt; MAX_FILES:\n                filenames.append(str(counter))\n                files.append(open(filenames[-1], 'w'))\n                counter += 1\n        files[users[user]].write(tweet)\n    for f in files:\n        f.close()\n    if len(users) &gt; MAX_FILES:\n        todo += filenames[:len(users)-MAX_FILES]\n    infile.close()\n    os.remove(infilename)\n</code>\n</pre>\n", "senID": 3}]]