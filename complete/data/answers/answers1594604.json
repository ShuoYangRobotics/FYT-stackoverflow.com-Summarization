[[{"text": ["Poor man's map-reduce:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Use split to break the file up into as many pieces as you have CPUs."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "split", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.gnu.org/manual/gawk/html%5Fnode/Split-Program.html"}]}, {"text": ["Use batch to run your muncher in parallel."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "batch", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://linux.about.com/library/cmd/blcmdl1%5Fbatch.htm"}]}, {"text": ["Use cat to concatenate the results."], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "cat", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.gnu.org/software/coreutils/manual/html%5Fnode/cat-invocation.html#cat-invocation"}]}], [{"text": ["If you are I/O bound, the best way I have found to optimize is to read or write the entire file into/out of memory at once, then operate out of RAM from there on."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "entire", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["With extensive testing I found that my runtime eded up bound not by the amount of data I read from/wrote to disk, but by the number of I/O operations I used to do it.", "That is what you need to optimize."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["I don't know Python, but if there is a way to tell it to write the whole file out of RAM in one go, rather than issuing a separate I/O for each byte, that's what you need to do."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Of course the drawback to this is that files can be considerably larger than available RAM.", "There are lots of ways to deal with that, but that is another question for another time."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}], [{"text": ["Python already does IO buffering and the OS should handle both prefetching the input file and delaying writes until it needs the RAM for something else or just gets uneasy about having dirty data in RAM for too long.", "Unless you force the OS to write them immediately, like closing the file after each write or opening the file in O_SYNC mode."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "should", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["If the OS isn't doing the right thing, you can try raising the buffer size (third parameter to open()).", "For some guidance on appropriate values given a 100MB/s 10ms latency IO system a 1MB IO size will result in approximately 50% latency overhead, while a 10MB IO size will result in 9% overhead.", "If its still IO bound, you probably just need more bandwidth.", "Use your OS specific tools to check what kind of bandwidth you are getting to/from the disks."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "open()", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Also useful is to check if step 4 is taking a lot of time executing or waiting on IO.", "If it's executing you'll need to spend more time checking which part is the culprit and optimize that, or split out the work to different processes."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Can you use a ramdisk for step 4?", "Low millions sounds doable if the rows are less than a couple of kB or so."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["Use buffered writes for step 4."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Write a simple function that simply appends the output onto a string, checks the string length, and only writes when you have enough which should be some multiple of 4k bytes.", "I would say start with 32k buffers and time it."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["You would have one buffer per file, so that most \"writes\" won't actually hit the disk."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Isn't it possible to collect a few thousand rows in ram, then go directly to the database server and execute them? "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["This would remove the save to and load from the disk that step 4 entails."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["If the database server is transactional, this is also a safe way to do it - just have the database begin before your first row and commit after the last."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["The first thing is to be certain of what you should optimize.", "You seem to not know precisely where your time is going.", "Before spending more time wondering, use a performance profiler to see exactly where the time is going."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["http://docs.python.org/library/profile.html"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://docs.python.org/library/profile.html", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/profile.html"}]}, {"text": ["When you know exactly where the time is going, you'll be in a better position to know where to spend your time optimizing."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}]]