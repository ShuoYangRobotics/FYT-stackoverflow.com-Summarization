[[{"text": ["Python takes a \"we're all consenting adults\" approach to information hiding.", "It's OK to use variables as though they were constants, and trust that users of your code won't do something stupid. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["In many situations, exp.", "when parsing long input streams, you may find it more useful to implement you tokenizer as a generator function.", "This way you can easily iterate over all the tokens without the need for lots of memory to build the list of tokens first."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["For generator see the original proposal or other online docs"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "original proposal", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.python.org/dev/peps/pep-0255/"}]}], [{"text": ["There's an undocumented class in the re module called re.Scanner.", "It's very straightforward to use for a tokenizer:"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "re", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "re.Scanner", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"code": "<pre>\n<code>\n import re\nscanner=re.Scanner([\n  (r\"[0-9]+\",   \tlambda scanner,token:(\"INTEGER\", token)),\n  (r\"[a-z_]+\",  \tlambda scanner,token:(\"IDENTIFIER\", token)),\n  (r\"[,.]+\",    \tlambda scanner,token:(\"PUNCTUATION\", token)),\n  (r\"\\s+\", None), # None == skip token.\n])\n\nresults, remainder=scanner.scan(\"45 pigeons, 23 cows, 11 spiders.\")\nprint results\n</code>\n</pre>\n", "senID": 1}, {"text": ["will result in"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n [('INTEGER', '45'),\n ('IDENTIFIER', 'pigeons'),\n ('PUNCTUATION', ','),\n ('INTEGER', '23'),\n ('IDENTIFIER', 'cows'),\n ('PUNCTUATION', ','),\n ('INTEGER', '11'),\n ('IDENTIFIER', 'spiders'),\n ('PUNCTUATION', '.')]\n</code>\n</pre>\n", "senID": 3}, {"text": ["I used re.Scanner to write a pretty nifty configuration/structured data format parser in only a couple hundred lines."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["\"Is there a better alternative to just simply returning a list of tuples?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Nope.", "It works really well."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Thanks for your help, I've started to bring these ideas together, and I've come up with the following.", "Is there anything terribly wrong with this implementation (particularly I'm concerned about passing a file object to the tokenizer):"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n class Tokenizer(object):\n\n  def __init__(self,file):\n     self.file = file\n\n  def __get_next_character(self):\n      return self.file.read(1)\n\n  def __peek_next_character(self):\n      character = self.file.read(1)\n      self.file.seek(self.file.tell()-1,0)\n      return character\n\n  def __read_number(self):\n      value = \"\"\n      while self.__peek_next_character().isdigit():\n          value += self.__get_next_character()\n      return value\n\n  def next_token(self):\n      character = self.__peek_next_character()\n\n      if character.isdigit():\n          return self.__read_number()\n</code>\n</pre>\n", "senID": 1}], [{"text": ["\"Is there a better alternative to just simply returning a list of tuples?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["That's the approach used by the \"tokenize\" module for parsing Python source code.", "Returning a simple list of tuples can work very well."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["I have recently built a tokenizer, too, and passed through some of your issues."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Token types are declared as \"constants\", i.e.", "variables with ALL_CAPS names, at the module level.", "For example,"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n _INTEGER = 0x0007\n_FLOAT = 0x0008\n_VARIABLE = 0x0009\n</code>\n</pre>\n", "senID": 2}, {"text": ["and so on.", "I have used an underscore in front of the name to point out that somehow those fields are \"private\" for the module, but I really don't know if this is typical or advisable, not even how much Pythonic.", "(Also, I'll probably ditch numbers in favour of strings, because during debugging they are much more readable."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["Tokens are returned as named tuples."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"code": "<pre>\n<code>\n from collections import namedtuple\nToken = namedtuple('Token', ['value', 'type'])\n# so that e.g. somewhere in a function/method I can write...\nt = Token(n, _INTEGER)\n# ...and return it properly\n</code>\n</pre>\n", "senID": 5}, {"text": ["I have used named tuples because the tokenizer's client code (e.g.", "the parser) seems a little clearer while using names (e.g.", "token.value) instead of indexes (e.g.", "token[0])."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["Finally, I've noticed that sometimes, especially writing tests, I prefer to pass a string to the tokenizer instead of a file object.", "I call it a \"reader\", and have a specific method to open it and let the tokenizer access it through the same interface."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"code": "<pre>\n<code>\n def open_reader(self, source):\n    \"\"\"\n    Produces a file object from source.\n    The source can be either a file object already, or a string.\n    \"\"\"\n    if hasattr(source, 'read'):\n        return source\n    else:\n        from io import StringIO\n        return StringIO(source)\n</code>\n</pre>\n", "senID": 8}], [{"text": ["When I start something new in Python I usually look first at some modules or libraries to use.", "There's 90%+ chance that there already is somthing available."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["For tokenizers and parsers this is certainly so.", "Have you looked at PyParsing ?"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "PyParsing", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://pyparsing.wikispaces.com/"}]}], [{"text": ["I've implemented a tokenizer for a C-like programming language.", "What I did was to split up the creation of tokens into two layers:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 2, "lis": [{"text": "a ", "tag": "none", "senID": 1}, {"text": "a ", "tag": "none", "senID": 2}]}, {"text": ["Both are generators.", "The benefits of this approach were:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"tag": "ul", "num": 4, "lis": [{"text": "Reading of the raw text is done only in the most primitive way, with simple regexps - fast and clean.", "tag": "none", "senID": 4}, {"text": "The second layer is already implemented as a primitive parser, to detect string literals and comments - re-use of parser technology. ", "tag": "none", "senID": 5}, {"text": "You don't have to strain the surface scanner with complex detections.", "tag": "none", "senID": 6}, {"text": "But the real parser gets tokens on the semantic level of the language to be parsed (again strings, comments).", "tag": "none", "senID": 7}]}, {"text": ["I feel quite happy with this layered approach."], "childNum": 0, "tag": "p", "senID": 8, "childList": []}], [{"text": ["\"Is there a better alternative to just simply returning a list of tuples\""], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["I had to implement a tokenizer, but it required a more complex approach than a list of tuples, therefore I implemented a class for each token.", "You can then return a list of class instances, or if you want to save resources, you can return something implementing the iterator interface and generate the next token while you progress in the parsing."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["I'd turn to the excellent Text Processing in Python by David Mertz"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "Text Processing in Python", "childNum": 1, "tag": "em", "pos": 0, "childList": [{"text": "Text Processing in Python", "tag": "a"}]}, {"href": "http://books.google.com/books?id=GxKWdn7u4w8C", "text": "Text Processing in Python", "childNum": 0, "tag": "a", "childList": []}]}]]