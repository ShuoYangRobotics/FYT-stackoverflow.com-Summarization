[[{"text": ["I'm going to assume you are hashing web pages.", "You have to hash at most 55 billion web pages (and that measure almost certainly overlooks some overlap). "], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "55 billion web pages", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://www.worldwidewebsize.com/"}]}, {"text": ["You are willing to accept a less than one in a billion chance of collision, which means that if we look at a hash function which number of collisions is close to what we would get if the hash was truly random[\u02c61], we want a hash range of size (55*10\u02c69)*10\u02c69.", "That is log2((55*10\u02c69)*10\u02c69) = 66 bits."], "childNum": 2, "tag": "p", "senID": 1, "childList": [{"text": "(55*10\u02c69)*10\u02c69", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "log2((55*10\u02c69)*10\u02c69) = 66", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}, {"text": ["[\u02c61]: since the hash can be considered to be chosen at random for this purpose,\n       p(collision) = (occupied range)/(total range)"], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "p(collision) = (occupied range)/(total range)", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Since there is a speed issue, but no real cryptographic concern, we can use a > 66-bits non-cryptographic hash with the nice collision distribution property outlined above."], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "with the nice collision distribution property outlined above", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.strchr.com/hash_functions?allcomments=1#comment_155"}]}, {"text": ["It looks like we are looking for the 128-bit version of the Murmur3 hash.", "People have been reporting speed increases upwards of 12x comparing Murmur3_128 to MD5 on a 64-bit machine.", "You can use this library to do your speed tests.", "See also this related answer, which:"], "childNum": 4, "tag": "p", "senID": 4, "childList": [{"text": "Murmur3", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "https://code.google.com/p/smhasher/wiki/MurmurHash3"}, {"text": "speed increases upwards of 12x", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "https://groups.google.com/group/alembic-dev/browse_thread/thread/179db1ebb7c6b51f?pli=1"}, {"text": "this library", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "https://code.google.com/p/smhasher/"}, {"text": "answer", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/a/5400389/47978"}]}, {"tag": "ul", "num": 2, "lis": [{"text": "shows speed test results in the range of python's ", "tag": "none", "senID": 5}, {"text": "spawned a ", "tag": "none", "senID": 6}]}, {"text": ["Finally, I hope to have outlined the reasoning that could allow you to compare with other functions of varied size should you feel the need for it (e.g.", "if you up your collision tolerance, if the size of your indexed set is smaller than the whole Internet, etc, ...)."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}], [{"text": ["You have to decide which is more important:  space or time."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If time, then you need to create unique representations of your large_item which take as little space as possible (probably some str value) that is easy (i.e.", "quick) to calculate and will not have collisions, and store them in a set."], "childNum": 3, "tag": "p", "senID": 1, "childList": [{"text": "large_item", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "str", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "set", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}, {"text": ["If space, find the quickest disk-backed solution you can and store the smallest possible unique value that will identify a large_item."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "large_item", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["So either way, you want small unique identifiers -- depending on the nature of large_item this may be a big win, or not possible."], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "large_item", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Update"], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "Update", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Perhaps a hybrid solution then:  Keep a set in memory of the normal Python hash, while also keeping the actual html content on disk, keyed by that hash; when you check to see if the current large_item is in the set and get a positive, double-check with the disk-backed solution to see if it's a real hit or not, then skip or process as appropriate.", "Something like this:"], "childNum": 3, "tag": "p", "senID": 5, "childList": [{"text": "set", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "large_item", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "set", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"code": "<pre>\n<code>\n import dbf\non_disk = dbf.Table('/tmp/processed_items', 'hash N(17,0); value M')\nindex = on_disk.create_index(lambda rec: rec.hash)\n\nfast_check = set()\ndef slow_check(hashed, item):\n    matches = on_disk.search((hashed,))\n    for record in matches:\n        if item == record.value:\n            return True\n    return False\n\nfor large_item in many_items:\n    hashed = hash(large_item) # only calculate once\n    if hashed not in fast_check or not slow_check(hashed, large_item):\n        on_disk.append((hashed, large_item))\n        fast_check.add(hashed)\n        process(large_item)\n</code>\n</pre>\n", "senID": 6}, {"text": ["FYI: dbf is a module I wrote which you can find on PyPI"], "childNum": 2, "tag": "p", "senID": 7, "childList": [{"text": "dbf", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"href": "http://python.org/pypi/dbf", "text": "PyPI", "childNum": 0, "tag": "a", "childList": []}]}], [{"text": ["If many_items already resides in memory, you are not creating another copy of the large_item.", "You are just storing a reference to it in the ignored set."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If many_items is a file or some other generator, you'll have to look at other alternatives."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Eg if many_items is a file, perhaps you can store a pointer to the item in the file instead of the actual item"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["As you have already seen few options but unfortunately none of them can fully address the situation partly because"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["As I read this following excerpt "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["I feel if you work on this, it might help you marginally.", "Here how it should be"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["I suppose you already have a working code to save your data in sqllite.", "You only need to define a tmpfs and use the path to store your database."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["Caveat: This is a linux only solution"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["a bloom filter?", "http://en.wikipedia.org/wiki/Bloom_filter"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://en.wikipedia.org/wiki/Bloom_filter", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Bloom_filter"}]}], [{"text": ["well you can always decorate large_item with a processed flag.", "Or something similar."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "large_item", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "processed", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}], [{"text": ["You can give a try to the str type __hash__ function."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n In [1]: hash('http://stackoverflow.com')\nOut[1]: -5768830964305142685\n</code>\n</pre>\n", "senID": 1}, {"text": ["It's definitely not a cryptographic hash function, but with a little chance you won't have too much collision.", "It works as described here: http://effbot.org/zone/python-hash.htm."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "http://effbot.org/zone/python-hash.htm", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://effbot.org/zone/python-hash.htm"}]}], [{"text": ["I suggest you profile standard Python hash functions and choose the fastest: they are all \"safe\" against collisions enough for your application."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Here are some benchmarks for hash, md5 and sha1:"], "childNum": 3, "tag": "p", "senID": 1, "childList": [{"text": "hash", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "md5", "childNum": 0, "tag": "code", "childList": []}, {"text": "sha1", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n In [37]: very_long_string = 'x' * 1000000\nIn [39]: %timeit hash(very_long_string)\n10000000 loops, best of 3: 86 ns per loop\n\nIn [40]: from hashlib import md5, sha1\n\nIn [42]: %timeit md5(very_long_string).hexdigest()\n100 loops, best of 3: 2.01 ms per loop\n\nIn [43]: %timeit sha1(very_long_string).hexdigest()\n100 loops, best of 3: 2.54 ms per loop\n</code>\n</pre>\n", "senID": 2}, {"text": ["md5 and sha1 are comparable in speed.", "hash is 20k times faster for this string and it does not seem to depend much on the size of the string itself."], "childNum": 3, "tag": "p", "senID": 3, "childList": [{"text": "md5", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "sha1", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "hash", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}], [{"text": ["how does your sql lite version work?", "If you insert all your strings into a database table and then run the query \"select distinct big_string from table_name\", the database should optimize it for you."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Another option for you would be to use hadoop."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Another option could be to split the strings into partitions such that each partition is small enough to fit in memory.", "then you only need to check for duplicates within each partition.", "the formula you use to decide the partition will choose the same partition for each duplicate.", "the easiest way is to just look at the first few digits of the string e.g."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n d=defaultdict(int)\nfor big_string in strings_generator:\n    d[big_string[:4]]+=1\nprint d\n</code>\n</pre>\n", "senID": 3}, {"text": ["now you can decide on your partitions, go through the generator again and write each big_string to a file that has the start of the big_string in the filename.", "Now you could just use your original method on each file and just loop through all the files"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["This can be achieved much more easily by performing simpler checks first, then investigating these cases with more elaborate checks.", "The example below contains extracts of your code, but it is performing the checks on much smaller sets of data.", "It does this by first matching on a simple case that is cheap to check.", "And if you find that a (filesize, checksum) pairs are not discriminating enough you can easily change it for a more cheap, yet vigorous check."], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "easily", "childNum": 0, "tag": "em", "pos": 0, "childList": []}, {"text": "but", "childNum": 0, "tag": "strong", "pos": 1, "childList": []}, {"text": "(filesize, checksum)", "childNum": 0, "tag": "code", "pos": 3, "childList": []}]}, {"code": "<pre>\n<code>\n # Need to define the following functions\ndef GetFileSize(filename):\n    pass\ndef GenerateChecksum(filename):\n    pass\ndef LoadBigString(filename):\n    pass\n\n# Returns a list of duplicates pairs.\ndef GetDuplicates(filename_list):\n    duplicates = list()\n    # Stores arrays of filename, mapping a quick hash to a list of filenames.\n    filename_lists_by_quick_checks = dict()\n    for filename in filename_list:\n        quickcheck = GetQuickCheck(filename)\n        if not filename_lists_by_quick_checks.has_key(quickcheck):\n            filename_lists_by_quick_checks[quickcheck] = list()\n        filename_lists_by_quick_checks[quickcheck].append(filename)\n    for quickcheck, filename_list in filename_lists.iteritems():\n        big_strings = GetBigStrings(filename_list)\n        duplicates.extend(GetBigstringDuplicates(big_strings))\n    return duplicates\n\ndef GetBigstringDuplicates(strings_generator):\n    processed = set()\n    for big_string in strings_generator:\n        if big_sring not in processed:\n            processed.add(big_string)\n            process(big_string)\n\n# Returns a tuple containing (filesize, checksum).\ndef GetQuickCheck(filename):\n    return (GetFileSize(filename), GenerateChecksum(filename))\n\n# Returns a list of big_strings from a list of filenames.\ndef GetBigStrings(file_list):\n    big_strings = list()\n    for filename in file_list:\n        big_strings.append(LoadBigString(filename))\n    return big_strings\n</code>\n</pre>\n", "senID": 1}]]