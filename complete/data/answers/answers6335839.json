[[{"text": ["islice() can be used to get the next n items of an iterator.", "Thus, list(islice(f, n)) will return a list of the next n lines of the file f.  Using this inside a loop will give you the file in chunks of n lines.", "At the end of the file, the list might be shorter, and finally the call will return an empty list."], "childNum": 6, "tag": "p", "senID": 0, "childList": [{"text": "islice()", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "n", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "list(islice(f, n))", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "n", "childNum": 0, "tag": "code", "pos": 2, "childList": []}, {"text": "f", "childNum": 0, "tag": "code", "childList": []}, {"text": "n", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n with open(...) as f:\n    while True:\n        next_n_lines = list(islice(f, n))\n        if not next_n_lines:\n            break\n        # process next_n_lines\n</code>\n</pre>\n", "senID": 1}, {"text": ["An alternative is to use the grouper pattern:"], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "grouper pattern", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/itertools.html#recipes"}]}, {"code": "<pre>\n<code>\n with open(...) as f:\n    for next_n_lines in izip_longest(*[f] * n):\n        # process next_n_lines\n</code>\n</pre>\n", "senID": 3}], [{"text": ["Since the requirement was added that there be statistically uniform distribution of the lines selected from the file, I offer this simple approach."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n \"\"\"randsamp - extract a random subset of n lines from a large file\"\"\"\n\nimport random\n\ndef scan_linepos(path):\n    \"\"\"return a list of seek offsets of the beginning of each line\"\"\"\n    linepos = []\n    offset = 0\n    with open(path) as inf:     \n        # WARNING: CPython 2.7 file.tell() is not accurate on file.next()\n        for line in inf:\n            linepos.append(offset)\n            offset += len(line)\n    return linepos\n\ndef sample_lines(path, linepos, nsamp):\n    \"\"\"return nsamp lines from path where line offsets are in linepos\"\"\"\n    offsets = random.sample(linepos, nsamp)\n    offsets.sort()  # this may make file reads more efficient\n\n    lines = []\n    with open(path) as inf:\n        for offset in offsets:\n            inf.seek(offset)\n            lines.append(inf.readline())\n    return lines\n\ndataset = 'big_data.txt'\nnsamp = 5\nlinepos = scan_linepos(dataset) # the scan only need be done once\n\nlines = sample_lines(dataset, linepos, nsamp)\nprint 'selecting %d lines from a file of %d' % (nsamp, len(linepos))\nprint ''.join(lines)\n</code>\n</pre>\n", "senID": 1}, {"text": ["I tested it on a mock data file of 3 million lines comprising 1.7GB on disk.", "The scan_linepos dominated the runtime taking about 20 seconds on my not-so-hot desktop. "], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "scan_linepos", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}, {"text": ["Just to check the performance of sample_lines I used the timeit module as so"], "childNum": 2, "tag": "p", "senID": 3, "childList": [{"text": "sample_lines", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "timeit", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n import timeit\nt = timeit.Timer('sample_lines(dataset, linepos, nsamp)', \n        'from __main__ import sample_lines, dataset, linepos, nsamp')\ntrials = 10 ** 4\nelapsed = t.timeit(number=trials)\nprint u'%dk trials in %.2f seconds, %.2f\u00b5s per trial' % (trials/1000,\n        elapsed, (elapsed/trials) * (10 ** 6))\n</code>\n</pre>\n", "senID": 4}, {"text": ["For various values of nsamp; when nsamp was 100, a single sample_lines completed in 460\u00b5s and scaled linearly up to 10k samples at 47ms per call."], "childNum": 3, "tag": "p", "senID": 5, "childList": [{"text": "nsamp", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "nsamp", "childNum": 0, "tag": "code", "childList": []}, {"text": "sample_lines", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["The natural next question is how good is Python&#39;s random number generator from module random?, and the answer is \"sub-cryptographic but certainly fine for bioinformatics\"."], "childNum": 1, "tag": "p", "senID": 6, "childList": [{"text": "how good is Python&#39;s random number generator from module random?", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/2145510/how-good-is-pythons-random-number-generator-from-module-random"}]}], [{"text": ["Used chunker function from What is the most \u201cpythonic\u201d way to iterate over a list in chunks?"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "What is the most \u201cpythonic\u201d way to iterate over a list in chunks?", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks"}]}, {"code": "<pre>\n<code>\n from itertools import izip_longest\n\ndef grouper(iterable, n, fillvalue=None):\n    \"grouper(3, 'ABCDEFG', 'x') --&gt; ABC DEF Gxx\"\n    args = [iter(iterable)] * n\n    return izip_longest(*args, fillvalue=fillvalue)\n\n\nwith open(filename) as f:\n    for lines in grouper(f, chunk_size, \"\"): #for every chunk_sized chunk\n        \"\"\"process lines like \n        lines[0], lines[1] , ... , lines[chunk_size-1]\"\"\"\n</code>\n</pre>\n", "senID": 1}], [{"text": ["Here is another way using groupby:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "groupby", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/itertools.html#itertools.groupby"}]}, {"code": "<pre>\n<code>\n from itertools import count, groupby\n\nN = 16\nwith open('test') as f:\n    for g, group in groupby(f, key=lambda _, c=count(): c.next()/N):\n        print list(group)\n</code>\n</pre>\n", "senID": 1}, {"text": ["How it works:"], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "How it works:", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Basically groupby() will group the lines by the return value of the key parameter and the key parameter is the lambda function lambda _, c=count(): c.next()/N and using the fact that the c argument will be bound to count() when the function will be defined so each time groupby() will call the lambda function and evaluate the return value to determine the grouper that will group the lines so :"], "childNum": 5, "tag": "p", "senID": 3, "childList": [{"text": "lambda", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/tutorial/controlflow.html#lambda-forms"}, {"text": "lambda _, c=count(): c.next()/N", "childNum": 0, "tag": "code", "childList": []}, {"href": "http://docs.python.org/library/itertools.html#itertools.count", "text": "count()", "childNum": 0, "tag": "a", "childList": []}, {"href": "http://stackoverflow.com/questions/1132941/least-astonishment-in-python-the-mutable-default-argument", "text": "function will be defined", "childNum": 0, "tag": "a", "childList": []}, {"text": "groupby()", "childNum": 0, "tag": "code", "childList": []}]}, {"code": "<pre>\n<code>\n # 1 iteration.\nc.next() =&gt; 0\n0 / 16 =&gt; 0\n# 2 iteration.\nc.next() =&gt; 1\n1 / 16 =&gt; 0\n...\n# Start of the second grouper.\nc.next() =&gt; 16\n16/16 =&gt; 1   \n...\n</code>\n</pre>\n", "senID": 4}], [{"text": ["The question appears to presume that there is efficiency to be gained by reading an \"enormous textfile\" in blocks of N lines at a time.", "This adds an application layer of buffering over the already highly optimized stdio library, adds complexity, and probably buys you absolutely nothing."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "stdio", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}, {"text": ["Thus:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n with open('my_very_large_text_file') as f:\n    for line in f:\n        process(line)\n</code>\n</pre>\n", "senID": 2}, {"text": ["is probably superior to any alternative in time, space, complexity and readability."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["See also Rob Pike's first two rules, Jackson's Two Rules, and PEP-20 The Zen of Python.", "If you really just wanted to play with islice you should have left out the large file stuff."], "childNum": 4, "tag": "p", "senID": 4, "childList": [{"text": "Rob Pike's first two rules", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://users.ece.utexas.edu/~adnan/pike.html"}, {"text": "Jackson's Two Rules", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Program_optimization#Quotes"}, {"text": "PEP-20 The Zen of Python", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://www.python.org/dev/peps/pep-0020/"}, {"text": "islice", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}], [{"text": ["Assuming \"batch\" means to want to process all 16 recs at one time instead of individually, read the file one record at a time and update a counter; when the counter hits 16, process that group.", "interim_list = []\ninfile = open(\"my_very_large_text_file\", \"r\")\nctr = 0\nfor rec in infile:\n    interim_list.append(rec)\n    ctr += 1\n    if ctr > 15:\n        process_list(interim_list)\n        interim_list = []\n        ctr = 0"], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "interim_list = []\ninfile = open(\"my_very_large_text_file\", \"r\")\nctr = 0\nfor rec in infile:\n    interim_list.append(rec)\n    ctr += 1\n    if ctr > 15:\n        process_list(interim_list)\n        interim_list = []\n        ctr = 0", "childNum": 1, "tag": "pre", "pos": 1, "childList": [{"text": "interim_list = []\ninfile = open(\"my_very_large_text_file\", \"r\")\nctr = 0\nfor rec in infile:\n    interim_list.append(rec)\n    ctr += 1\n    if ctr > 15:\n        process_list(interim_list)\n        interim_list = []\n        ctr = 0", "tag": "code"}]}, {"text": "interim_list = []\ninfile = open(\"my_very_large_text_file\", \"r\")\nctr = 0\nfor rec in infile:\n    interim_list.append(rec)\n    ctr += 1\n    if ctr > 15:\n        process_list(interim_list)\n        interim_list = []\n        ctr = 0", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["the final group"], "childNum": 0, "tag": "h2", "senID": 1, "childList": []}, {"text": ["process_list(interim_list)\n"], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "", "childNum": 0, "tag": "br", "pos": 0, "childList": []}]}]]