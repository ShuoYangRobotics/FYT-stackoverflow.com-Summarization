[[{"text": ["When going Python, you might be interested in mechanize and BeautifulSoup."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "mechanize", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://wwwsearch.sourceforge.net/mechanize/"}, {"href": "http://www.crummy.com/software/BeautifulSoup/", "text": "BeautifulSoup", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["Mechanize sort of simulates a browser (including options for proxying, faking browser identifications, page redirection etc.", ") and allows easy fetching of forms, links, ...", "The documentation is a bit rough/sparse though."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "Mechanize", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Some example code (from the mechanize website) to give you an idea:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n import mechanize\nbr = mechanize.Browser()\nbr.open(\"http://www.example.com/\")\n# follow second link with element text matching regular expression\nhtml_response = br.follow_link(text_regex=r\"cheese\\s*shop\", nr=1)\nprint br.title()\nprint  html_response\n</code>\n</pre>\n", "senID": 3}, {"text": ["BeautifulSoup allows to parse html content (which you could have fetched with mechanize) pretty easily, and supports regexes."], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "BeautifulSoup", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Some example code:"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n from BeautifulSoup import BeautifulSoup\nsoup = BeautifulSoup(html_response)\n\nrows = soup.findAll('tr')\nfor r in rows[2:]:  #ignore first two rows\n    cols = r.findAll('td')\n    print cols[0].renderContents().strip()    #print content of first column\n</code>\n</pre>\n", "senID": 6}, {"text": ["So, these 10 lines above are pretty much copy-paste ready to print the content of the first column of every table row on a website."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}], [{"text": ["There really is no good solution here.", "You are right as you suspect that Python is probably the best way to start because of it's incredibly strong support of regular expression."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["In order to implement something like this, strong knowledge of SEO (Search Engine Optimization) would help since effectively optimizing a webpage for search engines tells you how search engines behave.", "I would start with a site like SEOMoz."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "SEOMoz", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://www.seomoz.org"}]}, {"text": ["As far as identifying the \"about us\" page, you only have 2 options:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["a) For each page get the link of the about us page and feed it to your crawler."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["b) Parse all the links of the page for certain keywords like \"about us\", \"about\" \"learn more\" or whatever."], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "links", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["in using option b, be careful as you could get stuck in an infinite loop since a website will link to the same page many times especially if the link is in the header or footer a page may link back to itself even.", "To avoid this you'll need to create a list of visited links and make sure not to revisit them."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["Finally, I would recommend having your crawler respect instructions in the robot.txt file and it's probably a great idea not to follow links marked rel=\"nofollow\" as these are mostly used on external links.", "Again, learn this and more by reading up on SEO."], "childNum": 2, "tag": "p", "senID": 6, "childList": [{"text": "robot.txt", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "rel=\"nofollow\"", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}, {"text": ["Regards,"], "childNum": 0, "tag": "p", "senID": 7, "childList": []}], [{"text": ["Try out scrapy.", "It is a web scraping library for python.", "If a simple python-script is expected, try urllib2 in python."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "scrapy", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://scrapy.org"}, {"text": "urllib2", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/urllib2.html"}]}], [{"text": ["Heritrix has a bit of a steep learning curve, but can be configured in such a way that only the homepage, and a page that \"looks like\" (using a regex filter) an about page will get crawled."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Heritrix", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://crawler.archive.org/"}]}, {"text": ["More open source Java (web) crawlers: http://java-source.net/open-source/crawlers"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "http://java-source.net/open-source/crawlers", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://java-source.net/open-source/crawlers"}]}], [{"text": ["If you are going to builld a crawler you need to  (Java specific):"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["A bunch of other stuff too."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["It's not that difficult, but there are lots of fiddly edge cases (e.g.", "redirects, detecting encoding (checkout Tika))."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["For more basic requirements you could use wget.", "Heretrix is another option, but yet another framework to learn."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["Identifying About us pages can be done using various heuristics:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["if you wanted to be more quantitative about it you could use machine learning and a classifier (maybe Bayesian)."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["Saving the front page is obviously easier but front page redirects (sometimes to different domains, and often implemented in the HTML  meta redirect tag or even JS) are very common so you need to handle this."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}], [{"text": ["Python ==> Curl &lt;-- the best implementation of crawler"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["The following code can crawl 10,000 pages in 300 secs on a nice server. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n #! /usr/bin/env python\n# -*- coding: iso-8859-1 -*-\n# vi:ts=4:et\n# $Id: retriever-multi.py,v 1.29 2005/07/28 11:04:13 mfx Exp $\n\n#\n# Usage: python retriever-multi.py &lt;file with URLs to fetch&gt; [&lt;# of\n#          concurrent connections&gt;]\n#\n\nimport sys\nimport pycurl\n\n# We should ignore SIGPIPE when using pycurl.NOSIGNAL - see\n# the libcurl tutorial for more info.\ntry:\n    import signal\n    from signal import SIGPIPE, SIG_IGN\n    signal.signal(signal.SIGPIPE, signal.SIG_IGN)\nexcept ImportError:\n    pass\n\n\n# Get args\nnum_conn = 10\ntry:\n    if sys.argv[1] == \"-\":\n        urls = sys.stdin.readlines()\n    else:\n        urls = open(sys.argv[1]).readlines()\n    if len(sys.argv) &gt;= 3:\n        num_conn = int(sys.argv[2])\nexcept:\n    print \"Usage: %s &lt;file with URLs to fetch&gt; [&lt;# of concurrent connections&gt;]\" % sys.argv[0]\n    raise SystemExit\n\n\n# Make a queue with (url, filename) tuples\nqueue = []\nfor url in urls:\n    url = url.strip()\n    if not url or url[0] == \"#\":\n        continue\n    filename = \"doc_%03d.dat\" % (len(queue) + 1)\n    queue.append((url, filename))\n\n\n# Check args\nassert queue, \"no URLs given\"\nnum_urls = len(queue)\nnum_conn = min(num_conn, num_urls)\nassert 1 &lt;= num_conn &lt;= 10000, \"invalid number of concurrent connections\"\nprint \"PycURL %s (compiled against 0x%x)\" % (pycurl.version, pycurl.COMPILE_LIBCURL_VERSION_NUM)\nprint \"----- Getting\", num_urls, \"URLs using\", num_conn, \"connections -----\"\n\n\n# Pre-allocate a list of curl objects\nm = pycurl.CurlMulti()\nm.handles = []\nfor i in range(num_conn):\n    c = pycurl.Curl()\n    c.fp = None\n    c.setopt(pycurl.FOLLOWLOCATION, 1)\n    c.setopt(pycurl.MAXREDIRS, 5)\n    c.setopt(pycurl.CONNECTTIMEOUT, 30)\n    c.setopt(pycurl.TIMEOUT, 300)\n    c.setopt(pycurl.NOSIGNAL, 1)\n    m.handles.append(c)\n\n\n# Main loop\nfreelist = m.handles[:]\nnum_processed = 0\nwhile num_processed &lt; num_urls:\n    # If there is an url to process and a free curl object, add to multi stack\n    while queue and freelist:\n        url, filename = queue.pop(0)\n        c = freelist.pop()\n        c.fp = open(filename, \"wb\")\n        c.setopt(pycurl.URL, url)\n        c.setopt(pycurl.WRITEDATA, c.fp)\n        m.add_handle(c)\n        # store some info\n        c.filename = filename\n        c.url = url\n    # Run the internal curl state machine for the multi stack\n    while 1:\n        ret, num_handles = m.perform()\n        if ret != pycurl.E_CALL_MULTI_PERFORM:\n            break\n    # Check for curl objects which have terminated, and add them to the freelist\n    while 1:\n        num_q, ok_list, err_list = m.info_read()\n        for c in ok_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print \"Success:\", c.filename, c.url, c.getinfo(pycurl.EFFECTIVE_URL)\n            freelist.append(c)\n        for c, errno, errmsg in err_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print \"Failed: \", c.filename, c.url, errno, errmsg\n            freelist.append(c)\n        num_processed = num_processed + len(ok_list) + len(err_list)\n        if num_q == 0:\n            break\n    # Currently no more I/O is pending, could do something in the meantime\n    # (display a progress bar, etc.).\n    # We just call select() to sleep until some more data is available.\n    m.select(1.0)\n\n\n# Cleanup\nfor c in m.handles:\n    if c.fp is not None:\n        c.fp.close()\n        c.fp = None\n    c.close()\nm.close()\n</code>\n</pre>\n", "senID": 2}]]