[[{"text": ["As a general rule, a given construct in any language is not slow until it is measured."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["In Python, not only do timings often run counter to intuition but the tools for measuring execution time are exceptionally good."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "tools for measuring execution time", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/timeit.html"}]}], [{"text": ["1) Are you opening the same site many times, or many different site?", "If many different sites, I think urllib2 is good.", "If doing the same site over and over again, I have had some personal luck with urllib3 http://code.google.com/p/urllib3/"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://code.google.com/p/urllib3/", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://code.google.com/p/urllib3/"}]}, {"text": ["2) BeautifulSoup is easy to use, but is pretty slow.", "If you do have to use it, make sure to decompose your tags to get rid of memory leaks.. or it will likely lead to memory issues (did for me). "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["What do your memory and cpu look like?", "If you are maxing your CPU, make sure you are using real heavyweight threads, so you can run on more than 1 core."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Scrapy might be useful for you.", "If you don't need all of its functionality, you might just use twisted's twisted.web.client.getPage instead.", "Asynchronous IO in one thread is going to be way more performant and easy to debug than anything that uses multiple threads and blocking IO."], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "Scrapy", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://scrapy.org/"}, {"text": "twisted", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://twistedmatrix.com/"}, {"text": "twisted.web.client.getPage", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}], [{"text": ["Why is Dumb Guy's answer got -1?", "He is using old modules alright.", "But he is the first guy that propose a right approach using threads and provided a working example."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["I'm rewriting his code using modern Python modules like threading and Queue."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n import threading, urllib2\nimport Queue\n\nurls_to_load = [\n'http://stackoverflow.com/',\n'http://slashdot.org/',\n'http://www.archive.org/',\n'http://www.yahoo.co.jp/',\n]\n\ndef read_url(url, queue):\n    data = urllib2.urlopen(url).read()\n    print('Fetched %s from %s' % (len(data), url))\n    queue.put(data)\n\ndef fetch_parallel():\n    result = Queue.Queue()\n    threads = [threading.Thread(target=read_url, args = (url,result)) for url in urls_to_load]\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n    return result\n\ndef fetch_sequencial():\n    result = Queue.Queue()\n    for url in urls_to_load:\n        read_url(url,result)\n    return result\n</code>\n</pre>\n", "senID": 2}, {"text": ["Best time for find_sequencial() is 2s.", "Best time for fetch_parallel() is 0.9s."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["Also it is incorrect to say thread is useless in Python because of GIL.", "This is one of those case when thread is useful in Python because the the threads are blocked on I/O.", "As you can see in my result the parallel case is 2 times faster."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["Edit: Please take a look at Wai's post for a better version of this code.", "Note that there is nothing wrong with this code and it will work properly, despite the comments below."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "Edit:", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "will work properly", "childNum": 0, "tag": "strong", "pos": 1, "childList": []}]}, {"text": ["The speed of reading web pages is probably bounded by your Internet connection, not Python."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["You could use threads to load them all at once."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n import thread, time, urllib\nwebsites = {}\ndef read_url(url):\n  websites[url] = urllib.open(url).read()\n\nfor url in urls_to_load: thread.start_new(read_url, (url,))\nwhile websites.keys() != urls_to_load: time.sleep(0.1)\n\n# Now websites will contain the contents of all the web pages in urls_to_load\n</code>\n</pre>\n", "senID": 3}], [{"text": ["How about using pycurl? "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["You can apt-get it by"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n$ sudo apt-get python-pycurl\n</pre>\n", "senID": 2}]]