[[{"text": ["take a look at pytables.", "One of the advantages is you can work with very large amounts of data, stored on disk, as if it were in memory."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "pytables", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.pytables.org/moin"}]}, {"text": ["edit: Because the I/O performance will be a bottleneck (if not THE bottleneck), you will want to consider SSD technology: high I/O per second and virtually no seeking times.", "The size of your project is perfect for todays affordable SSD 'drives'."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["A couple libraries come to mind which you might want to evaluate:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 2, "lis": [{"text": ["joblib - Makes parallel computation easy, and provides transparent disk-caching of output and lazy re-evaluation."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "joblib", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://packages.python.org/joblib/"}]}, {"text": ["mrjob - Makes it easy to write Hadoop streaming jobs on Amazon Elastic MapReduce or your own Hadoop cluster."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "mrjob", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://packages.python.org/mrjob/"}]}]}], [{"text": ["Two ideas:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["It seems you're on the edge of what you can do with your hardware.", "It would help if you could describe what hardware (mostly, RAM) is available to you for this task.", "If there are 100k vectors, each of them with 1M ints, this gives ~370GB.", "If multiple passes method is viable and you've got a machine with 16GB RAM, then it is about ~25 passes -- should be easy to parallelize if you've got a cluster."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["You didn't mention either way, but if you're not, you should use NumPy arrays for your lists rather than native Python lists, which should help speed things up and reduce memory usage, as well as making whatever math you're doing faster and easier."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "NumPy", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://numpy.scipy.org/"}]}, {"text": ["If you're at all familiar with C/C++, you might also look into Cython, which lets you write some or all of your code in C, which is much faster than Python, and integrates well with NumPy arrays.", "You might want to profile your code to find out which spots are taking the most time, and write those sections in C."], "childNum": 2, "tag": "p", "senID": 1, "childList": [{"text": "Cython", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://cython.org/"}, {"text": "profile", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/profile.html"}]}, {"text": ["It's hard to say what the best approach will be, but of course any speedups you can make in critical parts of will help.", "Also keep in mind that once RAM is exhausted, your program will start running in virtual memory on disk, which will probably cause far more disk I/O activity than the program itself, so if you're concerned about disk I/O, your best bet is probably to make sure that the batch of data you're working on in memory doesn't get much greater than available RAM."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Hard to say exactly because there are a few details missing, eg.", "is this a dedicated box?", "Does the process run on several machines?", "Does the avail memory change?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["In general I recommend not reimplementing the job of the operating system.  "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Note this next paragraph doesn't seem to apply since the whole file is read each time:\nI'd test implementation three, giving it a healthy disk cache and see what happens.", "With plenty of cache performance might not be as bad as you'd expect."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["You'll also want to cache expensive calculations that will be needed soon.", "In short, when an expensive operation is calculated that can be used again, you store it in a dictionary (or perhaps disk, memcached, etc), and then look there first before calculating again.", "The Django docs have a good introduction."], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "introduction", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "https://docs.djangoproject.com/en/dev/topics/cache/"}]}], [{"text": ["Use a database.", "That problem seems large enough that language choice (Python, Perl, Java, etc) won't make a difference.", "If each dimension of the vector is a column in the table, adding some indexes is probably a good idea.", "In any case this is a lot of data and won't process terribly quickly."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["I'd suggest to do it this way: "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["1) Construct the easy pipeline you mentioned"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["2) Construct your vectors in memory and \"flush\" them into a DB.", "( Redis and MongoDB are good candidates)"], "childNum": 2, "tag": "p", "senID": 2, "childList": [{"text": "Redis", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://redis.io"}, {"href": "http://www.mongodb.org", "text": "MongoDB", "childNum": 0, "tag": "a", "childList": []}]}, {"text": ["3) Determine how much memory this procedure consumes and parallelize accordingly ( or even better use a map/reduce approach, or a distributed task queue like celery)"], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "celery", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://celeryproject.org/"}]}, {"text": ["Plus all the tips mentioned before (numPy etc..)"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["Think about using an existing in-memory DB solution like Redis.", "The problem of switching to disk once RAM is gone and tricks to tweak this process should already be in place.", "Python client as well."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Redis", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://redis.io/"}]}, {"text": ["Moreover this solution could scale vertically without much effort."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["From another comment I infer that your corpus fits into the memory, and you have some cores to throw at the problem, so I would try this:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 3, "lis": [{"text": "Find a method to have your corpus in memory. This might be a sort of ram disk with file system, or a database. No idea, which one is best for you. ", "tag": "none", "senID": 1}, {"text": ["Have a smallish shell script monitor ram usage, and spawn every second another process of the following, as long as there is x memory left (or, if you want to make things a bit more complex, y I/O bandwith to disk):"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["in the end you can collect and combine all vectors, if needed (this would be the reduce part)"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}]}], [{"text": ["Many of the methods discussed by others on this page are very helpful, and I recommend that anyone else needing to solve this sort of problem look at them."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["One of the crucial aspects of this problem is deciding when to stop building vectors (or whatever you're building) in memory and dump stuff to disk.", "This requires a (pythonesque) way of determining how much memory one has left."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["It turns out that the psutil python module does just the trick."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "psutil", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://code.google.com/p/psutil/"}]}, {"text": ["For example say I want to have a while-loop that adds stuff to a Queue for other processes to deal with until my RAM is 80% full.", "The follow pseudocode will do the trick:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n while (someCondition):\n   if psutil.phymem_usage().percent &gt; 80.0:\n      dumpQueue(myQueue,somefile)\n   else:\n      addSomeStufftoQueue(myQueue,stuff)\n</code>\n</pre>\n", "senID": 4}, {"text": ["This way you can have one process tracking memory usage and deciding that it's time to write to disk and free up some system memory (deciding which vectors to cache is a separate problem)."], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["PS.", "Props to to Sean for suggesting this module."], "childNum": 1, "tag": "p", "senID": 6, "childList": [{"text": "Sean", "tag": "a", "pos": 1, "childList": [], "childNum": 0, "href": "http://twitter.com/#!/seanhn"}]}], [{"text": ["Split the corpus evenly in size between parallel jobs (one per core) - process in parallel, ignoring any incomplete line (or if you cannot tell if it is incomplete, ignore the first and last line of that each job processes)."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["That's the map part."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Use one job to merge the 20+ sets of vectors from each of the earlier jobs - That's the reduce step."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["You stand to loose information from 2*N lines where N is the number of parallel processes, but you gain by not adding complicated logic to try and capture these lines for processing."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}]]