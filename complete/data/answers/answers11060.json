[[{"text": ["I started writing up a summary of my experience with my own code generator, then went back and re-read your question and found you had already touched upon the same issues yourself, focus on the execution results instead of the code layout/look."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Problem is, this is hard to test, the generated code might not be suited to actually run in the environment of the unit test system, and how do you encode the expected results?"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["I've found that you need to break down the code generator into smaller pieces and unit test those.", "Unit testing a full code generator is more like integration testing than unit testing if you ask me."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Recall that \"unit testing\" is only one kind of testing.", "You should be able to unit test the internal pieces of your code generator.", "What you're really looking at here is system level testing (a.k.a.", "regression testing).", "It's not just semantics... there are different mindsets, approaches, expectations, etc.", "It's certainly more work, but you probably need to bite the bullet and set up an end-to-end regression test suite: fixed C++ files -> SWIG interfaces -> python modules -> known output.", "You really want to check the known input (fixed C++ code) against expected output (what comes out of the final Python program).", "Checking the code generator results directly would be like diffing object files..."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "internal", "childNum": 0, "tag": "strong", "pos": 1, "childList": []}]}], [{"text": ["This is indeed a problem.", "And the code i'm generating is not suited very well to running in a test environment: it uses some complex Python tricks to register available SOAP methods etc and mocking or detecting this would lead to more obscure tests."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["In terms of encoding expectations, in this case I was going to have it generate code to access custom test specific code which I'd compile and link in, then I would make calls via my generated interface, and ask the underlying classes if the right thing had happened."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["This again, would lead to obscure tests, since the test logic would be split accross several places.", "I think unit tests should not be factored in this manner if possible."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Can you give an example of how you split your generator up?", "What were the components, and can this be generalised for other cases? "], "childNum": 0, "tag": "p", "senID": 3, "childList": []}], [{"text": ["Yes, results are the ONLY thing that matters.", "The real chore is writing a framework that allows your generated code to run independently... spend your time there."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}], [{"text": ["If you are running on *nux you might consider dumping the unittest framework in favor of a bash script or makefile.", "on windows you might consider building a shell app/function that runs the generator and then uses the code (as another process) and unittest that."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["A third option would be to generate the code and then build an app from it that includes nothing but a unittest.", "Again you would need a shell script or whatnot to run this for each input.", "As to how to encode the expected behavior, it occurs to me that it could be done in much the same way as you would for the C++ code just using the generated interface rather than the C++ one."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Just wanted to point out that you can still achieve fine-grained testing while verifying the results: you can test individual chunks of code by nesting them inside some setup and verification code:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n int x = 0;\nGENERATED_CODE\nassert(x == 100);\n</code>\n</pre>\n", "senID": 1}, {"text": ["Provided you have your generated code assembled from smaller chunks, and the chunks do not change frequently, you can exercise more conditions and test a little better, and hopefully avoid having all your tests break when you change specifics of one chunk."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Unit testing is just that testing a specific unit.", "So if you are writing a specification for class A, it is ideal if class A does not have the real concrete versions of class B and C."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Ok I noticed afterwards the tag for this question includes C++ / Python, but the principles are the same:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n public class A : InterfaceA \n    {   \n      InterfaceB b;\n\n      InterfaceC c;\n\n      public A(InterfaceB b, InterfaceC c)   {\n          this._b = b;\n          this._c = c;   }\n\n      public string SomeOperation(string input)   \n      {\n          return this._b.SomeOtherOperation(input) \n               + this._c.EvenAnotherOperation(input); \n      } \n    }\n</code>\n</pre>\n", "senID": 2}, {"text": ["Because the above System A injects interfaces to systems B and C, you can unit test just system A, without having real functionality being executed by any other system.", "This is unit testing."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["Here is a clever manner for approaching a System from creation to completion, with a different When specification for each piece of behaviour:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"code": "<pre>\n<code>\n public class When_system_A_has_some_operation_called_with_valid_input : SystemASpecification\n{\n    private string _actualString;\n\n    private string _expectedString;\n\n    private string _input;\n\n    private string _returnB;\n\n    private string _returnC;\n\n    [It]\n    public void Should_return_the_expected_string()\n    {\n        _actualString.Should().Be.EqualTo(this._expectedString);\n    }\n\n    public override void GivenThat()\n    {\n        var randomGenerator = new RandomGenerator();\n        this._input = randomGenerator.Generate&lt;string&gt;();\n        this._returnB = randomGenerator.Generate&lt;string&gt;();\n        this._returnC = randomGenerator.Generate&lt;string&gt;();\n\n        Dep&lt;InterfaceB&gt;().Stub(b =&gt; b.SomeOtherOperation(_input))\n                         .Return(this._returnB);\n        Dep&lt;InterfaceC&gt;().Stub(c =&gt; c.EvenAnotherOperation(_input))\n                         .Return(this._returnC);\n\n        this._expectedString = this._returnB + this._returnC;\n    }\n\n    public override void WhenIRun()\n    {\n        this._actualString = Sut.SomeOperation(this._input);\n    }\n}\n</code>\n</pre>\n", "senID": 5}, {"text": ["So in conclusion, a single unit / specification can have multiple behaviours, and the specification grows as you develop the unit / system; and if your system under test depends on other concrete systems within it, watch out."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}], [{"text": ["My recommendation would be to figure out a set of known input-output results, such as some simpler cases that you already have in place, and unit test the code that is produced.", "It's entirely possible that as you change the generator that the exact string that is produced may be slightly different... but what you really care is whether it is interpreted in the same way.", "Thus, if you test the results as you would test that code if it were your feature, you will find out if it succeeds in the ways you want."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "unit test the code that is produced", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"text": ["Basically, what you really want to know is whether your generator will produce what you expect without physically testing every possible combination (also: impossible).", "By ensuring that your generator is consistent in the ways you expect, you can feel better that the generator will succeed in ever-more-complex situations."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["In this way, you can also build up a suite of regression tests (unit tests that need to keep working correctly).", "This will help you make sure that changes to your generator aren't breaking other forms of code.", "When you encounter a bug that your unit tests didn't catch, you may want to include it to prevent similar breakage."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}]]