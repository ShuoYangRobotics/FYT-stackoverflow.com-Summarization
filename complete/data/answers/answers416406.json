[[{"text": ["Just so you know, you are asking for a solution to an ill-posed problem: no definitive solution exists.", "That's fine...it just makes it more fun.", "Your problem is ill-posed mostly because you don't know how many clusters you want.", "Clustering is one of the key areas of machine learning and there a quite a few approaches that have been developed over the years."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "ill-posed problem: no definitive solution exists.  That's fine...it just makes it more fun.  Your problem is ill-posed mostly because you don't know how many clusters you want.  Clustering is one of the key areas of machine learning", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Ill-posed"}]}, {"text": ["As Arachnid pointed out, the k-means algorithm tends to be a good one and it's pretty easy to implement.", "The results depend critically on the initial guess made and on the number of desired clusters.", "To overcome the initial guess problem, it's common to run the algorithm many times with random initializations and pick the best result.", "You'll need to define what \"best\" means.", "One measure would be the mean squared distance of each point to its cluster center.", "If you want to automatically guess how many clusters there are, you should run the algorithm with a whole range of numbers of clusters.", "For any good \"best\" measure, more clusters will always look better than fewer, so you'll need a way to penalize having too many clusters.", "The MDL discussion on wikipedia is a good starting point.  "], "childNum": 2, "tag": "p", "senID": 1, "childList": [{"text": "k-means", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/K_means"}, {"text": "MDL", "tag": "a", "pos": 7, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Minimum_description_length"}]}, {"text": ["K-means clustering is basically the simplest mixture model.", "Sometimes it's helpful to upgrade to a mixture of Gaussians learned by expectation maximization (described in the link just given).", "This can be more robust than k-means.", "It takes a little more effort to understand it, but when you do, it's not much harder than k-means to implement.  "], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "mixture model", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Mixture_model"}]}, {"text": ["There are plenty of other clustering techniques such as agglomerative clustering and spectral clustering.", "Agglomerative clustering is pretty easy to implement, but choosing when to stop building the clusters can be tricky.", "If you do agglomerative clustering, you'll probably want to look at kd trees for faster nearest neighbor searches.", "smacl's answer describes one slightly different way of doing agglomerative clustering using a Voronoi diagram."], "childNum": 2, "tag": "p", "senID": 3, "childList": [{"text": "clustering techniques", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Data_clustering"}, {"text": "kd trees", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Kd-tree"}]}, {"text": ["There are models that can automatically choose the number of clusters for you such as ones based on Latent Dirichlet Allocation, but they are a lot harder to understand an implement correctly.  "], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "Latent Dirichlet Allocation", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"}]}, {"text": ["You might also want to look at the mean-shift algorithm to see if it's closer to what you really want."], "childNum": 1, "tag": "p", "senID": 5, "childList": [{"text": "mean-shift", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.wisdom.weizmann.ac.il/~deniss/vision_spring04/files/mean_shift/mean_shift.ppt"}]}], [{"text": ["It sounds to me like you're looking for the K-means algorithm."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "K-means", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/K_means"}]}], [{"text": ["As I mentioned in the comment to your question, the answer is based on whether or not mass can be considered scalar in this context.", "If so, color based solutions are probably not going to work as color is often not taken as being scalar."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["For example, if I have a given area with 1 point of high mass, is that the same as having the same area with 10 points of 1/10 the mass?", "If this is true, mass is not scalar in this context, and I would tend to look at an algorithm used for spatially gouping similar non-scalable values, e.g.", "voronoi diagrams."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "voronoi diagrams", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Voronoi_diagram"}]}, {"src": "http://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Coloured_Voronoi_2D.svg/600px-Coloured_Voronoi_2D.svg.png", "tag": "img", "senID": 2}, {"text": ["In this case, where two adjacent voronoi areas have a close enough mass match and distance, they can be clustered together.", "You could repeat this to find all clusters."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["If on the other hand, your mass is scalable, or that the mass at an unknown position can be interpolated from surrounding points, I would tend to triangulate and contour the input data and use areas between contours to find clusters of similar mass."], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "triangulate", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Delaunay_trianglulation"}]}], [{"text": ["This sounds like color quantization, where you reduce the number of colors in an image.", "One way would be to plot the colors in space, and combine clusters into the center (or a weighted average) of a cluster."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["The exact name of the algorithm that triggered this memory fails me, but I'll edit the answer if it pops up, but in the meantime, you should look at color quantization and see if some of the algorithms are useful."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Start with the \"Convex Hull\" problem.", "You're also looking for some \"convex hull\"-like clusters."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Convex Hull", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Convex_hull"}]}, {"text": ["Note that \"clusters\" is vague.", "You have an average mass across your field.", "Some points have above average mass, and some below average.", "How far above average means you've found a cluster?", "How far apart do nodes have to be to be part of a cluster or a separate cluster?"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["What's the difference between two mountain peaks and a ridge?  "], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["You have to compute a \"topography\" - joining all points with equal density into regions.", "This requires that you pick a spot and work your want out from a point radially, locating positions where the densities are equal.", "You can connect those points into regions."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["If you picked your initial point wisely, the regions should nest.", "Picking your starting point is easy because you start at local highs."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}], [{"text": ["Since you are already talking about mass, why not a gravity based solution.", "A simple particle system would not need to be super accurate, and you would not have to run it for too long before you could make a much better guess at the number of clusters."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If you have a better idea about cluster numbers, k-means nearest neighbour becomes feasible."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}]]