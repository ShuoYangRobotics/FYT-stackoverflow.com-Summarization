[[{"text": ["I'm not sure if I completely understand what you're trying to do, is something like this?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n f1 = open ('car_names.txt')\nf2 = open ('car_descriptions.txt')\nfor car_name in f1.readlines ():\n        for i in range (6):   # echo the first 6 lines\n                print f2.readline ()\n        assert f2.readline() == '@CAR_NAME'  # skip the 7th, but assert that it is @CAR_NAME\n        print car_name    # print the real car name\n        for i in range (33):  # print the remaining 33 of the original 40\n               print f2.readline ()\n</code>\n</pre>\n", "senID": 1}], [{"text": ["First, make a generator that retrieves the car name from a sequence.", "You could yield every 7th line; I've made mine yield whatever line follows the line that starts with @CAR_NAME:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "@CAR_NAME", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}, {"code": "<pre>\n<code>\n def car_names(seq):\n    yieldnext=False\n    for line in seq:\n        if yieldnext: yield line\n        yieldnext = line.startswith('@CAR_NAME')\n</code>\n</pre>\n", "senID": 1}, {"text": ["Now you can use itertools.izip to go through both sequences in parallel:"], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "itertools.izip", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n from itertools import izip\nwith open(r'c:\\temp\\cars.txt') as f1:\n    with open(r'c:\\temp\\car_names.txt') as f2:\n        for (c1, c2) in izip(f1, car_names(f2)):\n            print c1, c2\n</code>\n</pre>\n", "senID": 3}], [{"text": ["Reading car_names.txt will save you a piddling amount of memory (really really tiny by today's standards;-) but it absolutely won't be any faster than slurping it down at one gulp (best case it will be exactly the same speed, probably even a little bit slower unless your underlying operating system and storage system do a great job at read-lookahead caching / buffering).", "So I suggest:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "car_names.txt", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"code": "<pre>\n<code>\n import fileinput\n\ncarnames = open('car_names.txt').readlines()\ncarnamit = iter(carnames)\n\nskip = False\nfor line in fileinput.input(['car_descriptions.txt'], True, '.bak'):\n  if not skip:\n    print line,\n  if '@CAR_NAME' in line:\n    print next(carnamit),\n    skip = True\n  else:\n    skip = False\n</code>\n</pre>\n", "senID": 1}, {"text": ["So measure the speed of this, and an alternative that does"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n carnamit = open('car_names.txt')\n</code>\n</pre>\n", "senID": 3}, {"text": ["at the start instead of reading all lines over a list like my first version -- I bet that the first version (in as much as there's any measurable and repeatable difference) will prove to be faster."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["BTW, the fileinput module of the standard library is documented here, and it's truly a convenient way to perform \"virtual rewriting in-place\" of text files (typically keeping the old version as a backup, just in case -- but even if the machine should crash in the middle of the operation the old version of the data will still be there, so in a sense the \"rewriting\" operates atomically with respect to machine crashes, a nice little touch;-)."], "childNum": 1, "tag": "p", "senID": 5, "childList": [{"text": "here", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://docs.python.org/library/fileinput.html?highlight=fileinput.input#fileinput.input"}]}], [{"code": "<pre>\n<code>\n for line1, line2 in zip(file(filename1), file(filename2)):\n    # do your thing\n</code>\n</pre>\n", "senID": 0}, {"text": ["or similar"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["12340 is not any data (in sense that there are much bigger data to process on the market)."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Even better approach would use build in sqlite module.", "If not use some simple format like CSV for example.", "This is a structure organized.", "If not use threads, you could process two files simultaneously.  "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["I think this fits the question:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 2, "lis": [{"text": "it reads the description file one line at a time", "tag": "none", "senID": 1}, {"text": "when it sees @CAR_NAME, it still emits it, but replaces the next line in the description file with the next line from the names file\n", "tag": "none", "senID": 2}]}, {"code": "<pre>\n<code>\n def merge_car_descriptions(namefile, descrfile):\n    names = open(namefile,'r')\n    descr = open(descrfile,'r')\n    for d in descr:\n        if '@CAR_NAME' in d:\n            yield d + names.readline()\n            descr.next()\n        else:\n            yield d\n\nif __name__=='__main__':\n    import sys\n    if len(sys.argv) != 3:\n        sys.exit(\"Syntax: %s car_names.txt car_descriptions.txt\" % sys.argv[0])\n    for l in merge_car_descriptions(sys.argv[1], sys.argv[2]):\n        print l,\n</code>\n</pre>\n", "senID": 3}]]