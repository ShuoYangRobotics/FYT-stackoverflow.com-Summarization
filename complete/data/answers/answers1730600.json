[[{"text": ["Months later, here's a small class PCA, and a picture:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n #!/usr/bin/env python\n\"\"\" a small class for Principal Component Analysis\nUsage:\n    p = PCA( A, fraction=0.90 )\nIn:\n    A: an array of e.g. 1000 observations x 20 variables, 1000 rows x 20 columns\n    fraction: use principal components that account for e.g.\n        90 % of the total variance\n\nOut:\n    p.U, p.d, p.Vt: from numpy.linalg.svd, A = U . d . Vt\n    p.dinv: 1/d or 0, see NR\n    p.eigen: the eigenvalues of A*A, in decreasing order (p.d**2).\n        eigen[j] / eigen.sum() is variable j's fraction of the total variance;\n        look at the first few eigen[] to see how many PCs get to 90 %, 95 % ...\n    p.npc: number of principal components,\n        e.g. 2 if the top 2 eigenvalues are &gt;= `fraction` of the total.\n        It's ok to change this; methods use the current value.\n\nMethods:\n    The methods of class PCA transform vectors or arrays of e.g.\n    20 variables, 2 principal components and 1000 observations,\n    using partial matrices U' d' Vt', parts of the full U d Vt:\n    A ~ U' . d' . Vt' where e.g.\n        U' is 1000 x 2\n        d' is diag([ d0, d1 ]), the 2 largest singular values\n        Vt' is 2 x 20.  Dropping the primes,\n\n    d . Vt      2 principal vars = p.vars_pc( 20 vars )\n    U           1000 obs = p.pc_obs( 2 principal vars )\n    U . d . Vt  1000 obs, p.obs( 20 vars ) = pc_obs( vars_pc( vars ))\n        fast approximate A . vars, using the `npc` principal components\n\n    Ut              2 pcs = p.obs_pc( 1000 obs )\n    V . dinv        20 vars = p.pc_vars( 2 principal vars )\n    V . dinv . Ut   20 vars, p.vars( 1000 obs ) = pc_vars( obs_pc( obs )),\n        fast approximate Ainverse . obs: vars that give ~ those obs.\n\n\nNotes:\n    PCA does not center or scale A; you usually want to first\n        A -= A.mean(A, axis=0)\n        A /= A.std(A, axis=0)\n    with the little class Center or the like, below.\n\nSee also:\n    http://en.wikipedia.org/wiki/Principal_component_analysis\n    http://en.wikipedia.org/wiki/Singular_value_decomposition\n    Press et al., Numerical Recipes (2 or 3 ed), SVD\n    PCA micro-tutorial\n    iris-pca .py .png\n\n\"\"\"\n\nfrom __future__ import division\nimport numpy as np\ndot = np.dot\n    # import bz.numpyutil as nu\n    # dot = nu.pdot\n\n__version__ = \"2010-04-14 apr\"\n__author_email__ = \"denis-bz-py at t-online dot de\"\n\n#...............................................................................\nclass PCA:\n    def __init__( self, A, fraction=0.90 ):\n        assert 0 &lt;= fraction &lt;= 1\n            # A = U . diag(d) . Vt, O( m n^2 ), lapack_lite --\n        self.U, self.d, self.Vt = np.linalg.svd( A, full_matrices=False )\n        assert np.all( self.d[:-1] &gt;= self.d[1:] )  # sorted\n        self.eigen = self.d**2\n        self.sumvariance = np.cumsum(self.eigen)\n        self.sumvariance /= self.sumvariance[-1]\n        self.npc = np.searchsorted( self.sumvariance, fraction ) + 1\n        self.dinv = np.array([ 1/d if d &gt; self.d[0] * 1e-6  else 0\n                                for d in self.d ])\n\n    def pc( self ):\n        \"\"\" e.g. 1000 x 2 U[:, :npc] * d[:npc], to plot etc. \"\"\"\n        n = self.npc\n        return self.U[:, :n] * self.d[:n]\n\n    # These 1-line methods may not be worth the bother;\n    # then use U d Vt directly --\n\n    def vars_pc( self, x ):\n        n = self.npc\n        return self.d[:n] * dot( self.Vt[:n], x.T ).T  # 20 vars -&gt; 2 principal\n\n    def pc_vars( self, p ):\n        n = self.npc\n        return dot( self.Vt[:n].T, (self.dinv[:n] * p).T ) .T  # 2 PC -&gt; 20 vars\n\n    def pc_obs( self, p ):\n        n = self.npc\n        return dot( self.U[:, :n], p.T )  # 2 principal -&gt; 1000 obs\n\n    def obs_pc( self, obs ):\n        n = self.npc\n        return dot( self.U[:, :n].T, obs ) .T  # 1000 obs -&gt; 2 principal\n\n    def obs( self, x ):\n        return self.pc_obs( self.vars_pc(x) )  # 20 vars -&gt; 2 principal -&gt; 1000 obs\n\n    def vars( self, obs ):\n        return self.pc_vars( self.obs_pc(obs) )  # 1000 obs -&gt; 2 principal -&gt; 20 vars\n\n\nclass Center:\n    \"\"\" A -= A.mean() /= A.std(), inplace -- use A.copy() if need be\n        uncenter(x) == original A . x\n    \"\"\"\n        # mttiw\n    def __init__( self, A, axis=0, scale=True, verbose=1 ):\n        self.mean = A.mean(axis=axis)\n        if verbose:\n            print \"Center -= A.mean:\", self.mean\n        A -= self.mean\n        if scale:\n            std = A.std(axis=axis)\n            self.std = np.where( std, std, 1. )\n            if verbose:\n                print \"Center /= A.std:\", self.std\n            A /= self.std\n        else:\n            self.std = np.ones( A.shape[-1] )\n        self.A = A\n\n    def uncenter( self, x ):\n        return np.dot( self.A, x * self.std ) + np.dot( x, self.mean )\n\n\n#...............................................................................\nif __name__ == \"__main__\":\n    import sys\n\n    csv = \"iris4.csv\"  # wikipedia Iris_flower_data_set\n        # 5.1,3.5,1.4,0.2  # ,Iris-setosa ...\n    N = 1000\n    K = 20\n    fraction = .90\n    seed = 1\n    exec \"\\n\".join( sys.argv[1:] )  # N= ...\n    np.random.seed(seed)\n    np.set_printoptions( 1, threshold=100, suppress=True )  # .1f\n    try:\n        A = np.genfromtxt( csv, delimiter=\",\" )\n        N, K = A.shape\n    except IOError:\n        A = np.random.normal( size=(N, K) )  # gen correlated ?\n\n    print \"csv: %s  N: %d  K: %d  fraction: %.2g\" % (csv, N, K, fraction)\n    Center(A)\n    print \"A:\", A\n\n    print \"PCA ...\" ,\n    p = PCA( A, fraction=fraction )\n    print \"npc:\", p.npc\n    print \"% variance:\", p.sumvariance * 100\n\n    print \"Vt[0], weights that give PC 0:\", p.Vt[0]\n    print \"A . Vt[0]:\", dot( A, p.Vt[0] )\n    print \"pc:\", p.pc()\n\n    print \"\\nobs &lt;-&gt; pc &lt;-&gt; x: with fraction=1, diffs should be ~ 0\"\n    x = np.ones(K)\n    # x = np.ones(( 3, K ))\n    print \"x:\", x\n    pc = p.vars_pc(x)  # d' Vt' x\n    print \"vars_pc(x):\", pc\n    print \"back to ~ x:\", p.pc_vars(pc)\n\n    Ax = dot( A, x.T )\n    pcx = p.obs(x)  # U' d' Vt' x\n    print \"Ax:\", Ax\n    print \"A'x:\", pcx\n    print \"max |Ax - A'x|: %.2g\" % np.linalg.norm( Ax - pcx, np.inf )\n\n    b = Ax  # ~ back to original x, Ainv A x\n    back = p.vars(b)\n    print \"~ back again:\", back\n    print \"max |back - x|: %.2g\" % np.linalg.norm( back - x, np.inf )\n\n# end pca.py\n</code>\n</pre>\n", "senID": 1}, {"src": "http://i44.tinypic.com/289aj9u.png", "tag": "img", "senID": 2}], [{"text": ["matploblib.mlab has a PCA implementation."], "childNum": 2, "tag": "p", "senID": 0, "childList": [{"text": "matploblib.mlab", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://matplotlib.sourceforge.net/api/mlab%5Fapi.html"}, {"href": "http://matplotlib.sourceforge.net/api/mlab%5Fapi.html#matplotlib.mlab.prepca", "text": "PCA implementation", "childNum": 0, "tag": "a", "childList": []}]}], [{"text": ["You might have a look at MDP.   "], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "MDP", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://mdp-toolkit.sourceforge.net/"}]}, {"text": ["I have not had the chance to test it myself, but I've bookmarked it exactly for the PCA functionality."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["SVD should work fine with 460 dimensions.", "It takes about 7 seconds on my Atom netbook.", "The eig() method takes more time (as it should, it uses more floating point operations) and will almost always be less accurate. "], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "more", "childNum": 0, "tag": "em", "pos": 2, "childList": []}]}, {"text": ["If you have less than 460 examples then what you want to do is diagonalize the scatter matrix (x - datamean)^T(x - mean), assuming your data points are columns, and then left-multiplying by (x - datamean).", "That might be faster in the case where you have more dimensions than data."], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "might", "childNum": 0, "tag": "em", "pos": 1, "childList": []}]}], [{"text": ["Here is another implementation of a PCA module for python using numpy, scipy and C-extensions.", "The module carries out PCA using either a SVD or the NIPALS (Nonlinear Iterative Partial Least Squares) algorithm which is implemented in C."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Here", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://folk.uio.no/henninri/pca%5Fmodule/"}]}], [{"text": ["I just finish reading the book Machine Learning: An Algorithmic Perspective.", "All code examples in the book was written by Python(and almost with Numpy).", "The code snippet of chatper10.2 Principal Components Analysis maybe worth a reading.", "It use numpy.linalg.eig.", "By the way, I think SVD can handle 460 * 460 dimensions very well.", "I have calculate a 6500*6500 SVD with numpy/scipy.linalg.svd on a very old PC:Pentium III 733mHz.", "To be honest, the script needs a lot of memory(about 1.xG) and a lot of time(about 30 minutes) to get the SVD result.", "But I think 460*460 on a modern PC will not be a big problem unless u need do SVD a huge number of times."], "childNum": 3, "tag": "p", "senID": 0, "childList": [{"text": "Machine Learning: An Algorithmic Perspective", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html"}, {"text": "chatper10.2 Principal Components Analysis", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://seat.massey.ac.nz/personal/s.r.marsland/Code/10/pca.py"}, {"text": "", "childNum": 0, "tag": "br", "pos": 3, "childList": []}]}]]