[[{"text": ["How are you doing the timing, and what's your laptop?"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["If you're doing the timing from the shell, you may be (as BiggAl suggests) hitting a delay just starting up python.", "That's real overhead and worth knowing about, but probably isn't your immediate concern.", "And I have trouble imaginging that this contributes 4 seconds of overhead... [Edited to add: although BiggAl suggests it really may be, under Windows]"], "childNum": 1, "tag": "p", "senID": 1, "childList": [{"text": "Edited to add", "childNum": 0, "tag": "strong", "pos": 2, "childList": []}]}, {"text": ["I think a more likely concern is memory bandwidth limitation.", "While you are going to fully use both your cores with this setup, you only have so much memory bandwidth, and that may end up being the limitation here.", "Each core is trying to write a lot of data (the range(N/2)) and then read it in (the sum) to do a fairly modest amount of computation (an\ninteger) and so I suspect computation isn't the bottleneck."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["I ran your same setup using timeit on a Nehalem box with pretty good memory-bandwidth per core, and did get the expected speedup:"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"code": "<pre>\n<code>\n from mpi4py import MPI\nimport timeit\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\n\nN = 10000000\n\ndef parSum():\n    if rank == 0:\n        ...etc\n\ndef serSum():\n    s = sum(range(N))\n\nif rank == 0:\n    print 'Parallel time:'\n    tp = timeit.Timer(\"parSum()\",\"from __main__ import parSum\")\n    print tp.timeit(number=10)\n\n    print 'Serial time:'\n    ts = timeit.Timer(\"serSum()\",\"from __main__ import serSum\")\n    print ts.timeit(number=10)\n</code>\n</pre>\n", "senID": 4}, {"text": ["from which I got "], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n $ mpirun -np 3 python ./sum.py\nParallel time:\n1.91955494881\nSerial time:\n3.84715008736\n</code>\n</pre>\n", "senID": 6}, {"text": ["If you think it's a memory bandwidth issue, you can test that by making the computation\nartificially compute-heavy; say using numpy and doing sum of more complicated functions of range:  sum(numpy.sin(range(N/2+1,N))), say.", "That should tilt the balance from memory access to computation.  "], "childNum": 1, "tag": "p", "senID": 7, "childList": [{"text": "sum(numpy.sin(range(N/2+1,N)))", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}], [{"text": ["In what follows, I assume you're using Python 2.x."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Depending on the hardware spec of your laptop, it is likely that there's heavy memory contention between processes 0 and 1."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["range(100000000/2) creates a list that takes 1.5GB of RAM on my PC, so you're looking at 3GB of RAM between the two processes.", "Using two cores to iterate over the two lists will likely result in memory bandwidth issues (and/or swapping).", "This is the most likely cause of the imperfect parallelization."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "range(100000000/2)", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Using xrange instead of range won't generate the lists and should parallelize a lot better by making the computation CPU-bound."], "childNum": 2, "tag": "p", "senID": 3, "childList": [{"text": "xrange", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "range", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["By the way, there's a bug in your code: the second (x)range should start at N/2, not N/2+1."], "childNum": 3, "tag": "p", "senID": 4, "childList": [{"text": "(x)range", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "N/2", "childNum": 0, "tag": "code", "childList": []}, {"text": "N/2+1", "childNum": 0, "tag": "code", "childList": []}]}], [{"text": ["Some thoughts:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"tag": "ul", "num": 3, "lis": [{"text": "Are you using python 2? If so, use ", "tag": "none", "senID": 1}, {"text": "Theoretically the algorithm bit should be 2x faster. In practise, it is more complicated than that. There is a cost for setting up threads or processes at the start of the algorithm which will add time to your run time; finally, there's a cost for synchronising the result at the end (waiting on joins). So the 2x speed increase will never actually be realised. For small values of any algorithm it is well known that serial algorithms outperform threaded counterparts; it is only when you reach an order of magnitude where the cost of thread creation is negligible compared to the work to be done that you notice an astronomical speed increase.", "tag": "none", "senID": 2}, {"text": ["Balancing of work may be a problem.", "On a 32 bit system, the maximum size of number that can fit into a register (and so be O(1) for add given the size of the numbers) is 4294967296 (2^32).", "Your sum, at large values, is 4999999950000000.", "Bignum addition is O(n) for the number of limbs (elements in the array) that you need, so you reach a slowdown as soon as you start using bignums as opposed to anything you can handle in a single memory address."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}]}], [{"text": ["Please read this Amdahl's Law"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Amdahl's Law", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Amdahl%27s_law"}]}, {"text": ["Your OS includes a large number of non-parallelizable bottlenecks.", "Your language library may also have some bottlenecks."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Interestingly, your intel hardware's Memory Write Ordering may also have some number of non-parallelizable bottlenecks."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Load balancing is one theory, also there is also going to be an obvious communication latency, but I wouldn't expect any of these, even in combination, to have that great a performance loss.", "I would guess that your largest overhead is that of starting 2 more instances of the python interpreter.", "Hopefully if you experiment with larger number you should find that the overhead does not in fact grow proportionality to N, but actually is a large constant plus a term dependent on N. For this reason you may want to stop the algorithm from going parallel for number less than some amount at which the performance improves."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["I'm not intimately acquainted with mpi, however it may be that you are better creating a pool of workers at the start of your application and have them wait for tasks, rather than creating them on the fly.", "This requires a more complex design, but only incurs the interpreter initialisation penalty once per application run."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}], [{"text": ["Probably its a bad load balancing: Node 0 has less work than node 1 since summing up the lower N/2 integers is faster than summing up the upper N/2 integers.", "As a consequence, node 2 gets the message from node 0 quite early and has to wait relatively long for node 1."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["EDIT: Sven Marnach is right; it's not the load balancing since sum(range(N)) and sum(range(N,2*N)) takes the same amount of time."], "childNum": 3, "tag": "p", "senID": 1, "childList": [{"text": "EDIT", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "sum(range(N))", "childNum": 0, "tag": "code", "childList": []}, {"text": "sum(range(N,2*N))", "childNum": 0, "tag": "code", "childList": []}]}], [{"text": ["I wrote a bit of code to test what bits of the mpi infrastructure take up time.", "This version of your code can use an abritary number of cores from 1 to lots and lots.", "The work is divided up evenly amongst the cores and sent back to host 0 to total.", "Host 0 also does work."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n import time\n\nt = time.time()\nimport pypar\nprint 'pypar init time', time.time()-t, 'seconds'\n\nrank = pypar.rank()\nhosts = pypar.size()\n\nN = 100000000\n\nnStart = (N/hosts) * rank\nif rank==hosts-1:\n    nStop = N\nelse:\n    nStop = ( ((N/hosts) * (rank+1)) )\nprint rank, 'working on', nStart, 'to', nStop\n\nt = time.time()\ns = sum(xrange(nStart,nStop))\nif rank == 0:\n    for p in range(1,hosts):\n        s += pypar.receive(p)\n        pypar.send(s,p) \nelse:\n    pypar.send(s,0) \n    s = pypar.receive(0)\nif rank==0:\n    print rank, 'total', s, 'in', time.time()-t, 'seconds'\npypar.Finalize()\n</code>\n</pre>\n", "senID": 1}, {"text": ["Results:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n pypar init time 1.68600010872 seconds\n1 working on 12500000 to 25000000\npypar init time 1.80400013924 seconds\n2 working on 25000000 to 37500000\npypar init time 1.98699998856 seconds\n3 working on 37500000 to 50000000\npypar init time 2.16499996185 seconds\n4 working on 50000000 to 62500000\nPypar (version 2.1.4.7) initialised MPI OK with 8 processors\npypar init time 1.5720000267 seconds\n0 working on 0 to 12500000\n0 total 4999999950000000 in 1.40100002289 seconds\npypar init time 2.34000015259 seconds\n6 working on 75000000 to 87500000\npypar init time 2.64600014687 seconds\n7 working on 87500000 to 100000000\npypar init time 2.23900008202 seconds\n5 working on 62500000 to 75000000\n</code>\n</pre>\n", "senID": 3}, {"text": ["Starting up the pypar and mpi libraries takes about 2.5 seconds.", "Then the actual work takes 1.4 seconds, to calculate and communicate back to host 0.", "Running as a single core it takes about 11 seconds.", "So using 8 cores scales nicely. "], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["Starting the mpiexec and python takes almost no time at all.", "As this pathetic test shows:"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n c:\\Data\\python speed testing&gt;time  0&lt;enter.txt\nThe current time is: 10:13:07.03\nEnter the new time:\n\nc:\\Data\\python speed testing&gt;mpiexec -n 1 python printTime.py\ntime.struct_time(tm_year=2011, tm_mon=8, tm_mday=4, tm_hour=10, tm_min=13, tm_sec=7, tm_wday=3, tm_yday=216, tm_isdst=0)\n</code>\n</pre>\n", "senID": 6}, {"text": ["Splitting out the actual time to run the summation from the time to setup the data and libraries yields good scalling of peformance improvements."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"src": "http://i.stack.imgur.com/NwdwK.png", "tag": "img", "senID": 8}]]