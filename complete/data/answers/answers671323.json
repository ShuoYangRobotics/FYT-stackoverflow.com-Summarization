[[{"text": ["Beautiful Soup can do this almost trivially:"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Beautiful Soup", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://www.crummy.com/software/BeautifulSoup/"}]}, {"code": "<pre>\n<code>\n from BeautifulSoup import BeautifulSoup as soup\n\nhtml = soup('&lt;body&gt;&lt;a href=\"123\"&gt;qwe&lt;/a&gt;&lt;a href=\"456\"&gt;asd&lt;/a&gt;&lt;/body&gt;')\nprint [tag.attrMap['href'] for tag in html.findAll('a', {'href': True})]\n</code>\n</pre>\n", "senID": 1}], [{"text": ["Another alternative to BeautifulSoup is lxml (http://codespeak.net/lxml/);"], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "http://codespeak.net/lxml/", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://codespeak.net/lxml/"}]}, {"code": "<pre>\n<code>\n import lxml.html\nlinks = lxml.html.parse(\"http://stackoverflow.com/\").xpath(\"//a/@href\")\nfor link in links:\n    print link\n</code>\n</pre>\n", "senID": 1}], [{"text": ["There's an HTML parser that comes standard in Python.", "Checkout htmllib."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "htmllib", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}], [{"text": ["What others haven't told you is that using regular expressions for this is not a reliable solution.", "Using regular expression will give you wrong results on many situations: if there are &lt;A> tags that are commented out, or if there are text in the page which include the string \"href=\", or if there are &lt;textarea&gt; elements with html code in it, and many others.", "Plus, the href attribute may exist on tags other that the anchor tag."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "", "childNum": 0, "tag": "br", "pos": 0, "childList": []}]}, {"text": ["What you need for this is XPath, which is a query language for DOM trees, i.e.", "it lets you retrieve any set of nodes satisfying the conditions you specify (HTML attributes are nodes in the DOM).", "XPath is a well standarized language now a days (W3C), and is well supported by all major languages.", "I strongly suggest you use XPath and not regexp for this.", "adw's answer shows one example of using XPath for your particular case."], "childNum": 4, "tag": "p", "senID": 1, "childList": [{"text": "XPath", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://en.wikipedia.org/wiki/Xpath"}, {"text": "", "childNum": 0, "tag": "br", "pos": 1, "childList": []}, {"text": "W3C", "tag": "a", "pos": 2, "childList": [], "childNum": 0, "href": "http://www.w3.org/TR/xpath"}, {"text": "", "childNum": 0, "tag": "br", "pos": 3, "childList": []}]}], [{"text": ["As previously mentioned: regex does not have the power to parse HTML.", "Do not use regex for parsing HTML.", "Do not pass Go.", "Do not collect \u00a3200."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Use an HTML parser."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["But for completeness, the primary problem is:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n re.match ('/href=\"(.*)\"/iU', line)\n</code>\n</pre>\n", "senID": 3}, {"text": ["You don't use the \u201c/.../flags\u201d syntax for decorating regexes in Python.", "Instead put the flags in a separate argument:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"code": "<pre>\n<code>\n re.match('href=\"(.*)\"', line, re.I|re.U)\n</code>\n</pre>\n", "senID": 5}, {"text": ["Another problem is the greedy \u2018.", "*\u2019 pattern.", "If you have two hrefs in a line, it'll happily suck up all the content between the opening \" of the first match and the closing \" of the second match.", "You can use the non-greedy \u2018.", "*?\u2019 or, more simply, \u2018[^\"]*\u2019 to only match up to the first closing quote."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["But don't use regexes for parsing HTML.", "Really."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}], [{"text": ["Don't divide the html content into lines, as there maybe multiple matches in a single line.", "Also don't assume there is always quotes around the url. "], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Do something like this:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n links = re.finditer(' href=\"?([^\\s^\"]+)', content)\n\nfor link in links:\n  print link\n</code>\n</pre>\n", "senID": 2}], [{"text": ["Well, just for completeness I will add here what I found to be the best answer, and I found it on the book Dive Into Python, from Mark Pilgrim."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Here follows the code to list all URL's from a webpage:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n from sgmllib import SGMLParser\n\nclass URLLister(SGMLParser):\n    def reset(self):                              \n        SGMLParser.reset(self)\n        self.urls = []\n\n    def start_a(self, attrs):                     \n        href = [v for k, v in attrs if k=='href']  \n        if href:\n            self.urls.extend(href)\n\nimport urllib, urllister\nusock = urllib.urlopen(\"http://diveintopython.org/\")\nparser = urllister.URLLister()\nparser.feed(usock.read())         \nusock.close()      \nparser.close()                    \nfor url in parser.urls: print url\n</code>\n</pre>\n", "senID": 2}, {"text": ["Thanks for all the replies."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}]]