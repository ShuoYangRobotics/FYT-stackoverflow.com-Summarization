[[{"text": ["If you want a really simple way to do this, just create a sqlite database:"], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"code": "<pre>\n<code>\n import sqlite3\nconn = sqlite3.connect('single.db')\ncur = conn.cursor()\ncur.execute(\"\"\"create table test(\nf1 text,\nf2 text,\nf3 text,\nf4 text,\nf5 text,\nf6 text,\nf7 text,\nf8 text,\nf9 text,\nf10 text,\nf11 text,\nf12 text,\nf13 text,\nf14 text,\nf15 text,\nprimary key(f1,  f2,  f3,  f4,  f5,  f6,  f7,  \n            f8,  f9,  f10,  f11,  f12,  f13,  f14,  f15))\n\"\"\"\nconn.commit()\n\n#simplified/pseudo code\nfor row in reader:\n    #assuming row returns a list-type object\n    try:\n        cur.execute('''insert into test values(?, ?, ?, ?, ?, ?, ?, \n                       ?, ?, ?, ?, ?, ?, ?, ?)''', row)\n        conn.commit()\n    except IntegrityError:\n        pass\n\nconn.commit()\ncur.execute('select * from test')\n\nfor row in cur:\n    #write row to csv file\n</code>\n</pre>\n", "senID": 1}, {"text": ["Then you wouldn't have to worry about any of the comparison logic yourself - just let sqlite take care of it for you.", "It probably won't be much faster than hashing the strings, but it's probably a lot easier.", "Of course you'd modify the type stored in the database if you wanted, or not as the case may be.", "Of course since you're already converting the data to a string you could just have one field instead.", "Plenty of options here."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["You are basically doing a merge sort, and removing duplicated entries."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Breaking the input into memory-sized pieces, sorting each of piece, then merging the pieces while removing duplicates is a sound idea in general. "], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Actually, up to a couple of gigs I would let the virtual memory system handle it and just write:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n input = open(infilename, 'rb')\noutput = open(outfile, 'wb')\n\nfor key,  group in itertools.groupby(sorted(input)):\n    output.write(key)\n</code>\n</pre>\n", "senID": 3}], [{"text": ["Your current method is not guaranteed to work properly."], "childNum": 1, "tag": "p", "senID": 0, "childList": [{"text": "Your current method is not guaranteed to work properly.", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Firstly, there is the small probability that two lines that are actually different can produce the same hash value.", "hash(a) == hash(b) does not always mean that a == b"], "childNum": 2, "tag": "p", "senID": 1, "childList": [{"text": "hash(a) == hash(b)", "childNum": 0, "tag": "code", "pos": 1, "childList": []}, {"text": "a == b", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["Secondly, you are making the probability higher with your \"reduce/lambda\" caper:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"code": "<pre>\n<code>\n &gt;&gt;&gt; reduce(lambda x,y: x+y, ['foo', '1', '23'])\n'foo123'\n&gt;&gt;&gt; reduce(lambda x,y: x+y, ['foo', '12', '3'])\n'foo123'\n&gt;&gt;&gt;\n</code>\n</pre>\n", "senID": 3}, {"text": ["BTW, wouldn't \"\".join(['foo', '1', '23']) be somewhat clearer?"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["BTW2, why not use a set instead of a dict for htable?"], "childNum": 3, "tag": "p", "senID": 5, "childList": [{"text": "set", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "dict", "childNum": 0, "tag": "code", "childList": []}, {"text": "htable", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["Here's a practical solution: get the \"core utils\" package from the GnuWin32 site, and install it.", "Then:"], "childNum": 2, "tag": "p", "senID": 6, "childList": [{"text": "Here's a practical solution:", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "GnuWin32", "tag": "a", "pos": -1, "childList": [], "childNum": 0, "href": "http://gnuwin32.sourceforge.net/"}]}, {"text": ["For each of steps 1 &amp; 3, you could use a Python script, or some of the other GnuWin32 utilities (head, tail, tee, cat, ...)."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}], [{"text": ["Your original solution is slightly incorrect: you could have different lines hashing to the same value (a hash collision), and your code would leave one of them out."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["In terms of algorithmic complexity, if you're expecting relatively few duplicates, I think the fastest solution would be to scan the file line by line, adding the hash of each line (as you did), but also storing the location of that line.", "Then when you encounter a duplicate hash, seek to the original place to make sure that it is a duplicate and not just a hash collision, and if so, seek back and skip the line."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["By the way, if the CSV values are normalized (i.e., records are considered equal iff the corresponding CSV rows are equivalent byte-for-byte), you need not involve CSV parsing here at all, just deal with plain text lines."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}], [{"text": ["Since I suppose you'll have to do this on a somewhat regular basis (or you'd have hacked a once-over script), and you mentioned you were interested in a theoretical solution, here's a possibility."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Read the input lines into B-Trees, ordered by each input line's hash value, writing them to disk when memory fills.", "We take care to store, on the B-Trees, the original lines attached to the hash (as a set, since we only care about unique lines).", "When we read a duplicate element, we check the lines set on the stored element and add it if it's a new line that happens to hash to the same value."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["Why B-Trees?", "They requires less disk reads when you only can (or want) to read parts of them to memory.", "The degree (number of children) on each node depends on the available memory and number of lines, but you don't want to have too many nodes."], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["Once we have those B-Trees on disk, we compare the lowest element from each of them.", "We remove the lowest of all, from all B-Trees that have it.", "We merge their lines sets, meaning that we have no duplicates left for those lines (and also that we have no more lines that hash to that value).", "We then write the lines from this merge into the output csv structure."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["We can separate half of the memory for reading the B-Trees, and half to keep the output csv in memory for some time.", "We flush the csv to disk when its half is full, appending to whatever has already been written.", "How much of each B-Tree we read on each step can be roughly calculated by (available_memory / 2) / number_of_btrees, rounded so we read full nodes."], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["In pseudo-Python:"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n ins = DictReader(...)\ni = 0\nwhile ins.still_has_lines_to_be_read():\n    tree = BTree(i)\n    while fits_into_memory:\n        line = ins.readline()\n        tree.add(line, key=hash)\n    tree.write_to_disc()\n    i += 1\nn_btrees = i\n\n# At this point, we have several (n_btres) B-Trees on disk\nwhile n_btrees:\n    n_bytes = (available_memory / 2) / n_btrees\n    btrees = [read_btree_from_disk(i, n_bytes)\n              for i in enumerate(range(n_btrees))]\n    lowest_candidates = [get_lowest(b) for b in btrees]\n    lowest = min(lowest_candidates)\n    lines = set()\n    for i in range(number_of_btrees):\n        tree = btrees[i]\n        if lowest == lowest_candidates[i]:\n            node = tree.pop_lowest()\n            lines.update(node.lines)\n        if tree.is_empty():\n        n_btrees -= 1\n\n    if output_memory_is_full or n_btrees == 0:\n        outs.append_on_disk(lines)\n</code>\n</pre>\n", "senID": 6}], [{"text": ["How about using heapq module to read pieces of file up to memory limit and write them out the sorted pieces (heapq keeps things always in sorted order)."], "childNum": 0, "tag": "p", "senID": 0, "childList": []}, {"text": ["Or you could catch the first word in line and divide the file to pieces by that.", "Then you can read the lines (maybe do ' '.join(line.split()) to unify the spacing/tabs in line if it is OK to change spacing) in set in alphabetic order clearing the set between the pieces (set removes duplicates) to get things half sorted (set is not in order, if you want you can read in to heap and write out to get sorted order, last occurrence in set replacing old values as you go.", ") Alternatively you can also sort the piece and remove duplicate lines with Joe Koberg's groupby solution.", "Lastly you can join pieces back together (you can of course do the writing as you go piece by piece to final file during sorting of pieces)"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}]]