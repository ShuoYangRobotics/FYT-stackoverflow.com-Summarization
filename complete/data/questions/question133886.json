[{"text": ["Simple regex-based lexer in Python"], "childNum": 0, "tag": "h2", "senID": 0, "childList": []}, {"text": ["Lexical analyzers are quite easy to write when you have regexes.", "Today I wanted to write a simple general analyzer in Python, and came up with:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n import re\nimport sys\n\nclass Token(object):\n    \"\"\" A simple Token structure.\n        Contains the token type, value and position. \n    \"\"\"\n    def __init__(self, type, val, pos):\n        self.type = type\n        self.val = val\n        self.pos = pos\n\n    def __str__(self):\n        return '%s(%s) at %s' % (self.type, self.val, self.pos)\n\n\nclass LexerError(Exception):\n    \"\"\" Lexer error exception.\n\n        pos:\n            Position in the input line where the error occurred.\n    \"\"\"\n    def __init__(self, pos):\n        self.pos = pos\n\n\nclass Lexer(object):\n    \"\"\" A simple regex-based lexer/tokenizer.\n\n        See below for an example of usage.\n    \"\"\"\n    def __init__(self, rules, skip_whitespace=True):\n        \"\"\" Create a lexer.\n\n            rules:\n                A list of rules. Each rule is a `regex, type`\n                pair, where `regex` is the regular expression used\n                to recognize the token and `type` is the type\n                of the token to return when it's recognized.\n\n            skip_whitespace:\n                If True, whitespace (\\s+) will be skipped and not\n                reported by the lexer. Otherwise, you have to \n                specify your rules for whitespace, or it will be\n                flagged as an error.\n        \"\"\"\n        self.rules = []\n\n        for regex, type in rules:\n            self.rules.append((re.compile(regex), type))\n\n        self.skip_whitespace = skip_whitespace\n        self.re_ws_skip = re.compile('\\S')\n\n    def input(self, buf):\n        \"\"\" Initialize the lexer with a buffer as input.\n        \"\"\"\n        self.buf = buf\n        self.pos = 0\n\n    def token(self):\n        \"\"\" Return the next token (a Token object) found in the \n            input buffer. None is returned if the end of the \n            buffer was reached. \n            In case of a lexing error (the current chunk of the\n            buffer matches no rule), a LexerError is raised with\n            the position of the error.\n        \"\"\"\n        if self.pos &gt;= len(self.buf):\n            return None\n        else:\n            if self.skip_whitespace:\n                m = self.re_ws_skip.search(self.buf[self.pos:])\n\n                if m:\n                    self.pos += m.start()\n                else:\n                    return None\n\n            for token_regex, token_type in self.rules:\n                m = token_regex.match(self.buf[self.pos:])\n\n                if m:\n                    value = self.buf[self.pos + m.start():self.pos + m.end()]\n                    tok = Token(token_type, value, self.pos)\n                    self.pos += m.end()\n                    return tok\n\n            # if we're here, no rule matched\n            raise LexerError(self.pos)\n\n    def tokens(self):\n        \"\"\" Returns an iterator to the tokens found in the buffer.\n        \"\"\"\n        while 1:\n            tok = self.token()\n            if tok is None: break\n            yield tok\n\n\nif __name__ == '__main__':\n    rules = [\n        ('\\d+',             'NUMBER'),\n        ('[a-zA-Z_]\\w+',    'IDENTIFIER'),\n        ('\\+',              'PLUS'),\n        ('\\-',              'MINUS'),\n        ('\\*',              'MULTIPLY'),\n        ('\\/',              'DIVIDE'),\n        ('\\(',              'LP'),\n        ('\\)',              'RP'),\n        ('=',               'EQUALS'),\n    ]\n\n    lx = Lexer(rules, skip_whitespace=True)\n    lx.input('erw = _abc + 12*(R4-623902)  ')\n\n    try:\n        for tok in lx.tokens():\n            print tok\n    except LexerError, err:\n        print 'LexerError at position', err.pos\n</code>\n</pre>\n", "senID": 2}, {"text": ["It works just fine, but I'm a bit worried that it's too inefficient.", "Are there any regex tricks that will allow me to write it in a more efficient / elegant way ? "], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["Specifically, is there a way to avoid looping over all the regex rules linearly to find one that fits ?"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["Thanks in advance"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}]