[{"text": ["SQLite Performance Benchmark -- why is :memory: so slow...only 1.5X as fast as disk?"], "childNum": 0, "tag": "h2", "senID": 0, "childList": []}, {"text": ["Why is :memory: in sqlite so slow?"], "childNum": 0, "tag": "h2", "senID": 1, "childList": []}, {"text": ["I've been trying to see if there are any performance improvements gained by using in-memory sqlite vs. disk based sqlite.", "Basically I'd like to trade startup time and memory to get extremely rapid queries which do not hit disk during the course of the application. "], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "not", "childNum": 0, "tag": "em", "pos": 1, "childList": []}]}, {"text": ["However, the following benchmark gives me only a factor of 1.5X in improved speed.", "Here, I'm generating 1M rows of random data and loading it into both a disk and memory based version of the same table.", "I then run random queries on both dbs, returning sets of size approx 300k.", "I expected the memory based version to be considerably faster, but as mentioned I'm only getting 1.5X speedups. "], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["I experimented with several other sizes of dbs and query sets; the advantage of :memory: does seem to go up as the number of rows in the db increases.", "I'm not sure why the advantage is so small, though I had a few hypotheses: "], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "does", "childNum": 0, "tag": "em", "pos": 0, "childList": []}]}, {"tag": "ul", "num": 4, "lis": [{"text": "the table used isn't big enough (in rows) to make :memory: a huge winner", "tag": "none", "senID": 5}, {"text": "more joins/tables would make the :memory: advantage more apparent", "tag": "none", "senID": 6}, {"text": "there is some kind of caching going on at the connection or OS level such that the previous results are accessible somehow, corrupting the benchmark", "tag": "none", "senID": 7}, {"text": "there is some kind of hidden disk access going on that I'm not seeing (I haven't tried lsof yet, but I did turn off the PRAGMAs for journaling)", "tag": "none", "senID": 8}]}, {"text": ["Am I doing something wrong here?", "Any thoughts on why :memory: isn't producing nearly instant lookups?", "Here's the benchmark: "], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"code": "<pre>\n<code>\n ==&gt; sqlite_memory_vs_disk_benchmark.py &lt;==\n\n#!/usr/bin/env python\n\"\"\"Attempt to see whether :memory: offers significant performance benefits.\n\n\"\"\"\nimport os\nimport time\nimport sqlite3\nimport numpy as np\n\ndef load_mat(conn,mat):\n    c = conn.cursor()\n\n    #Try to avoid hitting disk, trading safety for speed.\n    #http://stackoverflow.com/questions/304393\n    c.execute('PRAGMA temp_store=MEMORY;')\n    c.execute('PRAGMA journal_mode=MEMORY;')\n\n    # Make a demo table\n    c.execute('create table if not exists demo (id1 int, id2 int, val real);')\n    c.execute('create index id1_index on demo (id1);')\n    c.execute('create index id2_index on demo (id2);')\n    for row in mat:\n        c.execute('insert into demo values(?,?,?);', (row[0],row[1],row[2]))\n    conn.commit()\n\ndef querytime(conn,query):\n    start = time.time()\n    foo = conn.execute(query).fetchall()\n    diff = time.time() - start\n    return diff\n\n#1) Build some fake data with 3 columns: int, int, float\nnn   = 1000000 #numrows\ncmax = 700    #num uniques in 1st col\ngmax = 5000   #num uniques in 2nd col\n\nmat = np.zeros((nn,3),dtype='object')\nmat[:,0] = np.random.randint(0,cmax,nn)\nmat[:,1] = np.random.randint(0,gmax,nn)\nmat[:,2] = np.random.uniform(0,1,nn)\n\n#2) Load it into both dbs &amp; build indices\ntry: os.unlink('foo.sqlite')\nexcept OSError: pass\n\nconn_mem = sqlite3.connect(\":memory:\")\nconn_disk = sqlite3.connect('foo.sqlite')\nload_mat(conn_mem,mat)\nload_mat(conn_disk,mat)\ndel mat\n\n#3) Execute a series of random queries and see how long it takes each of these\nnumqs = 10\nnumqrows = 300000 #max number of ids of each kind\nresults = np.zeros((numqs,3))\nfor qq in range(numqs):\n    qsize = np.random.randint(1,numqrows,1)\n    id1a = np.sort(np.random.permutation(np.arange(cmax))[0:qsize]) #ensure uniqueness of ids queried\n    id2a = np.sort(np.random.permutation(np.arange(gmax))[0:qsize])\n    id1s = ','.join([str(xx) for xx in id1a])\n    id2s = ','.join([str(xx) for xx in id2a])\n    query = 'select * from demo where id1 in (%s) AND id2 in (%s);' % (id1s,id2s)\n\n    results[qq,0] = round(querytime(conn_disk,query),4)\n    results[qq,1] = round(querytime(conn_mem,query),4)\n    results[qq,2] = int(qsize)\n\n#4) Now look at the results\nprint \"  disk | memory | qsize\"\nprint \"-----------------------\"\nfor row in results:\n    print \"%.4f | %.4f | %d\" % (row[0],row[1],row[2])\n</code>\n</pre>\n", "senID": 10}, {"text": ["Here's the results.", "Note that disk takes about 1.5X as long as memory for a fairly wide range of query sizes. "], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"code": "<pre>\n<code>\n [ramanujan:~]$python -OO sqlite_memory_vs_disk_clean.py\n  disk | memory | qsize\n-----------------------\n9.0332 | 6.8100 | 12630\n9.0905 | 6.6953 | 5894\n9.0078 | 6.8384 | 17798\n9.1179 | 6.7673 | 60850\n9.0629 | 6.8355 | 94854\n8.9688 | 6.8093 | 17940\n9.0785 | 6.6993 | 58003\n9.0309 | 6.8257 | 85663\n9.1423 | 6.7411 | 66047\n9.1814 | 6.9794 | 11345\n</code>\n</pre>\n", "senID": 12}, {"text": ["Shouldn't RAM be almost instant relative to disk?", "What's going wrong here? "], "childNum": 0, "tag": "p", "senID": 13, "childList": []}, {"text": ["Edit"], "childNum": 0, "tag": "h2", "senID": 14, "childList": []}, {"text": ["Some good suggestions here. "], "childNum": 0, "tag": "p", "senID": 15, "childList": []}, {"text": ["I guess the main takehome point for me is that **there's probably no way to make :memory: absolutely faster, but there is a way to make disk access relatively slower.", "** "], "childNum": 2, "tag": "p", "senID": 16, "childList": [{"text": "absolutely faster", "childNum": 0, "tag": "em", "pos": 0, "childList": []}, {"text": "relatively slower.", "childNum": 0, "tag": "em", "pos": -1, "childList": []}]}, {"text": ["In other words, the benchmark is adequately measuring the realistic performance of memory, but not the realistic performance of disk (e.g.", "because the cache_size pragma is too big or because I'm not doing writes).", "I'll mess around with those parameters and post my findings when I get a chance.  "], "childNum": 0, "tag": "p", "senID": 17, "childList": []}, {"text": ["That said, if there is anyone who thinks I can squeeze some more speed out of the in-memory db (other than by jacking up the cache_size and default_cache_size, which I will do), I'm all ears..."], "childNum": 0, "tag": "p", "senID": 18, "childList": []}]