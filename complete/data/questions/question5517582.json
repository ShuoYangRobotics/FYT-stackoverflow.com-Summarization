[{"text": ["Dangerous Python Keywords?"], "childNum": 0, "tag": "h2", "senID": 0, "childList": []}, {"text": ["I am about to get a bunch of python scripts from an untrusted source."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["I'd like to be sure that no part of the code can hurt my system, meaning:"], "childNum": 0, "tag": "p", "senID": 2, "childList": []}, {"text": ["(1) the code is not allowed to import ANY MODULE"], "childNum": 1, "tag": "p", "senID": 3, "childList": [{"text": "(1) the code is not allowed to import ANY MODULE", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["(2) the code is not allowed to read or write any data, connect to the network etc"], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "(2) the code is not allowed to read or write any data, connect to the network etc", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["(the purpose of each script is to loop through a list, compute some data from input given to it and return the computed value)"], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"text": ["before I execute such code, I'd like to have a script 'examine' it and make sure that there's nothing dangerous there that could hurt my system."], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["I thought of using the following approach: check that the word 'import' is not used (so we are guaranteed that no modules are imported)"], "childNum": 1, "tag": "p", "senID": 7, "childList": [{"text": "check that the word 'import' is not used (so we are guaranteed that no modules are imported)", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["yet, it would still be possible for the user (if desired) to write code to read/write files etc (say, using open). "], "childNum": 1, "tag": "p", "senID": 8, "childList": [{"text": "open", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["Then here comes the question:"], "childNum": 0, "tag": "p", "senID": 9, "childList": []}, {"text": ["(1) where can I get a 'global' list of python methods (like open)? "], "childNum": 1, "tag": "p", "senID": 10, "childList": [{"text": "open", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["(2) Is there some code that I could add to each script that is sent to me (at the top) that would make some 'global' methods invalid for that script (for example, any use of the keyword open would lead to an exception)?"], "childNum": 1, "tag": "p", "senID": 11, "childList": [{"text": "open", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["I know that there are some solutions of python sandboxing.", "but please try to answer this question as I feel this is the more relevant approach for my needs."], "childNum": 0, "tag": "p", "senID": 12, "childList": []}, {"text": ["EDIT: suppose that I make sure that no import is in the file, and that no possible hurtful methods (such as open, eval, etc) are in it.", "can I conclude that the file is SAFE?", "(can you think of any other 'dangerous' ways that built-in methods can be run?"], "childNum": 4, "tag": "p", "senID": 13, "childList": [{"text": "EDIT:", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}, {"text": "import", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "open", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "eval", "childNum": 0, "tag": "code", "pos": -1, "childList": []}]}]