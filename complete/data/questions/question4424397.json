[{"text": ["python data type to track duplicates"], "childNum": 0, "tag": "h2", "senID": 0, "childList": []}, {"text": ["I often keep track of duplicates with something like this:"], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"code": "<pre>\n<code>\n processed = set() \nfor big_string in strings_generator:\n    if big_string not in processed:\n        processed.add(big_string)\n        process(big_string)\n</code>\n</pre>\n", "senID": 2}, {"text": ["I am dealing with massive amounts of data so don't want to maintain the processed set in memory.", "I have a version that uses sqlite to store the data on disk, but then this process runs much slower."], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["To cut down on memory use what do you think of using hashes like this:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"code": "<pre>\n<code>\n processed = set() \nfor big_string in string_generator:\n    key = hash(big_string)\n    if key not in ignored:\n        processed.add(key)\n        process(big_string)\n</code>\n</pre>\n", "senID": 5}, {"text": ["The drawback is I could lose data through occasional hash collisions.", "1 collision in 1 billion hashes would not be a problem for my use. "], "childNum": 0, "tag": "p", "senID": 6, "childList": []}, {"text": ["I tried the md5 hash but found generating the hashes became a bottleneck.  "], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["What would you suggest instead?"], "childNum": 0, "tag": "p", "senID": 8, "childList": []}]