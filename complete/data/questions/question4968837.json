[{"text": ["Postgres Performance Tips Loading in billions of rows"], "childNum": 0, "tag": "h2", "senID": 0, "childList": []}, {"text": ["I am in the middle of a project involving trying to grab numerous pieces of information out of 70GB worth of xml documents and loading it into a relational database (in this case postgres) I am currently using python scripts and psycopg2 to do this inserts and whatnot.", "I have found that as the number of rows in the some of the tables increase.", "(The largest of which is at around 5 million rows) The speed of the script (inserts) has slowed to a crawl.", "What was once taking a couple of minutes now takes about an hour."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["What can I do to speed this up?", "Was I wrong in using python and psycopg2 for this task?", "Is there anything I can do to the database that may speed up this process.", "I get the feeling I am going about this in entirely the wrong way. "], "childNum": 0, "tag": "p", "senID": 2, "childList": []}]