[{"text": ["How to reliably guess the encoding between MacRoman, CP1252, Latin1, UTF-8, and ASCII"], "childNum": 0, "tag": "h2", "senID": 0, "childList": []}, {"text": ["At work it seems like no week ever passes without some encoding-related conniption, calamity, or catastrophe.", "The problem usually derives from programmers who think they can reliably process a \u201ctext\u201d file without specifying the encoding.", "But you can't."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["So it's been decided to henceforth forbid files from ever having names that end in *.txt or *.text.", "The thinking is that those extensions mislead the casual programmer into a dull complacency regarding encodings, and this leads to improper handling.", "It would almost be better to have no\nextension at all, because at least then you know that you don\u2019t know what you\u2019ve got."], "childNum": 3, "tag": "p", "senID": 2, "childList": [{"text": "*.txt", "childNum": 0, "tag": "code", "pos": 0, "childList": []}, {"text": "*.text", "childNum": 0, "tag": "code", "pos": -1, "childList": []}, {"text": "know", "childNum": 0, "tag": "em", "pos": 2, "childList": []}]}, {"text": ["However, we aren\u2019t goint to go that far.", "Instead you will be expected to use a filename that ends in the encoding.", "So for text files, for example, these would be something like README.ascii, README.latin1, README.utf8, etc."], "childNum": 3, "tag": "p", "senID": 3, "childList": [{"text": "README.ascii", "childNum": 0, "tag": "code", "pos": 2, "childList": []}, {"text": "README.latin1", "childNum": 0, "tag": "code", "childList": []}, {"text": "README.utf8", "childNum": 0, "tag": "code", "childList": []}]}, {"text": ["For files that demand a particular extension, if one can specify the encoding inside the file itself, such as in Perl or Python, then you shall do that.", "For files like Java source where no such facility exists internal to the file, you will put the encoding before the extension, such as SomeClass-utf8.java."], "childNum": 1, "tag": "p", "senID": 4, "childList": [{"text": "SomeClass-utf8.java", "childNum": 0, "tag": "code", "pos": 1, "childList": []}]}, {"text": ["For output, UTF-8 is to be strongly preferred."], "childNum": 1, "tag": "p", "senID": 5, "childList": [{"text": "strongly", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["But for input, we need to figure out how to deal with the thousands of files in our codebase named *.txt.", "We want to rename all of them to fit into our new standard.", "But we can\u2019t possibly eyeball them all.", "So we need a library or program that actually works."], "childNum": 1, "tag": "p", "senID": 6, "childList": [{"text": "*.txt", "childNum": 0, "tag": "code", "pos": 0, "childList": []}]}, {"text": ["These are variously in ASCII, ISO-8859-1, UTF-8, Microsoft CP1252, or Apple MacRoman.", "Although we're know we can tell if something is ASCII, and we stand a good change of knowing if something is probably UTF-8, we\u2019re stumped about the 8-bit encodings.", "Because we\u2019re running in a mixed Unix environment (Solaris, Linux, Darwin) with most desktops being Macs, we have quite a few annoying MacRoman files.", "And these especially are a problem."], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"text": ["For some time now I\u2019ve been looking for a way to programmatically determine which of"], "childNum": 0, "tag": "p", "senID": 8, "childList": []}, {"text": ["a file is in, and I haven\u2019t found a program or library that can reliably distinguish between those the three different 8-bit encodings.", "We probably have over a thousand MacRoman files alone, so whatever charset detector we use has to be able to sniff those out.", "Nothing I\u2019ve looked at can manage the trick.", "I had big hopes for the ICU charset detector library, but it cannot handle MacRoman.", "I\u2019ve also looked at modules to do the same sort of thing in both Perl and Python, but again and again it\u2019s always the same story: no support for detecting MacRoman."], "childNum": 1, "tag": "p", "senID": 9, "childList": [{"text": "ICU charset detector library", "tag": "a", "pos": 3, "childList": [], "childNum": 0, "href": "http://userguide.icu-project.org/conversion/detection#TOC-CharsetDetector"}]}, {"text": ["What I am therefore looking for is an existing library or program that reliably determines which of those five encodings a file is in\u2014and preferably more than that.", "In particular it has to distinguish between the three 3-bit encoding I\u2019ve cited, especially MacRoman.", "The files are more than 99% English language text; there are a few in other languages, but not many."], "childNum": 1, "tag": "p", "senID": 10, "childList": [{"text": "especially MacRoman", "childNum": 0, "tag": "strong", "pos": 1, "childList": []}]}, {"text": ["If it\u2019s library code, our language preference is for it to be in Perl, C, Java, or Python, and in that order.", "If it\u2019s just a program, then we don\u2019t really care what language it\u2019s in so long as it comes in full source, runs on Unix, and is fully unencumbered."], "childNum": 0, "tag": "p", "senID": 11, "childList": []}, {"text": ["Has anyone else had this problem of a zillion legacy text files randomly encoded?", "If so, how did you attempt to solve it, and how successful were you?", "This is the most important aspect of my question, but I\u2019m also interested in whether you think encouraging programmers to name (or rename) their files with the actual encoding those files are in will help us avoid the problem in the future.", "Has anyone ever tried to enforce this on an institutional basis, and if so, was that successful or not, and why?"], "childNum": 1, "tag": "p", "senID": 12, "childList": [{"text": "that", "childNum": 0, "tag": "em", "pos": 3, "childList": []}]}, {"text": ["And yes, I fully understand why one cannot guarantee a definite answer given the nature of the problem.", "This is especially the case with small files, where you don\u2019t have enough data to go on.", "Fortunately, our files are seldom small.", "Apart from the random README file, most are in the size range of 50k to 250k, and many are larger.", "Anything more than a few K in size is guaranteed to be in English."], "childNum": 1, "tag": "p", "senID": 13, "childList": [{"text": "README", "childNum": 0, "tag": "code", "pos": 3, "childList": []}]}, {"text": ["The problem domain is biomedical text mining, so we sometimes deal with extensive and extremely large corpora, like all of PubMedCentral\u2019s Open Access respository.", "A rather huge file is the BioThesaurus 6.0, at 5.7 gigabytes.", "This file is especially annoying because it is almost all UTF-8.", "However, some numbskull went and stuck a few lines in it that are in some 8-bit encoding\u2014Microsoft CP1252, I believe.", "It takes quite a while before you trip on that one.", ":("], "childNum": 1, "tag": "p", "senID": 14, "childList": [{"text": "almost", "childNum": 0, "tag": "em", "pos": 2, "childList": []}]}]