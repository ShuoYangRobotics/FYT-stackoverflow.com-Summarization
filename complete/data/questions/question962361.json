[{"text": ["How can I speed up update/replace operations in PostgreSQL?"], "childNum": 0, "tag": "h2", "senID": 0, "childList": []}, {"text": ["We have a rather specific application that uses PostgreSQL 8.3 as a storage backend (using Python and psycopg2).", "The operations we perform  to the important tables are in the majority of cases inserts or updates (rarely deletes or selects)."], "childNum": 0, "tag": "p", "senID": 1, "childList": []}, {"text": ["For sanity reasons we have created our own Data Mapper-like layer that works reasonably well, but it has one big bottleneck, the update performance.", "Of course, I'm not expecting the update/replace scenario to be as speedy as the 'insert to an empty table' one, but it would be nice to get a bit closer."], "childNum": 1, "tag": "p", "senID": 2, "childList": [{"text": "Data Mapper", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://martinfowler.com/eaaCatalog/dataMapper.html"}]}, {"text": ["Note that this system is free from concurrent updates"], "childNum": 0, "tag": "p", "senID": 3, "childList": []}, {"text": ["We always set all the fields of each rows on an update, which can be seen in the terminology where I use the word 'replace' in my tests.", "I've so far tried two approaches to our update problem:"], "childNum": 0, "tag": "p", "senID": 4, "childList": []}, {"text": ["These both speeds up the updates a fair bit, although the latter slows down inserts a bit: "], "childNum": 0, "tag": "p", "senID": 5, "childList": []}, {"code": "<pre>\n<code>\n Multi-row insert           : 50000 items inserted in  1.32 seconds averaging 37807.84 items/s\nexecutemany() update       : 50000 items updated  in 26.67 seconds averaging  1874.57 items/s\nupdate_andres              : 50000 items updated  in  3.84 seconds averaging 13028.51 items/s\nupdate_merlin83 (i/d/i)    : 50000 items updated  in  1.29 seconds averaging 38780.46 items/s\nupdate_merlin83 (i/u)      : 50000 items updated  in  1.24 seconds averaging 40313.28 items/s\nreplace_item() procedure   : 50000 items replaced in  3.10 seconds averaging 16151.42 items/s\nMulti-row insert_or_replace: 50000 items inserted in  2.73 seconds averaging 18296.30 items/s\nMulti-row insert_or_replace: 50000 items replaced in  2.02 seconds averaging 24729.94 items/s\n</code>\n</pre>\n", "senID": 6}, {"text": ["Random notes about the test run:"], "childNum": 0, "tag": "p", "senID": 7, "childList": []}, {"tag": "ul", "num": 6, "lis": [{"text": "All tests are run on the same computer as the database resides; connecting to localhost.", "tag": "none", "senID": 8}, {"text": "Inserts and updates are applied to the database in batches of of 500 items, each sent in its own transaction (", "tag": "none", "senID": 9}, {"text": "All update/replace tests used the same values as were already in the database.", "tag": "none", "senID": 10}, {"text": "All data was escaped using the psycopg2 adapt() function.", "tag": "none", "senID": 11}, {"text": "All tables are truncated and vacuumed before use (", "tag": "none", "senID": 12}, {"text": ["The table looks like this:"], "childNum": 0, "tag": "p", "senID": 13, "childList": []}]}, {"text": ["So, the real question is: How can I speed up update/replace operations a bit more?", "(I think these findings might be 'good enough', but I don't want to give up without tapping the SO crowd :)"], "childNum": 0, "tag": "p", "senID": 14, "childList": []}, {"text": ["Also anyones hints towards a more elegant replace_item(), or evidence that my tests are completely broken would be most welcome."], "childNum": 0, "tag": "p", "senID": 15, "childList": []}, {"text": ["The test script is available here if you'd like to attempt to reproduce.", "Remember to check it first though...it WorksForMe, but..."], "childNum": 1, "tag": "p", "senID": 16, "childList": [{"text": "here", "tag": "a", "pos": 0, "childList": [], "childNum": 0, "href": "http://fnord.se/perftest.py"}]}, {"text": ["You will need to edit the db.connect() line to suit your setup."], "childNum": 0, "tag": "p", "senID": 17, "childList": []}, {"text": ["EDIT"], "childNum": 1, "tag": "p", "senID": 18, "childList": [{"text": "EDIT", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Thanks to andres in #postgresql @ freenode I have another test with a single-query update; much like a multi-row insert (listed as update_andres above)."], "childNum": 0, "tag": "p", "senID": 19, "childList": []}, {"code": "<pre>\n<code>\n UPDATE item\nSET a0=i.a0, a1=i.a1, a2=i.a2 \nFROM (VALUES ('00:00:00:00:00:01', 'v0', 'v1', 'v2'), \n             ('00:00:00:00:00:02', 'v3', 'v4', 'v5'),\n             ...\n      ) AS i(key, a0, a1, a2)\nWHERE item.key=i.key::macaddr\n</code>\n</pre>\n", "senID": 20}, {"text": ["EDIT"], "childNum": 1, "tag": "p", "senID": 21, "childList": [{"text": "EDIT", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Thanks to merlin83 in #postgresql @ freenode and jug/jwp below I have another test with an insert-to-temp/delete/insert approach (listed as \"update_merlin83 (i/d/i)\" above)."], "childNum": 0, "tag": "p", "senID": 22, "childList": []}, {"code": "<pre>\n<code>\n INSERT INTO temp_item (key, a0, a1, a2)\n    VALUES (\n        ('00:00:00:00:00:01', 'v0', 'v1', 'v2'),\n        ('00:00:00:00:00:02', 'v3', 'v4', 'v5'),\n        ...);\n\nDELETE FROM item\nUSING temp_item\nWHERE item.key=temp_item.key;\n\nINSERT INTO item (key, a0, a1, a2)\n    SELECT key, a0, a1, a2\n    FROM temp_item;\n</code>\n</pre>\n", "senID": 23}, {"text": ["My gut feeling is that these tests are not very representative to the performance in the real-world scenario, but I think the differences are great enough to give an indication of the most promising approaches for further investigation.", "The perftest.py script contains all updates as well for those of you who want to check it out.", "It's fairly ugly though, so don't forget your goggles :)"], "childNum": 0, "tag": "p", "senID": 24, "childList": []}, {"text": ["EDIT"], "childNum": 1, "tag": "p", "senID": 25, "childList": [{"text": "EDIT", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["andres in #postgresql @ freenode pointed out that I should test with an insert-to-temp/update variant (listed as \"update_merlin83 (i/u)\" above)."], "childNum": 0, "tag": "p", "senID": 26, "childList": []}, {"code": "<pre>\n<code>\n INSERT INTO temp_item (key, a0, a1, a2)\n    VALUES (\n        ('00:00:00:00:00:01', 'v0', 'v1', 'v2'),\n        ('00:00:00:00:00:02', 'v3', 'v4', 'v5'),\n        ...);\n\nUPDATE item\nSET a0=temp_item.a0, a1=temp_item.a1, a2=temp_item.a2\nFROM temp_item\nWHERE item.key=temp_item.key\n</code>\n</pre>\n", "senID": 27}, {"text": ["EDIT"], "childNum": 1, "tag": "p", "senID": 28, "childList": [{"text": "EDIT", "childNum": 0, "tag": "strong", "pos": 0, "childList": []}]}, {"text": ["Probably final edit:\nI changed my script to match our load scenario better, and it seems the numbers hold even when scaling things up a bit and adding some randomness.", "If anyone gets very different numbers from some other scenario I'd be interested in knowing about it."], "childNum": 0, "tag": "p", "senID": 29, "childList": []}]